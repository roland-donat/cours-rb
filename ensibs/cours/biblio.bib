
@article{dinwoodie_sensitivity_2012,
	title = {Sensitivity of {Offshore} {Wind} {Turbine} {Operation} \& {Maintenance} {Costs} to {Operational} {Parameters}},
	journal = {European Safety, Reliability and Data Association (42nd ESReDA Seminar)},
	author = {Dinwoodie, Iain and McMillan, David},
	year = {2012},
}

@article{dick_multivariate_2003,
	title = {Multivariate {Statistical} {Model} for {Predicting} {Occurrence} and {Location} of {Broken} {Rails}},
	volume = {1825},
	issn = {0361-1981},
	url = {http://dx.doi.org/10.3141/1825-07},
	doi = {10.3141/1825-07},
	abstract = {Broken rails are the leading cause of major accidents on U.S. railroads and frequently cause delays. A multivariate statistical model was developed to improve the prediction of broken-rail incidences (i.e., service failures). Improving the prediction of conditions that cause broken rails can assist railroads in allocating inspection, detection, and preventive resources more efficiently, to enhance safety, reduce the risk of hazardous materials transportation, improve service quality, and maximize rail assets. The service failure prediction model (SFPM) uses a combination of engineering and traffic data commonly recorded by major railroads. A Burlington Northern Santa Fe Railway database was developed in which the locations of approximately 1,800 service failures over 2 years were recorded. The data on each location were supplemented with information on other engineering and traffic volume parameters. A complementary database with the same parameters was developed for a randomly selected set of locations at which service failures had not occurred. The combined databases were analyzed using multivariate statistical methods to identify the variables and their combinations most strongly correlated with service failures. SFPM accuracy in predicting service failures at specific locations exceeded 85\%. Although further validation is necessary, SFPM is promising in the quantitative prediction of broken rails, thereby improving a railroad's ability to manage its assets and risks.},
	number = {-1},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Dick, R. C. and Barkan, C. and Chapman, E. and Stehly, M.},
	month = jan,
	year = {2003},
	keywords = {railway, reliability},
	pages = {48--55},
}

@inproceedings{dietrich_discrete_2017,
	title = {A discrete event simulation and evaluation framework for multi {UAV} system maintenance processes},
	booktitle = {Systems {Engineering} {Symposium} ({ISSE}), 2017 {IEEE} {International}},
	publisher = {IEEE},
	author = {Dietrich, Thomas and Krug, Silvia and Zimmermann, Armin},
	year = {2017},
	keywords = {discrete time, Maintenance, simulation},
	pages = {1--6},
	file = {Dietrich_al2017DES_UAV_Maintenance.pdf:/home/roland/Zotero/storage/IDKKNA9P/Dietrich_al2017DES_UAV_Maintenance.pdf:application/pdf},
}

@book{denise_clustering_1995,
	title = {Clustering {Without} ({Thinking} {About}) {Triangulation}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.63},
	abstract = {The undirected technique for evaluating belief networks [ Jensen et al., 1990a, Lauritzen and Spiegelhalter, 1988 ] requires clustering the nodes in the network into a junction tree. In the traditional view, the junction tree is constructed from the cliques of the moralized and triangulated belief network: triangulation is taken to be the primitive concept, the goal towards which any clustering algorithm (e.g. node elimination) is directed. In this paper, we present an alternative...},
	author = {Denise, D.},
	year = {1995},
	keywords = {graphical\_model, inference},
}

@article{dechter_bucket_1999,
	title = {Bucket {Elimination}: {A} {Unifying} {Framework} for {Reasoning}},
	volume = {113},
	url = {http://citeseer.ist.psu.edu/520583.html},
	abstract = {Bucket elimination is an algorithmic framework that generalizes dynamic programming to accommodate many problem-solving and reasoning tasks. Algorithms such as directional-resolution for propositional satisfiability, adaptive-consistency for constraint satisfaction, Fourier and Gaussian elimination for solving linear equalities and inequalities, and dynamic programming for combinatorial optimization, can all be accommodated within the bucket elimination framework. Many probabilistic...},
	number = {1-2},
	journal = {Artificial Intelligence},
	author = {Dechter, R.},
	year = {1999},
	keywords = {graphical\_model, inference},
	pages = {41--85},
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} {Via} the {EM} {Algorithm}},
	volume = {B 39},
	journal = {Journal of the Royal Statistical Society},
	author = {Dempster, A. and Laird, N. and Rubin, D. B.},
	year = {1977},
	keywords = {em},
	pages = {1--38},
}

@article{dekker_applications_1996,
	title = {Applications of maintenance optimization models: a review and analysis},
	volume = {51},
	url = {http://dx.doi.org/10.1016/0951-8320(95)00076-3},
	doi = {10.1016/0951-8320(95)00076-3},
	abstract = {In this paper we give an overview of applications of maintenance optimization models published so far. We analyze the role of these models in maintenance and discuss the factors which may have hampered applications. Finally, we discuss future prospects.},
	number = {3},
	journal = {Maintenance and reliability},
	author = {Dekker, Rommert},
	month = mar,
	year = {1996},
	pages = {229--240},
}

@article{dawson_look_2014,
	title = {Look before you (s) leep: evaluating the use of fatigue detection technologies within a fatigue risk management system for the road transport industry},
	volume = {18},
	number = {2},
	journal = {Sleep medicine reviews},
	author = {Dawson, Drew and Searle, Amelia K and Paterson, Jessica L},
	year = {2014},
	pages = {141--152},
}

@incollection{darwiche_using_2001,
	title = {Using {Recursive} {Decomposition} to {Construct} {Elimination} {Orders}, {Jointrees} and {Dtrees}},
	booktitle = {Trends in {Artificial} {Intelligence}, {Lecture} notes in {AI}, 2143},
	publisher = {Springer-Verlag},
	author = {Darwiche, A. and Hopkins, M.},
	year = {2001},
	keywords = {graphical\_model, inference},
	pages = {180--191},
}

@article{davis_piecewise-deterministic_1984,
	title = {Piecewise-{Deterministic} {Markov} {Processes}: {A} {General} {Class} of {Non}-{Diffusion} {Stochastic} {Models}},
	volume = {46},
	copyright = {Copyright © 1984 Royal Statistical Society},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345677},
	abstract = {A general class of non-diffusion stochastic model is introduced with a view to providing a framework for studying optimization problems arising in queueing systems, inventory theory, resource allocation and other areas. The corresponding stochastic processes are Markov processes consisting of a mixture of deterministic motion and random jumps. Stochastic calculus for these processes is developed and a complete characterization of the extended generator is given; this is the main technical result of the paper. The relevance of the extended generator concept in applied problems is discussed and some recent results on optimal control of piecewise-deterministic processes are described.},
	language = {English},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Davis, M. H. A.},
	year = {1984},
	pages = {pp. 353--388},
}

@article{darwiche_constant-space_2001,
	title = {Constant-space reasoning in dynamic {Bayesian} networks},
	volume = {26},
	issn = {0888613X},
	url = {http://dx.doi.org/10.1016/S0888-613X(00)00067-0},
	doi = {10.1016/S0888-613X(00)00067-0},
	abstract = {Dynamic Bayesian networks (DBNs) have been receiving increased attention as a tool for modeling complex stochastic processes, especially that they generalize the popular Hidden Markov Models (HMMs) and Kalman filters. Since DBNs are only a subclass of standard Bayesian networks, the structure-based algorithms developed for Bayesian networks can be immediately applied to reasoning with DBNs. Such structure-based algorithms, which are variations on elimination algorithms, take O( N exp( w )) time and space to compute the likelihood of an event, where N is the number of nodes in the network and w is the width of a corresponding elimination order. DBNs, however, pose two specific computational challenges that require DBN-specific solutions. First, DBNs are typically heavily connected, therefore, admitting only elimination orders of high width. Second, even if one can find an elimination order of a reasonable width, one cannot afford the space complexity of O( N exp( w )) since N = nT in this case, where n is the number of variables per time slice and T is the number of time slices in the DBN. For many applications, T is very large, making the space complexity of O( nT exp( w )) unrealistic. Therefore, one of the key challenges of DBNs is to develop efficient algorithms which space complexity is independent of the time span T , leading to what is known as constant-space algorithms. We study one of the main algorithms for achieving this constant-space complexity in this paper, which is based on ” slice-by-slice” elimination orders, and then suggest improvements on it based on new classes of elimination orders. We identify two topological parameters for DBNs and use them to prove a number of tight bounds on the time complexity of algorithms that we study. We also observe (experimentally) that the newly identified elimination orders tend to be better than ones based on general purpose elimination heuristics, such as min-fill. This suggests that constant-space algorithms, such as the ones we study here, should be used on DBNs even when space is not a concern.},
	number = {3},
	journal = {International Journal of Approximate Reasoning},
	author = {Darwiche, A.},
	month = apr,
	year = {2001},
	keywords = {inference, mgpm},
	pages = {161--178},
}

@article{darwiche_differential_2003,
	title = {A {Differential} {Approach} to {Inference} in {Bayesian} {Networks}},
	volume = {50},
	number = {3},
	journal = {Journal of the ACM},
	author = {Darwiche, Adnan},
	year = {2003},
	keywords = {graphical\_model, inference},
	pages = {280--305},
}

@inproceedings{cozman_generalizing_2000,
	title = {Generalizing {Variable} {Elimination} in {Bayesian} {Networks}},
	booktitle = {the {IBERAMIA}/{SBIA} {Workshops} on {Probabilistic} {Reasoning} in {Artificial} {Intelligence}},
	publisher = {Editora Tec Art},
	author = {Cozman, Fabio G.},
	year = {2000},
	note = {event-place: São Paulo},
	keywords = {graphical\_model, inference, elimination},
	pages = {27--32},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	number = {2},
	journal = {Journal of the Royal Statistical Society.},
	author = {Cox, D. R.},
	year = {1972},
	keywords = {reliability, estimation, regression},
	pages = {187--220},
}

@book{cowell_probabilistic_1999,
	address = {Berlin-Heidelberg-New York},
	title = {Probabilistic {Networks} and {Expert} {Systems}},
	publisher = {Springer-Verlag},
	author = {Cowell, R. G. and Dawid, A. P. and Lauritzen, S. L. and Spiegelhalter, D. J.},
	year = {1999},
}

@article{courtois_optimal_2006,
	title = {On the optimal scheduling of periodic tests and maintenance for reliable redundant components},
	volume = {91},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2004.11.013},
	doi = {10.1016/j.ress.2004.11.013},
	abstract = {Periodically, some m of the n redundant components of a dependable system may have to be taken out of service for inspection, testing or preventive maintenance. The system is then constrained to operate with lower (n/m) redundancy and thus with less reliability during these periods. However, more frequent periodic inspections decrease the probability that a component fail undetected in the time interval between successive inspections. An optimal time schedule of periodic preventive operations arises from these two conflicting factors, balancing the loss of redundancy during inspections against the reliability benefits of more frequent inspections. Considering no other factor than this decreased redundancy at inspection time, this paper demonstrates the existence of an optimal interval between inspections, which maximizes the mean time between system failures. By suitable transformations and variable identifications, an analytic closed form expression of the optimum is obtained for the general ( m , n ) case. The optimum is shown to be unique within the ranges of parameter values valid in practice; its expression is easy to evaluate and shown to be useful to analyze and understand the influence of these parameters. Inspections are assumed to be perfect, i.e. they cause no component failure by themselves and leave no failure undetected. In this sense, the optimum determines a lowest bound for the system failure rate that can be achieved by a system of n -redundant components, m of which require for inspection or maintenance recurrent periods of unavailability of length t . The model and its general closed form solution are believed to be new [2] and [5] . Previous work [1] , [4] and [10] had computed optimal values for an estimation of a time average of system unavailability, but by numerical procedures only and with different numerical approximations, other objectives and model assumptions (one component only inspected at a time), and taking into account failures caused by testing itself, repair and demands (see in particular [6] , [7] and [9] ). System properties and practical implications are derived from the closed form analytical expression. Possible extensions of the model are discussed. The model has been applied to the scheduling of the periodic tests of nuclear reactor protection systems.},
	number = {1},
	journal = {Reliability Engineering \& System Safety},
	author = {Courtois, P. and Delsarte, P.},
	month = jan,
	year = {2006},
	keywords = {maintenance, periodic, preventive},
	pages = {66--72},
}

@phdthesis{corset_aide_2003,
	type = {{PhD} {Thesis}},
	title = {Aide à l'optimisation de maintenance à partir de réseaux bayésiens et fiabilité dans un contexte doublement censuré},
	school = {Université Joseph Fourier},
	author = {Corset, Franck},
	year = {2003},
	keywords = {reliability, graphical\_model, maintenance, left\_censoring, right\_censoring},
}

@article{corana_minimizing_1987,
	title = {Minimizing multimodal functions of continuous variables with the "simulated annealing" algorithm},
	volume = {13},
	issn = {0098-3500},
	url = {http://dx.doi.org/10.1145/29380.29864},
	doi = {10.1145/29380.29864},
	abstract = {A new global optimization algorithm for functions of continuous variables is presented, derived from the ” Simulated Annealing” algorithm recently introduced in combinatorial optimization. The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered. The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions. The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.},
	number = {3},
	journal = {ACM Trans. Math. Softw.},
	author = {Corana, A. and Marchesi, M. and Martini, C. and Ridella, S.},
	month = sep,
	year = {1987},
	keywords = {annealing, global, mcmc, optimization, sampling},
	pages = {262--280},
}

@article{cooper_computational_1990,
	title = {The computational complexity of probabilistic inference using bayesian belief networks},
	volume = {42},
	url = {http://dx.doi.org/10.1016/0004-3702(90)90060-D},
	doi = {10.1016/0004-3702(90)90060-D},
	abstract = {Bayesian belief networks provide a natural, efficient method for representing probabilistic dependencies among a set of variables. For these reasons, numerous researchers are exploring the use of belief networks as a knowledge representation in artificial intelligence. Algorithms have been developed previously for efficient probabilistic inference using special classes of belief networks. More general classes of belief networks, however, have eluded efforts to develop efficient inference algorithms. We show that probabilistic inference using belief networks is NP-hard. Therefore, it seems unlikely that an exact algorithm can be developed to perform probabilistic inference efficiently over all classes of belief networks. This result suggests that research should be directed away from the search for a general, efficient probabilistic inference algorithm, and toward the design of efficient special-case, average-case, and approximation algorithms.},
	number = {2-3},
	journal = {Artificial Intelligence},
	author = {Cooper, G. F.},
	month = mar,
	year = {1990},
	keywords = {graphical\_model, inference},
	pages = {393--405},
}

@phdthesis{comelli_modeling_2008,
	type = {Theses},
	title = {Modeling, optimisation, simulation for supply chain tactical planning},
	url = {https://tel.archives-ouvertes.fr/tel-00730176},
	school = {Université Blaise Pascal - Clermont-Ferrand II},
	author = {Comelli, Michael},
	month = jul,
	year = {2008},
	keywords = {cash flow, de la valeur, flux financier, gestion de stock à demande différenciée, inventory management for differentiated demand, partage, Planification tactique des chaînes logistiques, recuit simulé, simulated annealing, Supply chain tactical planning, value sharing},
}

@article{cocozza-thivent_characterization_2006,
	title = {Characterization of the marginal distributions of markov processes used in dynamic reliability},
	volume = {2006},
	abstract = {In dynamic reliability, the evolution of a system is described by a piecewise deterministic Markov process (It ,Xt)t≥0 with state-space E ×R{\textasciicircum}d, where E is finite. The main result of the present paper is the characterization of the marginal distribution of the Markovprocess (It ,Xt)t≥0 at time t, as the unique solution of a set of explicit integro-differential equations, which can be seen as a weak form of the Chapman-Kolmogorov equation. Uniqueness is the difficult part of the result.},
	journal = {Journal of Applied Mathematics and Stochastic Analysis},
	author = {Cocozza-Thivent, C. and Eymard, R. and Mercier, S. and Roussignol, M.},
	year = {2006},
	keywords = {inference, hybrid\_systems},
	pages = {1--18},
}

@book{cocozza-thivent_processus_1997,
	series = {Mathématiques \& {Applications}},
	title = {Processus stochastiques et fiabilité des systèmes},
	number = {28},
	publisher = {Springer},
	author = {Cocozza-Thivent, Christiane},
	year = {1997},
	keywords = {reliability, continuous\_time, markov, process, semi-markov},
}

@article{cobb_operations_2006,
	title = {Operations for inference in continuous {Bayesian} networks with linear deterministic variables},
	volume = {42},
	url = {http://dx.doi.org/10.1016/j.ijar.2005.10.002},
	doi = {10.1016/j.ijar.2005.10.002},
	abstract = {An important class of continuous Bayesian networks are those that have linear conditionally deterministic variables (a variable that is a linear deterministic function of its parents). In this case, the joint density function for the variables in the network does not exist. Conditional linear Gaussian (CLG) distributions can handle such cases when all variables are normally distributed. In this paper, we develop operations required for performing inference with linear conditionally deterministic variables in continuous Bayesian networks using relationships derived from joint cumulative distribution functions. These methods allow inference in networks with linear deterministic variables and non-Gaussian distributions.},
	number = {1-2},
	journal = {PGM'04},
	author = {Cobb, Barry R. and Shenoy, Prakash P.},
	month = may,
	year = {2006},
	keywords = {graphical\_model, inference},
	pages = {21--36},
}

@book{chitra_life_2003,
	title = {Life based maintenance policy for minimum cost},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1182034},
	abstract = {The maintenance of any system can be categorized in two ways: failure based maintenance (corrective maintenance) and life based maintenance (preventive maintenance). The time interval at which the preventive maintenance could be scheduled is dependent on both the life distribution of the subsystems/components and the total cost involved in the maintenance activity. However, the corrective maintenance cannot be avoided when a random failure of a component occurs. The total cost of the maintenance depends on the number of components replaced during the entire operating period of the system and the respective cost involved in maintenance actions. In this paper the preventive maintenance schedule (t/sub 0/) is calculated taking the failure distribution of a component as Weibull. The variation in (t/sub 0/) with respect to the shape parameter (/spl beta/) and characteristic life (/spl alpha/) of the Weibull distribution was studied by the author for a given preventive maintenance cost and corrective maintenance cost. The ratio of the corrective maintenance cost to the preventive maintenance cost (R/sub cp/) has a significant effect on the maintenance schedule (t/sub 0/). Thus the variation of (t/sub 0/) with (R/sub cp/) is addressed in this paper. The ratio is varied between 10 and 1.5. It is found that the (t/sub 0/) value increases as shape parameter increases when the ratio is 10 whereas the (to) value decreases when the ratio is 1.5. This shows that as we decrease the cost ratio (R/sub cp/) the frequency of preventive maintenance decreases with shape parameter. This in turn implies that the frequency of the preventive maintenance schedule can be kept at minimum by bringing down the cost of corrective maintenance. Sensitivity analysis carried out on the (t/sub 0/) values indicates that the variation in the cost of maintenance is about 5 to 8 percent :in all the cases even when the value of (t/sub 0/) is increased. by 50\% over the optimum value. This indicates, while the optimum value of (to) obtained may be of academic interest, one can 'in practice' choose a convenient value near the optimum without appreciably affecting the advantage in terms of cost.},
	author = {Chitra, T.},
	year = {2003},
}

@inproceedings{chraibi_dynamic_2013,
	title = {Dynamic reliability modeling and assessment with {PyCATSHOO}: {Application} to a test case},
	booktitle = {Proceedings of {PSAM}},
	author = {Chraibi, Hassane},
	year = {2013},
	file = {Chraibi2013PyCATSHOO.pdf:/home/roland/Zotero/storage/J5DCY96D/Chraibi2013PyCATSHOO.pdf:application/pdf},
}

@inproceedings{chavira_compiling_2007,
	title = {Compiling {Bayesian} {Networks} {Using} {Variable} {Elimination}},
	booktitle = {Proceedings of the 20th {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI})},
	author = {Chavira, Mark and Darwiche, Adnan},
	year = {2007},
	keywords = {graphical\_model, inference},
	pages = {2443--2449},
}

@article{charbonnier_deterministic_1997,
	title = {Deterministic edge-preserving regularization in computed imaging},
	volume = {6},
	issn = {1057-7149},
	doi = {10.1109/83.551699},
	abstract = {Many image processing problems are ill-posed and must be regularized. Usually, a roughness penalty is imposed on the solution. The difficulty is to avoid the smoothing of edges, which are very important attributes of the image. In this paper, we first give conditions for the design of such an edge-preserving regularization. Under these conditions, we show that it is possible to introduce an auxiliary variable whose role is twofold. First, it marks the discontinuities and ensures their preservation from smoothing. Second, it makes the criterion half-quadratic. The optimization is then easier. We propose a deterministic strategy, based on alternate minimizations on the image and the auxiliary variable. This leads to the definition of an original reconstruction algorithm, called ARTUR. Some theoretical properties of ARTUR are discussed. Experimental results illustrate the behavior of the algorithm. These results are shown in the field of 2D single photon emission tomography, but this method can be applied in a large number of applications in image processing},
	number = {2},
	journal = {IEEE Transactions on Image Processing},
	author = {Charbonnier, P. and Blanc-Feraud, L. and Aubert, G. and Barlaud, M.},
	month = feb,
	year = {1997},
	keywords = {optimization, alternate minimizations, ARTUR, auxiliary variable, computed imaging, Computer applications, computerised tomography, deterministic edge-preserving regularization, discontinuities, edge detection, half-quadratic criterion, Image processing, image processing problems, image reconstruction, Image reconstruction, Markov random fields, medical image processing, minimisation, Minimization methods, Nonlinear equations, reconstruction algorithm, Reconstruction algorithms, single photon emission computed tomography, Smoothing methods, tomography, Tomography, Transforms},
	pages = {298--311},
}

@article{celeux_designing_2006,
	title = {Designing a {Bayesian} network for preventive maintenance from expert opinions in a rapid and reliable way},
	volume = {91},
	url = {http://dx.doi.org/10.1016/j.ress.2005.08.007},
	doi = {10.1016/j.ress.2005.08.007},
	abstract = {In this study, a Bayesian Network (BN) is considered to represent a nuclear plant mechanical system degradation. It describes a causal representation of the phenomena involved in the degradation process. Inference from such a BN needs to specify a great number of marginal and conditional probabilities. As, in the present context, information is based essentially on expert knowledge, this task becomes very complex and rapidly impossible. We present a solution, which consists of considering the BN as a log-linear model on which simplification constraints are assumed. This approach results in a considerable decrease in the number of probabilities to be given by experts. In addition, we give some simple rules to choose the most reliable probabilities. We show that making use of those rules allows to check the consistency of the derived probabilities. Moreover, we propose a feedback procedure to eliminate inconsistent probabilities. Finally, the derived probabilities that we propose to solve the equations involved in a realistic Bayesian network are expected to be reliable. The resulting methodology to design a significant and powerful BN is applied to a reactor coolant sub-component in EDF Nuclear plants in an illustrative purpose.},
	number = {7},
	journal = {Reliability Engineering \& System Safety},
	author = {Celeux, G. and Corset, F. and Lannoy, A. and Ricard, B.},
	month = jul,
	year = {2006},
	keywords = {reliability, graphical\_model, maintenance, expert\_opinion},
	pages = {849--856},
}

@article{casella_rao-blackwellisation_1996,
	title = {Rao-{Blackwellisation} of sampling schemes},
	volume = {83},
	url = {http://dx.doi.org/10.1093/biomet/83.1.81},
	doi = {10.1093/biomet/83.1.81},
	abstract = {This paper proposes a post-simulation improvement for two common Monte Carlo methods, the Accept-Reject and Metropolis algorithms. The improvement is based on a Rao-Blackwellisation method that integrates over the uniform random variables involved in the algorithms, and thus post-processes the standard estimators. We show how the Rao-Blackwellised versions of these algorithms can be implemented and, through examples, illustrate the improvement in variance brought by these new procedures. We also compare the improved version of the Metropolis algorithm with ordinary and Rao-Blackwellised importance sampling procedures for independent and general Metropolis set-ups.},
	number = {1},
	journal = {Biometrika},
	author = {Casella, George and Robert, Christian P.},
	month = mar,
	year = {1996},
	pages = {81--94},
}

@book{castillo_expert_1996,
	address = {New York, NY, USA},
	edition = {Erste},
	title = {Expert {Systems} and {Probabilistic} {Network} {Models}},
	isbn = {0-387-94858-9},
	url = {http://www.worldcat.org/isbn/0387948589},
	abstract = {Artificial intelligence and expert systems have seen a great deal of research in recent years, much of which has been devoted to methods for incorporating uncertainty into models. This book is devoted to providing a thorough and up- to-date survey of this field for researchers and students.},
	publisher = {Springer},
	author = {Castillo, Enrique and Gutiérrez, José M. and Hadi, Ali S.},
	month = dec,
	year = {1996},
	note = {Published: Hardcover},
}

@article{castanier_condition-based_2005,
	title = {A condition-based maintenance policy with non-periodic inspections for a two-unit series system},
	volume = {87},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2004.04.013},
	doi = {10.1016/j.ress.2004.04.013},
	abstract = {This paper considers a condition -based maintenance policy for a two-unit deteriorating system. Each unit is subject to gradual deterioration and is monitored by sequential non-periodic inspections. It can be maintained by good as new preventive or corrective replacements. Every inspection or replacement entails a set-up cost and a component-specific unit cost but if actions on the two components are combined, the set-up cost is charged only once. A parametric maintenance decision framework is proposed to coordinate inspection/replacement of the two components and minimize the long-run maintenance cost of the system. A stochastic model is developed on the basis of the semi-regenerative properties of the maintained system state and the associated cost model is used to assess and optimize the performance of the maintenance model. Numerical experiments emphasize the interest of a control of the operation groupings.},
	number = {1},
	journal = {Reliability Engineering \& System Safety},
	author = {Castanier, B. and Grall, A. and Berenguer, C.},
	month = jan,
	year = {2005},
	keywords = {maintenance, preventive, condition-based},
	pages = {109--120},
}

@inproceedings{carer_new_2002,
	title = {A new method for reliability assessment of electrical power supplies with standby redundancies},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Probabilistic} {Methods} {Applied} to {Power} {Systems} ({PMAPS})},
	author = {Carer, P and Bellvis, J and Bouissou, M and Domergue, Jean and Pestourie, J},
	year = {2002},
	file = {Carer_al2002PowerSupplies.pdf:/home/roland/Zotero/storage/TNTJD9QL/Carer_al2002PowerSupplies.pdf:application/pdf},
}

@inproceedings{broy_dynamic_2011,
	address = {Beijing, China},
	title = {Dynamic {Bayesian} {Networks} for assessing reliability of hybrid systems},
	copyright = {All rights reserved},
	booktitle = {Proceedings of {Mathematical} {Methods} in {Reliability}},
	author = {Broy, Perrine and Donat, Roland and Chraibi, Hassane and Dijoux, Yann and Bérenguer, Christophe},
	year = {2011},
	keywords = {heated\_tank, rbd},
	pages = {252--257},
}

@inproceedings{broy_new_2013,
	address = {Stellenbosch, South Africa},
	title = {A new methodology to model and assess reliability of large dynamic hybrid systems},
	copyright = {All rights reserved},
	booktitle = {Proceedings of {Mathematical} {Methods} in {Reliability}},
	author = {Broy, Perrine and Chraibi, Hassane and Donat, Roland and Bérenguer, Christophe and Dijoux, Yann},
	year = {2013},
	keywords = {ash},
	pages = {28--32},
}

@book{bunks_engineering_2012,
	title = {Engineering and scientific computing with {Scilab}},
	publisher = {Springer Science \& Business Media},
	author = {Bunks, Carey and Chancelier, J-P and Delebecque, François and Goursat, M and Nikoukhah, R and Steer, S and Gomez, Claude},
	year = {2012},
}

@inproceedings{broy_using_2011,
	address = {Troyes, France},
	title = {Using {Dynamic} {Bayesian} {Networks} to solve a dynamic reliability problem},
	copyright = {All rights reserved},
	booktitle = {Annual {Conference} of the {European} {Safety} and {Reliability} {Association}},
	author = {Broy, Perrine and Chraibi, Hassane and Donat, Roland},
	month = sep,
	year = {2011},
	keywords = {heated\_tank, rbd},
	pages = {335--341},
}

@book{bremaud_markov_2001,
	edition = {Corrected},
	title = {Markov {Chains}: {Gibbs} {Fields}, {Monte} {Carlo} {Simulation}, and {Queues}},
	isbn = {0-387-98509-3},
	url = {http://www.worldcat.org/isbn/0387985093},
	abstract = {This book discusses both the theory and applications of Markov chains. Theauthor studies both discrete-time and continuous-time chains and connectedtopics such as finite Gibbs fields, non-homogeneous Markov chains, discretetime regenerative processes, Monte Carlo simulation, simulated annealing, andqueueing networks are also developed in this accessible and self-containedtext. The text is firstly an introduction to the theory of stochasticprocesses at the undergraduate or beginning graduate level. Its primaryobjective is to initiate the student to the art of stochastic modelling. Thetreatment is mathematical, with definitions, theorems, proofs and a number ofclassroom examples which help the student to fully grasp the content of themain results. Problems of varying difficulty are proposed at the close of eachchapter. The text is motivated by significant applications and progressivelybrings the student to the borders of contemporary research. Students andresearchers in operations research and electrical engineering as well as inphysics, biology and the social sciences will find this book of interest.},
	publisher = {Springer-Verlag New York Inc.},
	author = {Bremaud, P.},
	month = feb,
	year = {2001},
	keywords = {markov, chain},
}

@article{bracquemond_survey_2003,
	title = {A survey on discrete lifetime distributions},
	volume = {10},
	number = {1},
	journal = {International Journal on Reliability, Quality, and Safety Engineering},
	author = {Bracquemond, C. and Gaudoin, O.},
	year = {2003},
	keywords = {reliability, discrete\_time},
	pages = {69--98},
}

@article{brameret_automated_2015,
	title = {Automated generation of partial {Markov} chain from high level descriptions},
	volume = {139},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S095183201500054X},
	doi = {http://dx.doi.org/10.1016/j.ress.2015.02.009},
	abstract = {Abstract We propose an algorithm to generate partial Markov chains from high level implicit descriptions, namely AltaRica models. This algorithm relies on two components. First, a variation on Dijkstra׳s algorithm to compute shortest paths in a graph. Second, the definition of a notion of distance to select which states must be kept and which can be safely discarded. The proposed method solves two problems at once. First, it avoids a manual construction of Markov chains, which is both tedious and error prone. Second, up the price of acceptable approximations, it makes it possible to push back dramatically the exponential blow-up of the size of the resulting chains. We report experimental results that show the efficiency of the proposed approach.},
	journal = {Reliability Engineering \& System Safety},
	author = {Brameret, P.-A. and Rauzy, A. and Roussel, J.-M.},
	year = {2015},
	keywords = {Model Based Safety Assessment},
	pages = {179 -- 187},
}

@inproceedings{boyen_tractable_1998,
	address = {Madison, Wisconsin},
	title = {Tractable {Inference} for {Complex} {Stochastic} {Processes}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.6648},
	abstract = {The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief state—a probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs),...},
	booktitle = {Uncertainty in artificial {Intelligence}},
	author = {Boyen, X. and Koller, D.},
	month = jul,
	year = {1998},
	keywords = {graphical\_model, inference},
	pages = {32--42},
}

@phdthesis{bracquemond_modelisation_2001,
	type = {{PhD} {Thesis}},
	title = {Modélisation stochastique du {Vieillissement} en temps discret},
	school = {Institut National Politechnique de Grenoble},
	author = {Bracquemond, Cyril},
	month = oct,
	year = {2001},
	keywords = {reliability, discrete\_time},
}

@inproceedings{bouissou_comparaison_2006,
	title = {Comparaison des langages de modélisation {AltaRica} et {FIGARO}},
	booktitle = {Actes du 14ème congres de fiabilité et maintenabilité de l’{IMdR} (λμ14)},
	author = {Bouissou, Marc and Seguin, Christel},
	year = {2006},
}

@article{bouissou_knowledge_1991,
	title = {Knowledge modelling and reliability processing: {Presentation} of the {FIGARO} language and associated tools},
	journal = {IFAC/IFIP/EWICS/SRE Symposium},
	author = {Bouissou, Marc and Bouhadana, Henri and Bannelier, Marc and Villatte, Nathalie},
	year = {1991},
	pages = {69--75},
}

@article{bouissou_new_2003,
	title = {A new formalism that combines advantages of fault-trees and {Markov} models: {Boolean} {Logic} {Driven} {Markov} {Processes}},
	volume = {82},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/S0951-8320(03)00143-1},
	doi = {10.1016/S0951-8320(03)00143-1},
	abstract = {This paper introduces a modeling formalism that enables the analyst to combine concepts inherited from fault trees and Markov models in a new way. We call this formalism Boolean logic Driven Markov Processes (BDMP). It has two advantages over conventional models used in dependability assessment: it allows the definition of complex dynamic models while remaining nearly as readable and easy to build as fault-trees, and it offers interesting mathematical properties, which enable an efficient processing for BDMP that are equivalent to Markov processes with huge state spaces. We give a mathematical definition of BDMP, the demonstration of their properties, and several examples to illustrate how powerful and easy to use they are. From a mathematical point of view, a BDMP is nothing more than a certain way to define a global Markov process, as the result of several elementary processes which can interact in a given manner. An extreme case is when the processes are independent. Then we simply have a fault-tree, the leaves of which are associated to independent Markov processes.},
	number = {2},
	journal = {Reliability Engineering \& System Safety},
	author = {Bouissou, Marc and Bon, Jean-Louis},
	month = nov,
	year = {2003},
	keywords = {reliability, dynamic\_models, fault\_trees},
	pages = {149--163},
}

@inproceedings{bouissou_automated_2005,
	title = {Automated dependability analysis of complex systems with the {KB3} workbench: the experience of {EDF} {R}\&{D}},
	booktitle = {Proceedings of the {International} {Conference} on {Energy} and {Environment} ({CIEM})},
	author = {Bouissou, Marc},
	year = {2005},
}

@inproceedings{bouissou_generalization_2007,
	title = {A {Generalization} of {Dynamic} {Fault} {Trees} through {Boolean} logic {Driven} {Markov} {Processes} ({BDMP})®},
	booktitle = {Proceedings of {ESREL} 2007},
	author = {Bouissou, Marc},
	year = {2007},
	note = {event-place: Stavanger, Norway},
	keywords = {fault\_trees, bdmp},
}

@book{bouissou_gestion_2008,
	title = {Gestion de la {Complexité} dans les Études {Quantitatives} de {Sûreté} de {Fonctionnement} de {Systèmes}},
	isbn = {2-7430-1093-2},
	publisher = {Paris : TEC et DOC},
	author = {Bouissou, Marc},
	year = {2008},
}

@inproceedings{bouissou_boolean_2002,
	title = {Boolean {Logic} {Driven} {Markov} {Processes}: a powerful new formalism for specifying and solving very large {Markov} models},
	booktitle = {Proceedings of the 6th {Probabilistic} {Safety} {Assessment} and {Management} {Conference}},
	author = {Bouissou, Marc},
	month = jun,
	year = {2002},
	note = {event-place: Puerto Rico},
}

@inproceedings{bouissou_bdmp_2008,
	title = {{BDMP} ({Boolean} logic {Driven} {Markov} {Processes}), as an alternative to {Event} {Trees}},
	booktitle = {{ESREL} 2008},
	author = {Bouissou, Marc},
	month = sep,
	year = {2008},
	note = {event-place: Valencia, Spain},
	keywords = {bdmp, event-trees},
}

@inproceedings{bouissou_digraphs_2010,
	address = {La Rochelle, France},
	title = {Digraphs : le retour},
	booktitle = {17th {IMdR} annual {Conference} on {Dependability} ({Lambda}-{Mu})},
	author = {Bouissou, Marc},
	month = oct,
	year = {2010},
}

@book{bouissou_manuel_nodate,
	title = {Manuel de référence du langage {FIGARO}},
	publisher = {EDF R\&D - Département MRI},
	author = {Bouissou, Marc},
	keywords = {figaro},
}

@incollection{bouissou_two_2010,
	title = {Two computational methods for performing availability analysis of power-systems},
	publisher = {CRC Press},
	author = {Bouissou, Marc},
	editor = {Soares, Carlos G.},
	year = {2010},
	keywords = {bdmp},
}

@article{bouillaut_virmalab_2011,
	title = {{VirMaLab} — atelier virtuel de maintenance: un outil d'aide à la décision pour l'optimisation des politiques de maintenance},
	volume = {27},
	issn = {1951-6614},
	url = {http://dx.doi.org/10.1007/s13547-011-0022-4},
	doi = {10.1007/s13547-011-0022-4},
	abstract = {Cet article introduit une approche générique nommée VirMaLab (pour atelier virtuel de maintenance) permettant de développer des modèles de maintenance, basée sur la fiabilité, de systèmes multicomposants et multiétats. S'appuyant sur le formalisme des modèles graphiques probabilistes (MGP) [ou réseaux bayésiens (RB)], cette approche stochastique modélise aussi bien les processus de dégradation caractérisant le système étudié (une approche semimarkovienne originale sera proposée pour cela) que les procédures de diagnostic mises en oeuvre et les actions de maintenance qui en découlent. L'intégration de coûts (d'exploitation, d'indisponibilité, de maintenance{\textbackslash}ldots) est également possible. À titre d'illustration, deux applications de cette démarche sont introduites, dédiées à la maintenance du rail. La première s'intéresse à l'optimisation du compromis régénération/réparation pour le tronçon central du RER A, tandis que la seconde vise à permettre l'évaluation et la comparaison de différentes stratégies de maintenance pour la prévention de la rupture du rail dans un contexte de modernisation des automatismes de lignes à roulement fer du métro parisien.},
	number = {4},
	journal = {Recherche Transports Sécurité},
	author = {Bouillaut, L. and François, O. and Aknin, P. and Donat, R. and Bondeux, S. and Dubois, S.},
	year = {2011},
	pages = {241--257},
}

@inproceedings{bouillaut_estimation_2009,
	title = {Estimation of {Multi}-component {Systems} reliability: {Comparison} of two {Graphical} {Model} {Approaches}},
	copyright = {All rights reserved},
	booktitle = {proceedings of the 13th {IFAC} {Symposium} on {Information} {Control} {Problems} in {Manufacturing}},
	author = {Bouillaut, Laurent and Donat, Roland and Neji, Abdelmoez and Aknin, Patrice},
	month = jun,
	year = {2009},
	note = {event-place: Moscow, Russia},
	keywords = {reliability, graphical\_model, duration},
	pages = {1733--1738},
}

@article{boudali_discrete-time_2005,
	title = {A discrete-time {Bayesian} network reliability modeling and analysis framework},
	volume = {87},
	url = {http://dx.doi.org/10.1016/j.ress.2004.06.004},
	doi = {10.1016/j.ress.2004.06.004},
	abstract = {Dependability tools are becoming an indispensable tool for modeling and analyzing (critical) systems. However the growing complexity of such systems calls for increasing sophistication of these tools. Dependability tools need to not only capture the complex dynamic behavior of the system components, but they must be also easy to use, intuitive, and computationally efficient. In general, current tools have a number of shortcomings including lack of modeling power, incapacity to efficiently handle general component failure distributions, and ineffectiveness in solving large models that exhibit complex dependencies between their components. We propose a novel reliability modeling and analysis framework based on the Bayesian network (BN) formalism. The overall approach is to investigate timed Bayesian networks and to find a suitable reliability framework for dynamic systems. We have applied our methodology to two example systems and preliminary results are promising. We have defined a discrete-time BN reliability formalism and demonstrated its capabilities from a modeling and analysis point of view. This research shows that a BN based reliability formalism is a powerful potential solution to modeling and analyzing various kinds of system components behaviors and interactions. Moreover, being based on the BN formalism, the framework is easy to use and intuitive for non-experts, and provides a basis for more advanced and useful analyses such as system diagnosis.},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Boudali, H. and Dugan, J. B.},
	month = mar,
	year = {2005},
	keywords = {reliability, graphical\_model, discrete\_time, modelling},
	pages = {337--349},
}

@article{bon_algorithm_1994,
	title = {An algorithm in order to implement reliability exponential approximations},
	volume = {43},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/0951832094900302},
	doi = {http://dx.doi.org/10.1016/0951-8320(94)90030-2},
	abstract = {Calculation of the reliability of large repairable systems is discussed here. We first present the background: a rule-based model and exponential approximations. The rule-based model allows us to described very large and complex systems. The exponential approximations simplify the reliability computation, reducing it to the computation of some mean sojourn times, and a probability: the probability of going from the perfect-operation state to the system-failure state. In order to compute this probability, we propose an algorithm with the following characteristics: it assumes the process is a semi-Markov process, it uses an exploration of graph circuitless sequences, it allows us to use a stop criterion, it is not suited for systems with a large number of active redundancies. We tested this algorithm on both theoretical and real cases, and the results were satisfactory.},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Bon, J. L. and Collet, J.},
	year = {1994},
	pages = {263 -- 268},
}

@techreport{bondeux_voie_2006,
	title = {La {Voie} de la {RATP}},
	institution = {RATP EST Voie},
	author = {Bondeux, S. and Mielnik, C. and Burnet, P. and Clément, H. and Etienne, J. Y. and Grébille, R. and Lévy, D. and Naszalyi, A.},
	year = {2006},
	keywords = {railway},
}

@inproceedings{bolvin_methode_2011,
	address = {Lyon, France},
	title = {Une méthode d'estimation de la probabilité des accidents majeurs de barrages : la méthode du noeud papillon},
	url = {http://hal-ineris.ccsd.cnrs.fr/ineris-00973632},
	booktitle = {Colloque technique {CFBR} / {AFEID} ”{Pratique} des études de dangers des barrages”},
	publisher = {CFBR},
	author = {Bolvin, Christophe and Balouin, Thibault and Vallee, Agnès and Flauw, Yann},
	month = nov,
	year = {2011},
	keywords = {BARRIERE, FREQUENCE, NOEUD PAPILLON, PROBABILITE},
	pages = {33--40},
}

@article{bobbio_improving_2001,
	title = {Improving the analysis of dependable systems by mapping fault trees into {Bayesian} networks},
	volume = {71},
	url = {http://dx.doi.org/10.1016/S0951-8320(00)00077-6},
	doi = {10.1016/S0951-8320(00)00077-6},
	abstract = {Bayesian Networks (BN) provide a robust probabilistic method of reasoning under uncertainty. They have been successfully applied in a variety of real-world tasks but they have received little attention in the area of dependability. The present paper is aimed at exploring the capabilities of the BN formalism in the analysis of dependable systems. To this end, the paper compares BN with one of the most popular techniques for dependability analysis of large, safety critical systems, namely Fault Trees (FT). The paper shows that any FT can be directly mapped into a BN and that basic inference techniques on the latter may be used to obtain classical parameters computed from the former (i.e. reliability of the Top Event or of any sub-system, criticality of components, etc). Moreover, by using BN, some additional power can be obtained, both at the modeling and at the analysis level. At the modeling level, several restrictive assumptions implicit in the FT methodology can be removed and various kinds of dependencies among components can be accommodated. At the analysis level, a general diagnostic analysis can be performed. The comparison of the two methodologies is carried out by means of a running example, taken from the literature, that consists of a redundant multiprocessor system.},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Bobbio, A. and Portinale, L. and Minichino, M. and Ciancamerla, E.},
	month = mar,
	year = {2001},
	keywords = {graphical\_model, fault\_trees, modelling},
	pages = {249--260},
}

@article{bezanson_julia:_2012,
	title = {Julia: {A} {Fast} {Dynamic} {Language} for {Technical} {Computing}},
	volume = {abs/1209.5145},
	url = {http://arxiv.org/abs/1209.5145},
	journal = {CoRR},
	author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B. and Edelman, Alan},
	year = {2012},
}

@article{bechta_dugan_dynamic_1992,
	title = {Dynamic fault-tree models for fault-tolerant computer systems},
	volume = {41},
	issn = {0018-9529},
	doi = {10.1109/24.159800},
	abstract = {Reliability analysis of fault-tolerant computer systems for critical applications is complicated by several factors. Systems designed to achieve high levels of reliability frequently employ high levels of redundancy, dynamic redundancy management, and complex fault and error recovery techniques. This paper describes dynamic fault-tree modeling techniques for handling these difficulties. Three advanced fault-tolerant computer systems are described: a fault-tolerant parallel processor, a mission avionics system, and a fault-tolerant hypercube. Fault-tree models for their analysis are presented. HARP (Hybrid Automated Reliability Predictor) is a software package developed at Duke University and NASA Langley Research Center that can solve those fault-tree models},
	number = {3},
	journal = {Reliability, IEEE Transactions on},
	author = {Bechta Dugan, J. and Bavuso, Salvatore J. and Boyd, M.A.},
	month = sep,
	year = {1992},
	keywords = {reliability, Aerospace electronics, Application software, Computer errors, Concurrent computing, dynamic fault-tree modeling, dynamic redundancy management, error recovery, fault tolerant computing, Fault tolerant systems, fault-tolerant computer systems, HARP, hypercube, hypercube networks, Hypercubes, mission avionics system, NASA, parallel architectures, parallel processor, Predictive models, redundancy, Redundancy, reliability theory, software package, Software packages},
	pages = {363--377},
	file = {Dugan_al1992DFT.pdf:/home/roland/Zotero/storage/S3NTHEDE/Dugan_al1992DFT.pdf:application/pdf},
}

@article{bar-noy_efficient_2004,
	title = {Efficient algorithms for periodic scheduling},
	volume = {45},
	issn = {13891286},
	url = {http://dx.doi.org/10.1016/j.comnet.2003.12.017},
	doi = {10.1016/j.comnet.2003.12.017},
	abstract = {In a perfectly periodic schedule , time is divided into time slots, and each client gets a slot precisely every predefined number of time slots. The input to a schedule design algorithm is a frequency request for each client, and its task is to construct a perfectly periodic schedule that matches the requests as "closely" as possible. The quality of the schedule is measured by the ratios between the requested frequency and the allocated frequency for each client (either by the weighted average or by the maximum of these ratios over all clients). Perfectly Periodic schedules enjoy maximal fairness, and are very useful in many contexts of asymmetric communication, e.g., push systems and Bluetooth networks. However, finding an optimal perfectly periodic schedule is NP-hard. Tree scheduling is a methodology for developing perfectly periodic schedules based on hierarchical round-robin, where the hierarchy is represented by trees. In this paper, we study algorithms for constructing scheduling trees. First, we give optimal (exponential time) algorithms for both the average and the maximum measures. Second, we present a few efficient heuristic algorithms for generating schedule trees, based on the structure and the analysis of the optimal algorithms. Simulation results indicate that some of these heuristics produce excellent schedules in practice, sometimes even beating the best known non-perfectly periodic scheduling algorithms.},
	number = {2},
	journal = {Computer Networks},
	author = {Bar-Noy, A. and Dreizin, V. and Pattshamir, B.},
	month = jun,
	year = {2004},
	keywords = {maintenance, periodic, preventive},
	pages = {155--173},
}

@article{barlow_optimum_1960,
	title = {Optimum {Preventive} {Maintenance} {Policies}},
	volume = {8},
	url = {http://dx.doi.org/10.1287/opre.8.1.90},
	doi = {10.1287/opre.8.1.90},
	abstract = {Two types of preventive maintenance policies are considered. A policy is defined to be optimum if it maximizes "limiting efficiency," i.e., fractional amount of up-time over long intervals. Elementary renewal theory is used to obtain optimum policies. The optimum policies are determined, in each case, as unique solutions of certain integral equations depending on the failure distribution. It is shown that both solutions are also minimum cost solutions when the proper identifications are made. The two optimum policies are compared under certain restrictions. 10.1287/opre.8.1.90},
	number = {1},
	journal = {Operations Research},
	author = {Barlow, R. and Hunter, L.},
	month = jan,
	year = {1960},
	keywords = {maintenance, preventive},
	pages = {90--100},
}

@article{barkan_railroad_2003,
	title = {Railroad derailment factors affecting hazardous materials transportation risk},
	volume = {1825},
	issn = {0361-1981},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Barkan, C. P. L. and Dick, C. T. and Anderson, R.},
	year = {2003},
	keywords = {railway, reliability},
	pages = {64--74},
}

@article{berenguer_mathematical_2008,
	title = {On the mathematical condition-based maintenance modelling for continuously deteriorating systems},
	volume = {6},
	number = {2},
	journal = {International Journal of Materials and Structural Reliability},
	author = {Bérenguer, C.},
	month = sep,
	year = {2008},
	keywords = {reliability, maintenance, condition-based},
	pages = {133--151},
}

@incollection{barbu_nonparametric_2006,
	title = {Nonparametric {Estimation} for {Failure} {Rate} {Functions} of {Discrete} {Time} semi-{Markov} {Processes}},
	url = {http://dx.doi.org/10.1007/0-387-26023-4_5},
	booktitle = {Probability, {Statistics} and {Modelling} in {Public} {Health}},
	publisher = {Springer U.S.},
	author = {Barbu, Vlad and Limnios, Nikolaos},
	year = {2006},
	doi = {10.1007/0-387-26023-4_5},
	keywords = {markov, process, discrete\_time, learning},
	pages = {53--72},
}

@article{barbu_discrete_2004,
	title = {Discrete time semi-{Markov} processes for reliability and survival analysis},
	volume = {33},
	number = {11},
	journal = {Communication in Statistics - Theory and Methods},
	author = {Barbu, V. and Boussemart, M. and Limnios, N.},
	year = {2004},
	keywords = {reliability, markov, process, discrete\_time},
	pages = {2833--2868},
}

@phdthesis{ayadi_optimization_2013,
	type = {Theses},
	title = {Optimization of {Preventive} {Maintenance} {Policies} in a context of modelisation by probabilistic graphical models},
	url = {https://tel.archives-ouvertes.fr/tel-01274312},
	school = {Université Paris-Est},
	author = {Ayadi, Inès},
	month = aug,
	year = {2013},
	keywords = {Graphical probabilistic models, Metaheuristic, Métaheuristique, Modèles graphiques probabilistes, Optimisation, Optimization},
	file = {Ayadi2013Th_Optimisation.pdf:/home/roland/Zotero/storage/LZEUHDGX/Ayadi2013Th_Optimisation.pdf:application/pdf},
}

@book{aven_stochastic_1999,
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {Stochastic {Models} in {Reliability}},
	publisher = {Springer},
	author = {Aven, Terje and Jensen, Uwe},
	year = {1999},
	keywords = {reliability, maintenance, optimization, stochastic},
}

@inproceedings{aussem_conservative_2008,
	title = {A {Conservative} {Feature} {Subset} {Selection} {Algorithm} with {Missing} {Data}},
	booktitle = {Proceedings of the 8th {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {Aussem, Alexandre and Rodrigues de Morais, Sergio},
	month = dec,
	year = {2008},
	note = {event-place: Pisa, Italy},
	keywords = {graphical\_model, structure\_learning},
	pages = {725--730},
}

@article{atwood_binomial_1986,
	title = {The {Binomial} {Failure} {Rate} {Common} {Cause} {Model}},
	volume = {28},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1986.10488115},
	doi = {10.1080/00401706.1986.10488115},
	abstract = {The binomial failure rate model can be used to estimate the rate of simultaneous failure of more than one component of a system. This review article defines the model, shows how quantities of interest are estimated, and presents checks for lack of fit. Emphasis is on the statistical and computational ideas rather than on the details of the method.},
	number = {2},
	journal = {Technometrics},
	author = {Atwood, Corwin L.},
	year = {1986},
	pages = {139--148},
}

@article{arulampalam_tutorial_2002,
	title = {A tutorial on particle filters for online nonlinear/non-{Gaussian} {Bayesian} tracking},
	volume = {50},
	issn = {1053-587X},
	url = {http://dx.doi.org/10.1109/78.978374},
	doi = {10.1109/78.978374},
	abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
	number = {2},
	journal = {Signal Processing, IEEE Transactions on},
	author = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
	month = feb,
	year = {2002},
	keywords = {mcmc},
	pages = {174--188},
}

@article{arias_operating_2007,
	title = {Operating with potentials of discrete variables},
	volume = {46},
	url = {http://dx.doi.org/10.1016/j.ijar.2006.12.002},
	doi = {10.1016/j.ijar.2006.12.002},
	abstract = {A potential is a function that maps each configuration of a set of variables onto a real number. In the context of probabilistic graphical models, every family of probability distributions and every utility function is a potential, and the process of inference gives rise to new potentials. In principle, potentials defined on discrete variables might be represented as multidimensional arrays, but in practice they are implemented as linear arrays. In this paper we prove that in case of large potentials, the cost of retrieving their elements is significantly higher than the cost of multiplying, maximizing, or summing them. For this reason, we present an alternative algorithm that sequentially retrieves the elements of a potential implemented as a linear array without having to multiply the coordinates of each configuration by the offsets. We analyze theoretically and empirically the computational savings of this algorithm when applied to potential operations, such as marginalization, addition, multiplication, division, and conditioning. We also discuss the savings that can be obtained by multiplying several potentials at the same time, and by integrating the multiplication and marginalization of potentials.},
	number = {1},
	journal = {Special Section: Random Sets and Imprecise Probabilities (Issues in Imprecise Probability)},
	author = {Arias, M. and Díez, F. J.},
	month = sep,
	year = {2007},
	keywords = {graphical\_model, inference},
	pages = {166--187},
}

@article{apostolakis_foundations_1987,
	title = {The foundations of models of dependence in probabilistic safety assessment},
	volume = {18},
	issn = {0143-8174},
	url = {http://www.sciencedirect.com/science/article/pii/0143817487900977},
	doi = {http://dx.doi.org/10.1016/0143-8174(87)90097-7},
	number = {3},
	journal = {Reliability Engineering},
	author = {Apostolakis, George and Moieni, Parviz},
	year = {1987},
	pages = {177 -- 195},
}

@article{antoni_modelisation_2005,
	title = {Modélisation du lien économique entre entretien et régénération : voie courante},
	volume = {annales 2004/2005},
	journal = {Direction de la stratégie SNCF},
	author = {Antoni, Mark and Bernard, Franck},
	month = may,
	year = {2005},
	keywords = {railway, maintenance, optimization, sncf, weibull},
	pages = {75--87},
}

@article{andrieu_introduction_2003,
	title = {An {Introduction} to {MCMC} for {Machine} {Learning}},
	volume = {50},
	url = {http://www.cs.princeton.edu/courses/archive/spring06/cos598C/papers/AndrieuFreitasDoucetJordan2003.pdf},
	doi = {10.1023/A:1020281327116},
	abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
	number = {1},
	journal = {Machine Learning},
	author = {Andrieu, C. and De Freitas, N. and Doucet, A. and Jordan, M. I.},
	month = jan,
	year = {2003},
	keywords = {mcmc},
	pages = {5--43},
}

@article{anderson_railroad_2004,
	title = {Railroad {Accident} {Rates} for {Use} in {Transportation} {Risk} {Analysis}},
	volume = {1863},
	issn = {0361-1981},
	url = {http://dx.doi.org/10.3141/1863-12},
	doi = {10.3141/1863-12},
	abstract = {Annual safety statistics published by FRA provide train accident counts for various groupings, such as railroad, accident type, cause, track type and class, train length, and speed. However, hazardous materials transportation risk analysis often requires more detailed accident rate statistics for specific combinations of these groupings. The statistics that are presented enable more precise determination of the probability that Class I and non-Class I railroad freight trains will be involved in an accident on various classes of main-line track. An increase in the overall accident rate from 1997 to 2001 can be largely attributed to the increase in yard accidents. During that time, the main-line derailment rate for Class I freight trains remained nearly constant. Track class-specific derailment rates for Class I main-line freight trains show two orders of magnitude difference between the lowest and highest FRA track classes. Depending on the risk analysis question, accounting for these differences in rates will often be important in developing an accurate estimate of risk over the length of a route or at particular locations along a route. A sensitivity analysis suggests that the distribution of freight train miles by FRA track class may have changed since a study conducted by the Association of American Railroads in the early 1990s. More up-to-date estimates of track class-specific accident rates would require new data on this distribution.},
	number = {-1},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Anderson, R. and Barkan, C.},
	month = jan,
	year = {2004},
	keywords = {railway, reliability},
	pages = {88--98},
}

@article{alvarez_smoothed_2005,
	title = {Smoothed nonparametric estimation in window censored semi-{Markov} processes},
	volume = {131},
	url = {http://dx.doi.org/10.1016/j.jspi.2004.03.003},
	doi = {10.1016/j.jspi.2004.03.003},
	abstract = {Consider a process that jumps among a finite set of states, with random times spent in between. In semi-Markov processes transitions follow a Markov chain and the sojourn distributions depend only on the connecting states. Suppose that the process started far in the past, achieving stationary. We consider non-parametric estimation by modelling the log-hazard of the sojourn times through linear splines; and we obtain maximum penalized likelihood estimators when data consist of several i.i.d. windows. We prove consistency using Grenander's method of sieves.},
	number = {2},
	journal = {Journal of Statistical Planning and Inference},
	author = {Alvarez, Enrique E.},
	month = may,
	year = {2005},
	keywords = {estimation, continuous\_time, process, semi-markov, splines, window\_censoring},
	pages = {209--229},
}

@article{alrabghi_state_2015,
	title = {State of the art in simulation-based optimisation for maintenance systems},
	volume = {82},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/S0360835214004513},
	doi = {https://doi.org/10.1016/j.cie.2014.12.022},
	abstract = {Recently, more attention has been directed towards improving and optimising maintenance in manufacturing systems using simulation. This paper aims to report the state of the art in simulation-based optimisation of maintenance by systematically classifying the published literature and outlining main trends in modelling and optimising maintenance systems. The authors investigate application areas and published real case studies as well as researched maintenance strategies and policies. Much of the research in this area is focusing on preventive maintenance and optimising preventive maintenance frequency that will lead to the minimum cost. Discrete event simulation was the most reported technique to model maintenance systems whereas modern optimisation methods such as Genetic Algorithms was the most reported optimisation method in the literature. On this basis, the paper identifies the current gaps and discusses future prospects. Further research can be done to develop a framework that guides the experimenting process with different maintenance strategies and policies. More real case studies can be conducted on multi-objective optimisation and condition based maintenance especially in a production context.},
	journal = {Computers \& Industrial Engineering},
	author = {Alrabghi, Abdullah and Tiwari, Ashutosh},
	year = {2015},
	keywords = {Maintenance, Optimisation, Modelling, Review, Simulation},
	pages = {167 -- 182},
	file = {Alrabghi_Tiwari2015SA_Maintenance_Simulation.pdf:/home/roland/Zotero/storage/EKVFPUVZ/Alrabghi_Tiwari2015SA_Maintenance_Simulation.pdf:application/pdf},
}

@article{aldrich_r.._1997,
	title = {R.{A}. {Fisher} and the making of maximum likelihood 1912-1922},
	volume = {12},
	abstract = {In 1922 R. A. Fisher introduced the method of maximum likelihood. He first presented the numerical procedure in 1912. This paper considers Fisher's changing justifications for the method, the concepts he developed around it (including likelihood, sufficiency, efficiency and information) and the approaches he discarded (including inverse probability).},
	number = {3},
	journal = {Statistical Science},
	author = {Aldrich, John},
	year = {1997},
	keywords = {maximum\_likelihood, review},
	pages = {162--176},
}

@article{aldemir_survey_2013,
	title = {A survey of dynamic methodologies for probabilistic safety assessment of nuclear power plants},
	volume = {52},
	issn = {0306-4549},
	url = {http://www.sciencedirect.com/science/article/pii/S030645491200271X},
	doi = {http://dx.doi.org/10.1016/j.anucene.2012.08.001},
	abstract = {Dynamic methodologies for probabilistic safety assessment (PSA) are defined as those which use a time-dependent phenomenological model of system evolution along with its stochastic behavior to account for possible dependencies between failure events. Over the past 30 years, numerous concerns have been raised in the literature regarding the capability of the traditional static modeling approaches such as the event-tree/fault-tree methodology to adequately account for the impact of process/hardware/software/firmware/human interactions on the stochastic system behavior. A survey of the types of dynamic \{PSA\} methodologies proposed to date is presented, as well as a brief summary of an example application for the \{PSA\} modeling of a digital feedwater control system of an operating pressurized water reactor. The use of dynamic methodologies for \{PSA\} modeling of passive components and phenomenological uncertainties are also discussed.},
	journal = {Annals of Nuclear Energy},
	author = {Aldemir, Tunc},
	year = {2013},
	keywords = {Probabilistic safety assessment},
	pages = {113 -- 124},
}

@article{al-dabbagh_reliability_2010,
	title = {Reliability modeling of networked control systems using dynamic flowgraph methodology},
	volume = {95},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832010001262},
	doi = {http://dx.doi.org/10.1016/j.ress.2010.05.005},
	abstract = {The recent trend in deploying communication networks in digital control systems to form Networked Control Systems (NCSs) brings the need for utilizing dynamic methods to assess the reliability of these systems. The methods should be able to capture the behaviour and interaction of the hardware, the software and the communication network in the NCSs. They should also be able to incorporate time dependency and multi-state behaviour. In this paper, it is demonstrated how the Dynamic Flowgraph Methodology (DFM) can be a promising method to fulfill these requirements. The behaviour and the effect of the communication network on \{NCS\} performance is emphasized. The information provided by analysis of the implemented model facilitates the improvement of the control system performance.},
	number = {11},
	journal = {Reliability Engineering \& System Safety},
	author = {Al-Dabbagh, Ahmad W. and Lu, Lixuan},
	year = {2010},
	keywords = {Reliability assessment},
	pages = {1202 -- 1209},
	file = {AlDabbagh_Lu2010DFM.pdf:/home/roland/Zotero/storage/262LD944/AlDabbagh_Lu2010DFM.pdf:application/pdf},
}

@article{akaike_statistical_1970,
	title = {Statistical {Predictor} {Identification}},
	volume = {22},
	journal = {Ann. Inst. Statist. Math.},
	author = {Akaike, H},
	year = {1970},
	pages = {203--217},
}

@inproceedings{akaike_information_1973,
	address = {Akademiai Kiado, Budapest},
	title = {Information {Theory} and {Extension} of the {Maximum} {Likelihood} {Principle}},
	booktitle = {Proceedings of the 2nd {International} {Symposium} of {Information} {Theory}},
	author = {Akaike, H.},
	editor = {{B.N.Petrov} and F.Csaki (eds.)},
	year = {1973},
	pages = {267--281},
}

@book{ajmone_marsan_modelling_1995,
	title = {Modelling with generalized stochastic {Petri} nets},
	publisher = {John Wiley},
	author = {Ajmone Marsan, Marco Giuseppe and Balbo, Gianfranco and Conte, Gianni and Donatelli, Susanna and Franceschinis, Giuliana},
	year = {1995},
}

@incollection{cepin_common_2011,
	title = {Common {Cause} {Failures}},
	booktitle = {Assessment of {Power} {System} {Reliability}},
	publisher = {Springer},
	author = {Čepin, Marko},
	year = {2011},
	pages = {125--138},
}

@book{cepin_assessment_2011,
	title = {Assessment of {Power} {System} reliability},
	publisher = {Springer},
	author = {Čepin, Marko},
	year = {2011},
}

@article{cui_bayesian_2020,
	title = {Bayesian network and game theory risk assessment model for third-party damage to oil and gas pipelines},
	volume = {134},
	issn = {0957-5820},
	url = {http://www.sciencedirect.com/science/article/pii/S0957582019306664},
	doi = {10.1016/j.psep.2019.11.038},
	abstract = {Tremendous amounts of oil and gas products are transported in pipelines worldwide resulting in increasing interest to identify the hazards and evaluate the associated risks associated with this critical infrastructure. Third-party intrusion is one of the least quantifiable factors being considered during the pipeline hazard assessment stage despite the substantial contributing to the total number of oil and gas pipeline incidents. This is because a probabilistic risk assessment cannot reliably model human actions and be applied to intentional acts. Due to the distinctive motivations of third-party damage, an unintentional third-party damage Bayesian Network model and a game-theoretic model on malicious intrusion will therefore be built, to examine the mechanism of pipeline failure caused by this mode. This study is conducted aiming at investigating pipeline risk resulting from third-party damage, and will formulate risk assessment models to identify threats, prioritize risks and determine which integrity plan should apply to different pipeline segments given the condition of third-party interference (both the accidental damage and malicious acts).},
	language = {en},
	urldate = {2020-05-05},
	journal = {Process Safety and Environmental Protection},
	author = {Cui, Yan and Quddus, Noor and Mashuga, Chad V.},
	month = feb,
	year = {2020},
	keywords = {Bayesian network, Game theory, Pipeline damage, Pipeline hazard assessment, Pipeline safety, Risk assessment},
	pages = {178--188},
	file = {Cui et al. - 2020 - Bayesian network and game theory risk assessment m.pdf:/home/roland/Zotero/storage/EMW8SQSL/Cui et al. - 2020 - Bayesian network and game theory risk assessment m.pdf:application/pdf},
}

@article{zhang_rbms-bn_2019,
	title = {An {RBMs}-{BN} method to {RUL} prediction of traction converter of {CRH2} trains},
	volume = {85},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S095219761930140X},
	doi = {10.1016/j.engappai.2019.06.001},
	abstract = {Remaining useful life (RUL) prediction is essential to ensure safety and reliability of engineering systems. To achieve better prediction performance, causalities among the physical quantities are considered by applying Bayesian Network (BN) to RUL prediction. For this purpose, several improvements on BN modeling are made in this paper, to handle the closed-loop control structure of engineering systems, and to improve prediction performance with reduced complexity. Taking the traction converter of CRH2 trains as the object of the research, a closed-loop Bond Graph (BG) model is firstly developed to describe the causality of multi-domain physical quantities, which is then transformed to be a BN structure. Then, multi-dimensional features are extracted from the condition monitoring data and are used as the inputs to the nodes of BN model. Finally, Restricted Boltzmann Machines (RBMs) are used to further extract the latent features that cannot be directly observed or measured, but greatly improve the accuracy of the BN based RUL prediction. Case study is conducted using a hardware-in-loop simulation platform for traction system of China Railway High-speed (CRH2) trains, to predict RUL of the DC-link circuit with degradation of capacitance or resistance. The experimental results can show the validity and superiority of the proposed RBMs-BN based RUL prediction method.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Zhang, Chuanyu and Wang, Cunsong and Lu, Ningyun and Jiang, Bin},
	month = oct,
	year = {2019},
	keywords = {Bayesian network, Bond graph, Restricted Boltzmann Machine, RUL prediction},
	pages = {46--56},
	file = {Zhang et al. - 2019 - An RBMs-BN method to RUL prediction of traction co.pdf:/home/roland/Zotero/storage/5PH2A5BF/Zhang et al. - 2019 - An RBMs-BN method to RUL prediction of traction co.pdf:application/pdf},
}

@article{sanchez_approach_2020,
	title = {An {Approach} {Based} on {Bayesian} {Network} for {Improving} {Project} {Management} {Maturity}: {An} {Application} to {Reduce} {Cost} {Overrun} {Risks} in {Engineering} {Projects}},
	volume = {119},
	issn = {0166-3615},
	shorttitle = {An {Approach} {Based} on {Bayesian} {Network} for {Improving} {Project} {Management} {Maturity}},
	url = {http://www.sciencedirect.com/science/article/pii/S0166361519309480},
	doi = {10.1016/j.compind.2020.103227},
	abstract = {The project management field has the imperative to increase the success probability of projects. Experts have developed several Project Management Maturity (PMM) models to assess project management practices and improve the project outcome. However, the current literature lacks models that allow experts to correlate the measured maturity with the expected probability of success. The present paper develops a general framework and a method to estimate the impact of PMM on project performance. It uses Bayesian networks to formalize project management experts’ knowledge and to extract knowledge from a database of past projects. An industrial case concerning large projects in the oil and gas industry is used to illustrate the application of the method to reduce the risk of project cost (or budget) overruns.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Computers in Industry},
	author = {Sanchez, Felipe and Bonjour, Eric and Micaelli, Jean-Pierre and Monticolo, Davy},
	month = aug,
	year = {2020},
	keywords = {Bayesian Networks, Cost Overrun, Knowledge Modeling, Maturity Model, Project Management},
	pages = {103227},
	file = {Sanchez et al. - 2020 - An Approach Based on Bayesian Network for Improvin.pdf:/home/roland/Zotero/storage/L8IJY8I2/Sanchez et al. - 2020 - An Approach Based on Bayesian Network for Improvin.pdf:application/pdf},
}

@article{silva_spatio-temporal_2020,
	title = {A spatio-temporal {Bayesian} {Network} approach for deforestation prediction in an {Amazon} rainforest expansion frontier},
	volume = {35},
	issn = {2211-6753},
	url = {http://www.sciencedirect.com/science/article/pii/S2211675319301447},
	doi = {10.1016/j.spasta.2019.100393},
	abstract = {In the last decade, Brazil has successfully managed to reduce deforestation in the Amazon forest. However, continued increases in annual deforestation rates call for environmental modeling to support short-term decision-making. This paper presents the functioning of a stepwise spatio-temporal Bayesian Network approach for spatially explicit analysis of deforestation risk based on observation data. The study area comprises a deforestation expansion frontier located in the southwest of Pará state, Brazil. The proposed approach has been successful in estimating deforestation risk over the years. Among the selected variables to compose the Bayesian Network model, distance from hot spots and distance from degraded areas present the highest contribution, while protected areas variable present a significant mitigation effect on the phenomenon. Accuracy assessment indices corroborate the agreement between deforestation events and predictions.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Spatial Statistics},
	author = {Silva, Alexsandro C. O. and Fonseca, Leila M. G. and Körting, Thales S. and Escada, Maria Isabel S.},
	month = mar,
	year = {2020},
	keywords = {Bayesian Networks, Brazilian Amazon forest, Deforestation, Environmental modeling, Spatio-temporal modeling},
	pages = {100393},
	file = {Silva et al. - 2020 - A spatio-temporal Bayesian Network approach for de.pdf:/home/roland/Zotero/storage/TDA3R4TT/Silva et al. - 2020 - A spatio-temporal Bayesian Network approach for de.pdf:application/pdf},
}

@article{atoui_single_2019,
	title = {A single {Bayesian} network classifier for monitoring with unknown classes},
	volume = {85},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197619301800},
	doi = {10.1016/j.engappai.2019.07.016},
	abstract = {In this paper, the Conditional Gaussian Networks (CGNs), a form of Bayesian Networks (BN), are used as a statistical process monitoring approach to detect and diagnose faults. The proposed approach improves the structure of Bayesian networks and generalizes a few results regarding statistical tests and the use of an exclusion criterion. The proposed framework is evaluated using data from the benchmark Tennessee Eastman Process (TEP) with various scenarios.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Atoui, M. Amine and Cohen, Achraf and Verron, Sylvain and Kobi, Abdessamad},
	month = oct,
	year = {2019},
	keywords = {Bayesian networks, Classification, Exclusion criteria, Fault detection and diagnosis},
	pages = {681--690},
	file = {Atoui et al. - 2019 - A single Bayesian network classifier for monitorin.pdf:/home/roland/Zotero/storage/KVE7WFW9/Atoui et al. - 2019 - A single Bayesian network classifier for monitorin.pdf:application/pdf},
}

@article{desforges_prognostic_2017,
	title = {A prognostic function for complex systems to support production and maintenance co-operative planning based on an extension of object oriented {Bayesian} networks},
	volume = {86},
	issn = {0166-3615},
	url = {http://www.sciencedirect.com/science/article/pii/S0166361517300076},
	doi = {10.1016/j.compind.2017.01.002},
	abstract = {The high costs of complex systems lead companies to improve their efficiency. This improvement can particularly be achieved by reducing their downtimes because of failures or for maintenance purposes. This reduction is the main goal of Condition-Based Maintenance and of Prognostics and Health Management. Both those maintenance policies need to install appropriate sensors and data processes not only to assess the current health of their critical components but also their future health. These future health assessments, also called prognostics, produce the Remaining Useful Life of the components associated to imprecision quantifications. In the case of complex systems where components are numerous, the matter is to assess the health of whole systems from the prognostics of their components (the local prognostics). In this paper, we propose a generic function that assesses the future availability of complex systems from their local prognostics (the prognostics of their components) by using inferences rules. The results of this function can then be used as decision support indicators for planning productive and maintenance tasks. This function exploits a proposed extension for Object Oriented Bayesian Networks (OOBN) used to model the complex system in order to assess the probabilities of failure of components, functions and subsystems. The modeling of the complex system is required and it is presented as well as modeling transformations to tackle some OOBN limitations. Then, the computing inference rules used to define the future availability of complex systems are presented. The extension added to OOBN consists in indicating the components that should first be maintained to improve the availabilities of the functions and subsystems in order to provide a second kind of decision support indicators for maintenance. A fictitious multi-component system bringing together most of the structures encountered in complex systems is modeled and the results obtained from the application of the proposed generic function are presented as well as ways that production and maintenance planning can used the computed indicators. Then we show how the proposed generic prognostic function can be used to predict propagations of failures and their effects on the functioning of functions and subsystems.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Computers in Industry},
	author = {Desforges, Xavier and Diévart, Mickaël and Archimède, Bernard},
	month = apr,
	year = {2017},
	keywords = {Availability assessment, Complex systems, Preventive maintenance, Production planning, Prognostics},
	pages = {34--51},
	file = {Desforges et al. - 2017 - A prognostic function for complex systems to suppo.pdf:/home/roland/Zotero/storage/R9T4R8SP/Desforges et al. - 2017 - A prognostic function for complex systems to suppo.pdf:application/pdf},
}

@article{tong_dynamic_2020,
	title = {A {Dynamic} {Bayesian} {Network}-based approach to {Resilience} {Assessment} of {Engineered} {Systems}},
	volume = {65},
	issn = {0950-4230},
	url = {http://www.sciencedirect.com/science/article/pii/S0950423020302813},
	doi = {10.1016/j.jlp.2020.104152},
	abstract = {Traditional risk assessment approaches mainly focus on the pre-failure scenarios with certain information. For complex systems, the scope of risk assessment needs to be extended to include the post-failure phase; because the emerging hazards of these systems cannot be wholly identified and are usually highly uncertain. Thus, resilience assessment needs to be investigated. Most of the existing literature quantify resilience based on a system's performance loss caused by disruptions. These studies fail to assess the probability of a system to sustain or restore to a normal operational state after disruptions occur, how this probability changes with time, and how fast the system can be restored. The dynamic and probabilistic characteristics of resilience must be considered in systemic resilience assessment, in which the engineered system, human and organizational factors, and external disruptions are considered. This paper aims to develop a dynamic Bayesian network (DBN)-based approach to the probabilistic assessment of the system resilience by incorporating temporal processes of adaption and recovery into the analysis of system functionality. The proposed method also provides a new way to define resilience in terms of the probability of system functionality change during and after a disruption. A case study on the Chevron refinery accident is used to demonstrate the applicability of the proposed methodology.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Journal of Loss Prevention in the Process Industries},
	author = {Tong, Qi and Yang, Ming and Zinetullina, Altyngul},
	month = may,
	year = {2020},
	keywords = {Dynamic bayesian network, Functionality, Resilience assessment, Safety},
	pages = {104152},
	file = {Tong et al. - 2020 - A Dynamic Bayesian Network-based approach to Resil.pdf:/home/roland/Zotero/storage/BC94P6Y9/Tong et al. - 2020 - A Dynamic Bayesian Network-based approach to Resil.pdf:application/pdf},
}

@article{tidriri_decision_2019,
	title = {A decision fusion based methodology for fault {Prognostic} and {Health} {Management} of complex systems},
	volume = {83},
	issn = {1568-4946},
	url = {http://www.sciencedirect.com/science/article/pii/S1568494619304028},
	doi = {10.1016/j.asoc.2019.105622},
	abstract = {Prognostic and Health Management (PHM) represents an active field of research and a major scientific challenge in many domains. It usually focuses on the failure time or the Remaining Useful Life (RUL) prediction of a system. This paper presents a generic framework, based on a discrete Bayesian Network (BN), particularly tailored for decision fusion of heterogeneous prognostic methods. The BN parameters are computed according to the fixed prognostic objectives. The effectiveness of the proposed decision fusion based methodology for the prognostic is demonstrated through the RULs estimation of turbofan engines. The application highlights the ability of the approach to estimate RULs which overpasses the performance of most other published results in the literature.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Applied Soft Computing},
	author = {Tidriri, Khaoula and Verron, Sylvain and Tiplica, Teodor and Chatti, Nizar},
	month = oct,
	year = {2019},
	keywords = {Bayesian network, Complex systems, Decision fusion, Fault prognostic, Generic framework},
	pages = {105622},
	file = {Tidriri et al. - 2019 - A decision fusion based methodology for fault Prog.pdf:/home/roland/Zotero/storage/EI639FEX/Tidriri et al. - 2019 - A decision fusion based methodology for fault Prog.pdf:application/pdf},
}

@article{sevinc_bayesian_2020,
	title = {A {Bayesian} network model for prediction and analysis of possible forest fire causes},
	volume = {457},
	issn = {0378-1127},
	url = {http://www.sciencedirect.com/science/article/pii/S0378112719311776},
	doi = {10.1016/j.foreco.2019.117723},
	abstract = {Possible causes of a forest fire ignition could be human-caused (arson, smoking, hunting, picnic fire, shepherd fire, stubble burning) or natural-caused (lightning strikes, power lines). Temperature, relative humidity, tree species, distance from road, wind speed, distance from agricultural land, amount of burnt area, month and distance from settlement are the risk factors that may affect the occurrence of forest fires. This study introduces the use of Bayesian network model to predict the possible forest fire causes, as well as to perform an analysis of the multilateral interactive relations among them. The study was conducted in Mugla Regional Directorate of Forestry area located in the southwest of Turkey. The fire data, which were recorded between 2008 and 2018 in the area, were provided by General Directorate of Forestry. In this study, after applying some different structural learning algorithms, a Bayesian network, which is built on the nodes relative humidity, temperature, wind speed, month, distance from settlement, amount of burnt area, distance from agricultural land, distance from road and tree species, was estimated. The model showed that month is the first and temperature is the second most effective factor on the forest fire ignitions. The Bayesian network model approach adopted in this study could also be used with data obtained from different areas having different sizes.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Forest Ecology and Management},
	author = {Sevinc, Volkan and Kucuk, Omer and Goltas, Merih},
	month = feb,
	year = {2020},
	keywords = {Bayesian networks, Forest fires, Sensitivity analysis, Structural learning},
	pages = {117723},
	file = {Sevinc et al. - 2020 - A Bayesian network model for prediction and analys.pdf:/home/roland/Zotero/storage/RF8AREE9/Sevinc et al. - 2020 - A Bayesian network model for prediction and analys.pdf:application/pdf},
}

@article{kraisangka_bayesian_2018,
	title = {A {Bayesian} network interpretation of the {Cox}'s proportional hazard model},
	volume = {103},
	issn = {0888-613X},
	url = {http://www.sciencedirect.com/science/article/pii/S0888613X17306308},
	doi = {10.1016/j.ijar.2018.09.007},
	abstract = {Cox's proportional hazards (CPH) model is quite likely the most popular modeling technique in survival analysis. While the CPH model is able to represent a relationship between a collection of risks and their common effect, Bayesian networks have become an attractive alternative with an increased modeling power and far broader applications. Our paper focuses on a Bayesian network interpretation of the CPH model (BN-Cox). We provide a method of encoding knowledge from existing CPH models in the process of knowledge engineering for Bayesian networks. This is important because in practice we often have CPH models available in the literature and no access to the original data from which they have been derived. We compare the accuracy of the resulting BN-Cox model to the original CPH model, Kaplan–Meier estimate, and Bayesian networks learned from data, including Naive Bayes, Tree Augmented Naive Bayes, Noisy-Max, and parameter learning by means of the EM algorithm. BN-Cox model came out as the most accurate of all BN approaches and very close to the original CPH model. We study two approaches for simplifying the BN-Cox model for the sake of representational and computational efficiency: (1) parent divorcing and (2) removing less important risk factors. We show that removing less important risk factors leads to smaller loss of accuracy.},
	language = {en},
	urldate = {2020-05-05},
	journal = {International Journal of Approximate Reasoning},
	author = {Kraisangka, Jidapa and Druzdzel, Marek J.},
	month = dec,
	year = {2018},
	keywords = {Bayesian network, Cox's proportional hazard model},
	pages = {195--211},
	file = {Kraisangka et Druzdzel - 2018 - A Bayesian network interpretation of the Cox's pro.pdf:/home/roland/Zotero/storage/4CIAABCN/Kraisangka et Druzdzel - 2018 - A Bayesian network interpretation of the Cox's pro.pdf:application/pdf},
}

@article{carriger_bayesian_2020,
	title = {A {Bayesian} network approach to refining ecological risk assessments: {Mercury} and the {Florida} panther ({Puma} concolor coryi)},
	volume = {418},
	issn = {0304-3800},
	shorttitle = {A {Bayesian} network approach to refining ecological risk assessments},
	url = {http://www.sciencedirect.com/science/article/pii/S0304380019304193},
	doi = {10.1016/j.ecolmodel.2019.108911},
	abstract = {Traditionally hazard quotients (HQs) have been computed for ecological risk assessment, often without quantifying the underlying uncertainties in the risk estimate. We demonstrate a Bayesian network approach to quantitatively assess uncertainties in HQs using a retrospective case study of dietary mercury (Hg) risks to Florida panthers (Puma concolor coryi). The Bayesian network was parameterized, using exposure data from a previous Monte Carlo-based assessment of Hg risks (Barron et al., 2004. ECOTOX 13:223), as a representative example of the uncertainty and complexity in HQ calculations. Mercury HQs and risks to Florida panthers determined from a Bayesian network analysis were nearly identical to those determined using the prior Monte Carlo probabilistic assessment and demonstrated the ability of the Bayesian network to replicate conventional HQ-based approaches. Sensitivity analysis of the Bayesian network showed greatest influence on risk estimates from daily ingested dose by panthers and mercury levels in prey, and less influence from toxicity reference values. Diagnostic inference was used in a high-risk scenario to demonstrate the capabilities of Bayesian networks for examining probable causes for observed effects. Application of Bayesian networks in the computation of HQs provides a transparent and quantitative analysis of uncertainty in risks.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Ecological Modelling},
	author = {Carriger, John F. and Barron, Mace G.},
	month = feb,
	year = {2020},
	keywords = {Bayesian networks, Dynamic discretization, Florida panther, Mercury, Monte Carlo analysis, Terrestrial risk assessment},
	pages = {108911},
	file = {Carriger et Barron - 2020 - A Bayesian network approach to refining ecological.pdf:/home/roland/Zotero/storage/K3LMWD3D/Carriger et Barron - 2020 - A Bayesian network approach to refining ecological.pdf:application/pdf},
}

@article{wang_bayesian_2020,
	title = {A {Bayesian} network approach for cybersecurity risk assessment implementing and extending the {FAIR} model},
	volume = {89},
	issn = {0167-4048},
	url = {http://www.sciencedirect.com/science/article/pii/S0167404819300604},
	doi = {10.1016/j.cose.2019.101659},
	abstract = {Quantitative risk assessment can play a crucial role in effective decision making about cybersecurity strategies. The Factor Analysis of Information Risk (FAIR) is one of the most popular models for quantitative cybersecurity risk assessment. It provides a taxonomic framework to classify cybersecurity risk into a set of quantifiable risk factors and combines this with quantitative algorithms, in the form of a kind of Monte Carlo (MC) simulation combined with statistical approximation techniques, to estimate cybersecurity risk. However, the FAIR algorithms restrict both the type of statistical distributions that can be used and the expandability of the model structure. Moreover, the applied approximation techniques (including using cached data and interpolation methods) introduce inaccuracy into the FAIR model. To address restrictions of the FAIR model, we develop a more flexible alternative approach, which we call FAIR-BN, to implement the FAIR model using Bayesian Networks (BNs). To evaluate the performance of FAIR and FAIR-BN, we use a MC method (FAIR-MC) to implement calculations of the FAIR model without using any of the approximation techniques adopted by FAIR, thus avoiding the corresponding inaccuracy that can be introduced. We compare the empirical results generated by FAIR and FAIR-BN against a large number of samples generated using FAIR-MC. Both FAIR and FAIR-BN provide consistent results compared with FAIR-MC for general cases. However, the FAIR-BN achieves higher accuracy in several cases that cannot be accurately modelled by the FAIR model. Moreover, we demonstrate that FAIR-BN is more flexible and extensible by showing how it can incorporate process-oriented and game-theoretic methods. We call the resulting combined approach “Extended FAIR-BN” (EFBN) and show that it has the potential to provide an integrated solution for cybersecurity risk assessment and related decision making.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Computers \& Security},
	author = {Wang, Jiali and Neil, Martin and Fenton, Norman},
	month = feb,
	year = {2020},
	keywords = {Game theory, Bayesian networks, Adversarial risk analysis, Cybersecurity risk assessment, FAIR model, Monte Carlo simulation, Risk aggregation},
	pages = {101659},
	file = {Wang et al. - 2020 - A Bayesian network approach for cybersecurity risk.pdf:/home/roland/Zotero/storage/ZN2XNLK6/Wang et al. - 2020 - A Bayesian network approach for cybersecurity risk.pdf:application/pdf},
}

@article{carvalho_systematic_2019,
	title = {A systematic literature review of machine learning methods applied to predictive maintenance},
	volume = {137},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/S0360835219304838},
	doi = {10.1016/j.cie.2019.106024},
	abstract = {The amount of data extracted from production processes has increased exponentially due to the proliferation of sensing technologies. When processed and analyzed, data can bring out valuable information and knowledge from manufacturing process, production system and equipment. In industries, equipment maintenance is an important key, and affects the operation time of equipment and its efficiency. Thus, equipment faults need to be identified and solved, avoiding shutdown in the production processes. Machine Learning (ML) methods have been emerged as a promising tool in Predictive Maintenance (PdM) applications to prevent failures in equipment that make up the production lines in the factory floor. However, the performance of PdM applications depends on the appropriate choice of the ML method. The aim of this paper is to present a systematic literature review of ML methods applied to PdM, showing which are being explored in this field and the performance of the current state-of-the-art ML techniques. This review focuses on two scientific databases and provides a useful foundation on the ML techniques, their main results, challenges and opportunities, as well as it supports new research works in the PdM field.},
	language = {en},
	urldate = {2020-05-02},
	journal = {Computers \& Industrial Engineering},
	author = {Carvalho, Thyago P. and Soares, Fabrízzio A. A. M. N. and Vita, Roberto and Francisco, Roberto da P. and Basto, João P. and Alcalá, Symone G. S.},
	month = nov,
	year = {2019},
	keywords = {Artificial intelligence, Machine learning, PdM, Predictive maintenance, Systematic literature review},
	pages = {106024},
	file = {Carvalho et al. - 2019 - A systematic literature review of machine learning.pdf:/home/roland/Zotero/storage/HFMWYFMG/Carvalho et al. - 2019 - A systematic literature review of machine learning.pdf:application/pdf},
}

@article{noauthor_cyber_2017,
	title = {Cyber {Security} {Risk} {Evaluation} of a {Nuclear} {I}\&{C} {Using} {BN} and {ET}},
	volume = {49},
	issn = {1738-5733},
	url = {https://sciencedirect.ezproxy.univ-ubs.fr/science/article/pii/S1738573316302935},
	doi = {10.1016/j.net.2016.11.004},
	abstract = {Cyber security is an important issue in the field of nuclear engineering because nuclear facilities use digital equipment and digital systems that can…},
	language = {en},
	number = {3},
	urldate = {2020-04-30},
	journal = {Nuclear Engineering and Technology},
	month = apr,
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {517--524},
}

@article{kannan_systems-based_2020,
	title = {A systems-based approach for modeling of microbiologically influenced corrosion implemented using static and dynamic {Bayesian} networks},
	volume = {65},
	issn = {09504230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950423019304474},
	doi = {10.1016/j.jlp.2020.104108},
	abstract = {Microbiologically influenced corrosion (MIC) is a microbial community assisted degradation of materials affecting chemical processing and oil and gas industries. MIC has been implicated in incidents involving loss of containment of hazardous hydrocarbons which have led to fires and explosions, economic and environmental impact. The interplay between abiotic environmental factors and dynamic biotic factors in MIC are poorly un­ derstood. There is a lack of mechanistic understanding of MIC and very few models are available to predict or assess MIC threat. Here we report on the development of a model to assess the susceptibility to MIC. The highresolution model utilizes 60 independent nodes, including operational and historical failure analysis data, and is built by combining empirical relationships between the abiotic and biotic variables impacting MIC. Both static and dynamic Bayesian-network (BN) approaches were used to combine heuristic and quantitative states of variables to ultimately yield a susceptibility measure for MIC. A confidence-in-information metric was generated to reflect the amount of data used in the estimation. A susceptibility to MIC of 45\%–60\% was estimated by the model for ten different scenarios simulated using case-studies from literature. The susceptibility to MIC estimated by these scenarios was further interpreted in the context of these cases. This systems-based MIC model can be utilized as an independent estimator of susceptibility or can be incorporated as a sub-model within compre­ hensive safety threat assessment models currently utilized in industry.},
	language = {en},
	urldate = {2020-04-30},
	journal = {Journal of Loss Prevention in the Process Industries},
	author = {Kannan, Pranav and Kotu, Susmitha Purnima and Pasman, Hans and Vaddiraju, Sreeram and Jayaraman, Arul and Mannan, M. Sam},
	month = may,
	year = {2020},
	pages = {104108},
	file = {Kannan et al. - 2020 - A systems-based approach for modeling of microbiol.pdf:/home/roland/Zotero/storage/H3Z3DALK/Kannan et al. - 2020 - A systems-based approach for modeling of microbiol.pdf:application/pdf},
}

@mastersthesis{zweig_forward-backward_1996,
	title = {A forward-backward algorithm for inference in {Bayesian} networks and an empirical comparison with {HMMs}},
	school = {Department of Computer Science, U.C. Berkeley},
	author = {Zweig, G.},
	year = {1996},
	keywords = {graphical\_model, inference, dynamic},
}

@techreport{noauthor_generation_2004,
	address = {Palo Alto, CA},
	title = {Generation {Risk} {Assessment} ({GRA}) {Plant} {Implementation} {Guide}},
	number = {10081121},
	institution = {EPRI},
	year = {2004},
	keywords = {modelling, gra, power\_plant},
}

@book{zwingelstein_maintenance_1996,
	series = {Diagnostic et maintenance},
	title = {La maintenance basée sur la fiabilité - guide pratique d'application de la {RCM}},
	publisher = {Hermes},
	author = {Zwingelstein, Gilles},
	year = {1996},
	keywords = {reliability, maintenance, rcm},
}

@phdthesis{zille_modelisation_2009,
	type = {{PhD} {Thesis}},
	title = {Modélisation et évaluation des stratégies de maintenance complexes sur des systèmes multi-composants},
	school = {Université de Technologie de Troyes},
	author = {Zille, Valérie},
	year = {2009},
	keywords = {maintenance, petri\_net},
}

@article{zhao_probabilistic_2007,
	title = {Probabilistic {Model} for {Predicting} {Rail} {Breaks} and {Controlling} {Risk} of {Derailment}},
	volume = {1995},
	issn = {0361-1981},
	abstract = {A model is developed to analyze the risk of derailment of railway vehicles by using a probabilistic approach to model the development of rail defects leading to rail breaks and further to derailment. The risk of derailment is measured by the expected number of rail breaks multiplied by the severity of rail break. To evaluate the risk, four submodels are combined to predict the rate of occurrence of rail defects and breaks. These submodels include one to predict the expected number of weld defects, considering the effect of rail repair; a fatigue model of the rails; and a grinding model to take into account removal of defects through maintenance. The fourth submodel concerns the impact of inspections that are imperfect and of nonconstant frequency. The performance and application of the proposed combined model are illustrated by an example that shows the effectiveness of alternative measures in reducing the risk of derailment. It also shows that the proposed model can be used as a tool for quantitatively evaluating the risk of derailment and for decision making on control of the risk.},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Zhao, J. and Chan, A. H. C. and Burrow, M. P. N.},
	year = {2007},
	keywords = {railway, reliability},
	pages = {76--83},
}

@article{zhang_exploiting_1996,
	title = {Exploiting causal independence in bayesian network inference},
	volume = {5},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.9686},
	abstract = {A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as "or", "sum " or "max", on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms. 1.},
	journal = {Journal of Artificial Intelligence Research},
	author = {Zhang, Nevin L. and Poole, David},
	year = {1996},
	keywords = {graphical\_model, inference},
	pages = {301--328},
}

@article{yang_use_2006,
	title = {The use of lifetime functions in the optimization of interventions on existing bridges considering maintenance and failure costs},
	volume = {91},
	url = {http://dx.doi.org/10.1016/j.ress.2005.06.001},
	doi = {10.1016/j.ress.2005.06.001},
	abstract = {In the last decade, it became clear that life-cycle cost analysis of existing civil infrastructure must be used to optimally manage the growing number of aging and deteriorating structures. The uncertainties associated with deteriorating structures require the use of probabilistic methods to properly evaluate their lifetime performance. In this paper, the deterioration and the effect of maintenance actions are analyzed considering the performance of existing structures characterized by lifetime functions. These functions allow, in a simple manner, the consideration of the effect of aging on the decrease of the probability of survival of a structure, as well as the effect of maintenance actions. Models for the effects of proactive and reactive preventive maintenance, and essential maintenance actions are presented. Since the probability of failure is different from zero during the entire service life of a deteriorating structure and depends strongly on the maintenance strategy, the cost of failure is included in this analysis. The failure of one component in a structure does not usually lead to failure of the structure and, as a result, the safety of existing structures must be analyzed using a system reliability framework. The optimization consists of minimizing the sum of the cumulative maintenance and expected failure cost during the prescribed time horizon. Two examples of application of the proposed methodology are presented. In the first example, the sum of the maintenance and failure costs of a bridge in Colorado is minimized considering essential maintenance only and a fixed minimum acceptable probability of failure. In the second example, the expected lifetime cost, including maintenance and expected failure costs, of a multi-girder bridge is minimized considering reactive preventive maintenance actions.},
	number = {6},
	journal = {Reliability Engineering \& System Safety},
	author = {Yang, Seung-Ie and Frangopol, Dan M. and Kawakami, Yoriko and Neves, Luis C.},
	month = jun,
	year = {2006},
	pages = {698--705},
}

@inproceedings{yoon_cityride:_2012,
	title = {Cityride: {A} {Predictive} {Bike} {Sharing} {Journey} {Advisor}},
	doi = {10.1109/MDM.2012.16},
	abstract = {In this paper, we present a personal journey advisor application for helping people to navigate the city using the available bike-sharing system. For a given origin and destination, the application suggests the best pair of stations to be used to take and return a city-bike, in order to minimize the overall walking and biking travel time as well as maximizing the probability to find available bikes at the first station and returning slots at the second one. To solve the journey advisor optimization problem, we modeled real mobile bikers' behavior in terms of travel time, and used the predicted availability at every bike station to choose the pair of stations which maximizes a measure of optimality. To develop the application, we built a spatio-temporal prediction system able to estimate the number of available bikes for each station in short and long term, outperforming already developed solutions. The prediction system is based on an underlying spatial interaction network among the bike stations, and takes into account the temporal patterns included in the data. The City ride application was tested with real data from the Dublin bike-sharing system.},
	booktitle = {2012 {IEEE} 13th {International} {Conference} on {Mobile} {Data} {Management}},
	author = {Yoon, J. W. and Pinelli, F. and Calabrese, F.},
	month = jul,
	year = {2012},
	keywords = {Autoregressive processes, Availability, Bayesian methods, bike-sharing, Cities and towns, city navigation, Cityride, Dublin bike sharing system, interactive systems, journey advisor optimization, journey planner, Market research, mobile bikers behavior, mobile computing, mobile phone application, mobility analysis, navigation, optimisation, personal journey advisor, Prediction algorithms, predictive bike sharing journey advisor, Real-time systems, spatial interaction network},
	pages = {306--311},
}

@techreport{yedidia_understanding_2003,
	address = {San Francisco, CA, USA},
	title = {Understanding belief propagation and its generalizations},
	url = {http://portal.acm.org/citation.cfm?id=779352},
	abstract = {"Inference" problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines.We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps explaining the successes of the BP algorithm and enables connections to be made with variational approaches to approximate inference.The connection of BP with the Bethe approximation also suggests a way to construct new message-passing algorithms based on improvements to Bethe's approximation introduced Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.},
	institution = {Morgan Kaufmann Publishers Inc.},
	author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
	year = {2003},
	keywords = {graphical\_model, inference},
	pages = {239--269},
}

@inproceedings{yanar_system_1998,
	title = {System structuring for risk analysis using object oriented methodology},
	volume = {1},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Probabilistic} {Safety} {Assessment} and {Management} ({PSAM} {IV}), {New} {York}},
	author = {Yanar, DK},
	year = {1998},
	pages = {227--32},
}

@article{xing_reliability_2010,
	title = {Reliability analysis of static and dynamic fault-tolerant systems subject to probabilistic common-cause failures},
	volume = {224},
	number = {1},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
	author = {Xing, L and Boddu, P and Sun, Y and Wang, W},
	year = {2010},
	pages = {43--53},
	file = {Xing_al2010Reliability.pdf:/home/roland/Zotero/storage/H5SBMATV/Xing_al2010Reliability.pdf:application/pdf},
}

@book{wood_reliability-metric_2001,
	title = {Reliability-metric varieties and their relationships},
	url = {http://dx.doi.org/10.1109/RAMS.2001.902451},
	abstract = {Although reliability theory is based on the concept of failures, the definition of a failure may be very different from different perspectives. To a hardware engineer, a failure means a part replacement and verification of the failure. To a manufacturing repair depot, a failure is a returned part. To the finance department, a failure is a warranty claim. To a service organization, a failure is a service call for corrective maintenance (as opposed to planned or preventive maintenance). To a customer, a failure is a degradation of service or capability of the product. These various definitions of failure lead to various types of reliability metrics, such as part replacement rate or service call rate, in addition to the classic reliability metrics such as constant failure rate. This paper describes many types of reliability metrics and their interrelationships, both in theory and in practice. For the practical metric relationships, we draw from our experience with many reliability metrics at the Tandem Business Unit of the Compaq Computer Corporation. Our main conclusions are: the classical engineering view of failures is not sufficient for customer satisfaction with reliability in our market; multiple reliability metrics need to be tracked to satisfy the needs of various organizations; and many different functional groups must work together to ensure customer satisfaction with reliability},
	author = {Wood, A. P.},
	year = {2001},
	doi = {10.1109/RAMS.2001.902451},
	keywords = {reliability},
}

@article{wilson_bayesian_2007,
	title = {Bayesian networks for multilevel system reliability},
	volume = {92},
	url = {http://www.sciencedirect.com/science/article/B6V4T-4M7CDC6-1/2/3eb86bccfe785054968744e3d235f00b},
	abstract = {Bayesian networks have recently found many applications in systems reliability; however, the focus has been on binary outcomes. In this paper we extend their use to multilevel discrete data and discuss how to make joint inference about all of the nodes in the network. These methods are applicable when system structures are too complex to be represented by fault trees. The methods are illustrated through four examples that are structured to clarify the scope of the problem.},
	number = {10},
	journal = {Reliability Engineering \& System Safety},
	author = {Wilson, Alyson G. and Huzurbazar, Aparna V.},
	month = oct,
	year = {2007},
	keywords = {reliability, graphical\_model, inference},
	pages = {1413--1420},
}

@techreport{wierman_common-cause_2007,
	title = {Common-{Cause} {Failure} {Database} and {Analysis} system : {Event} {Data} {Collection}, {Classification} and {Coding}},
	url = {http://www.osti.gov/scitech/servlets/purl/6573115},
	abstract = {This report on the Common-Cause Failure Database and Analysis System presents an overview of common-cause failure (CCF) analysis methods for use in the U.S. commercial nuclear power industry. Idaho National Laboratory staff identify equipment failures that contribute to CCF events through searches of Licensee Event Reports, Nuclear Plant Reliability Data System failure reports, and Equipment Performance and Information Exchange failure reports. The staff then enter the event information into a personal computer-based data analysis system (CCF system). This report summarizes how data are gathered, evaluated, and coded into the CCF system, and describes the process for using the data to estimate probabilistic risk assessment common-cause failure parameters.},
	number = {NUREG/CR-6268, Rev. 1},
	institution = {U.S. Nuclear Regulatory Commission},
	author = {Wierman, T.E. and Rasmuson, D.M. and Mosleh, A},
	month = sep,
	year = {2007},
	file = {Wierman_al2007NUREG-CR-6268.pdf:/home/roland/Zotero/storage/HPG4LA6N/Wierman_al2007NUREG-CR-6268.pdf:application/pdf},
}

@article{weiss_correctness_2001,
	title = {Correctness of belief propagation in {Gaussian} graphical models of arbitrary topology},
	volume = {13},
	number = {10},
	journal = {Neural computation},
	author = {Weiss, Yair and Freeman, William T},
	year = {2001},
	pages = {2173--2200},
}

@article{weinberger_google_2008,
	title = {Google {C}++ style guide},
	journal = {Section: Line Length. url: http://google-styleguide. googlecode. com/svn/trunk/cppguide. xml\# Line\_Length},
	author = {Weinberger, Benjy and Silverstein, Craig and Eitzmann, Gregory and Mentovai, Mark and Landray, Tashana},
	year = {2008},
}

@book{wei_exponential_1998,
	title = {Exponential {Family} {Nonlinear} {Models}},
	publisher = {Springer Verlag},
	author = {Wei, Bo-Cheng},
	year = {1998},
	keywords = {exponential\_family},
}

@article{weber_modelisation_2003,
	title = {Modélisation de processus industriels par {Réseaux} {Bayésiens} {Orientés} {Objet} ({RBOO})},
	volume = {X},
	number = {X},
	journal = {Revue d'Intelligence Artificielle},
	author = {Weber, Philippe and Suhner, Marie-Christine},
	year = {2003},
	keywords = {graphical\_model, maintenance, modelling, methodology, object-oriented},
	pages = {1--29},
}

@inproceedings{weber_reliability_2003,
	address = {Washington D.C., USA},
	title = {Reliability modelling with dynamic bayesian networks},
	booktitle = {Proceedings of the 5th {IFAC} {Symposium} on fault {Detection}, {Supervision} and {Safety} of {Technical} {Processes}},
	author = {Weber, P. and Jouffe, L.},
	month = jun,
	year = {2003},
	keywords = {reliability, graphical\_model, modelling, dynamic},
}

@book{warrington_modelling_2002,
	title = {Modelling of maintenance, within discrete event simulation},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=981652},
	abstract = {The concept of maintenance free operating periods (MFOP) requires co-ordination of failure avoidance, failure anticipation and maintenance delay techniques, with the objective of enhancing operational capability in a cost-effective manner. Individual aspects might be modelled mathematically but discrete event simulation is required for an analysis with the necessary fidelity. The ultra reliable aircraft model (URAM) is an aircraft reliability and maintenance discrete event simulation designed to investigate MFOP concepts. Analysis during its development identified several important factors pertaining to maintenance: Allocation and scheduling of resource; maintenance objectives and their operational \& time-based over-ride; forward planning of scheduled \& prognostics based maintenance; diagnostics, covering both fault visibility \& isolation; task prioritisation, based on system structural importance, criticality importance, time \& cost. These have been successfully implemented, making URAM an efficient and representative simulation of aircraft reliability, maintenance support and operational tasking. The factors and their implementation in URAM may easily be transferred to discrete event simulation of other complex assets such as ships and railway systems},
	author = {Warrington, L. and Jones, J. A. and Davis, N.},
	year = {2002},
}

@article{walt_numpy_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	url = {http://scitation.aip.org/content/aip/journal/cise/13/2/10.1109/MCSE.2011.37},
	doi = {http://dx.doi.org/10.1109/MCSE.2011.37},
	number = {2},
	journal = {Computing in Science \& Engineering},
	author = {Walt, Stéfan van der and Colbert, S. Chris and Varoquaux, Gaël},
	year = {2011},
	pages = {22--30},
}

@article{weber_complex_2006,
	title = {Complex system reliability modelling with {Dynamic} {Object} {Oriented} {Bayesian} {Networks} ({DOOBN})},
	volume = {91},
	url = {http://dx.doi.org/10.1016/j.ress.2005.03.006},
	doi = {10.1016/j.ress.2005.03.006},
	abstract = {Nowadays, the complex manufacturing processes have to be dynamically modelled and controlled to optimise the diagnosis and the maintenance policies. This article presents a methodology that will help developing Dynamic Object Oriented Bayesian Networks (DOOBNs) to formalise such complex dynamic models. The goal is to have a general reliability evaluation of a manufacturing process, from its implementation to its operating phase. The added value of this formalisation methodology consists in using the a priori knowledge of both the system's functioning and malfunctioning. Networks are built on principles of adaptability and integrate uncertainties on the relationships between causes and effects. Thus, the purpose is to evaluate, in terms of reliability, the impact of several decisions on the maintenance of the system. This methodology has been tested, in an industrial context, to model the reliability of a water (immersion) heater system.},
	number = {2},
	journal = {Selected Papers Presented at QUALITA 2003},
	author = {Weber, Philippe and Jouffe, Lionel},
	month = feb,
	year = {2006},
	keywords = {reliability, graphical\_model, dynamic, object-oriented},
	pages = {149--162},
}

@article{waeyenbergh_stepping_2001,
	title = {A stepping stone towards knowledge based maintenance},
	volume = {12},
	number = {2},
	journal = {South African Journal of Industrial Engineering},
	author = {Waeyenbergh, Geert and Pintelon, Liliane and Gelders, Ludo},
	year = {2001},
	keywords = {knowledge-based maintenance, predictive maintenance},
	pages = {61--61},
	file = {Waeyenbergh_al2001Stepping.pdf:/home/roland/Zotero/storage/ECDISSFZ/Waeyenbergh_al2001Stepping.pdf:application/pdf},
}

@inproceedings{vogel_modeling_2010,
	title = {Modeling of repositioning activities in bike-sharing systems},
	booktitle = {World conference on transport research ({WCTR})},
	author = {Vogel, Patrick and Mattfeld, Dirk C},
	year = {2010},
	file = {Vogel_Mattfeld2010RepositioningBSS.pdf:/home/roland/Zotero/storage/YTIV3PHX/Vogel_Mattfeld2010RepositioningBSS.pdf:application/pdf},
}

@article{vishnu_reliability_2016,
	title = {Reliability {Based} {Maintenance} {Strategy} {Selection} in {Process} {Plants}: {A} {Case} {Study}},
	volume = {25},
	issn = {2212-0173},
	url = {http://www.sciencedirect.com/science/article/pii/S2212017316305655},
	doi = {https://doi.org/10.1016/j.protcy.2016.08.211},
	abstract = {The importance of maintenance function has increased due to its role in keeping and improving the availability, product quality, safety requirements and operating cost levels of the process plants. Accordingly, maintenance strategy selection became one of the most important decision making activity in the industry. This paper proposes a general approach to implement Reliability Centered Maintenance (RCM) in process plants. RCM is a recently evolved maintenance strategy that incorporates all the advantages of traditional maintenance strategies. More precisely, RCM selects the most appropriate and tailor made maintenance strategy for all the equipment in the plant based on its criticality score and reliability parameters. RCM implementation requires the collection and analysis of historical failure and maintenance data to determine current condition of the equipments. Subsequently, maintenance strategy is framed for the unit by following Analytical Hierarchy Process (AHP) based methodology. This should be done by taking expert opinions of personals from both the maintenance and production departments. RCM implementation model presented here is validated with the maintenance history data of a process plant manufacturing titanium dioxide with a production capacity of 20,000 metric tonnes per annum. Currently the firm follows a combination of scheduled and breakdown maintenance strategies. However, RCM implementation in this plant is justified by the maintenance simulation results that revealed the current poor availability and performance of the equipments.},
	journal = {Procedia Technology},
	author = {Vishnu, C. R. and Regikumar, V.},
	year = {2016},
	keywords = {Analytical Hierarchy Process., Maintenance Simulation, Reliability Centered Maintenance},
	pages = {1080 -- 1087},
	file = {Vishnu_Regikumar2016RBM_Strategy_Selection.pdf:/home/roland/Zotero/storage/I8X4NTCI/Vishnu_Regikumar2016RBM_Strategy_Selection.pdf:application/pdf},
}

@book{villemeur_surete_1997,
	edition = {Première},
	series = {Direction des études et recherches d'{Electricité} de {France}},
	title = {Sûreté de fonctionnement des systèmes industriels},
	isbn = {978-2-212-01615-4},
	url = {http://www.eyrolles.com/Sciences/Livre/9782212016154/Livre_Surete_de_fonctionnement_des_systemes_industriels.php},
	publisher = {Eyrolles, EDF},
	author = {Villemeur, Alain},
	month = mar,
	year = {1997},
	keywords = {methodology, safety},
}

@book{vesely_fault_1981,
	address = {Washington, DC},
	title = {Fault {Tree} {Handbook}},
	abstract = {Since 1975, a short course entitled "System Safety and Reliability Analysis" has been presented to over 200 NRC personnel and contractors. The course has been taught jointly by David F. Haasl, Institute of System Sciences, Professor Norman H. Roberts, University of Washington, and members of the Probabilistic Analysis Staff, NRC, as part of a risk assessment training program sponsored by the Probabilistic Analysis Staff.This handbook has been developed not only to serve as text for the System Safety and Reliability Course, but also to make available to others a set of otherwise undocumented material on fault tree construction and evaluation. The publication of this handbook is in accordance with the recommendations of the Risk Assessment Review Group Report (NUREG/CR-0400) in which it was stated that the fault/event tree methodology both can and should be used more widely by the NRC. It is hoped that this document will help to codify and systematize the fault tree approach to systems analysis.},
	publisher = {U.S. Nuclear Regulatory Commission},
	author = {Vesely, W. E. and Goldberg, F. F. and Roberts, N. H. and Haasl, D. F.},
	year = {1981},
	keywords = {fault\_trees},
}

@inproceedings{verma_causal_1988,
	title = {Causal {Networks}: {Semantics} and {Expressiveness}},
	booktitle = {Proceedings of the 4th {Workshop} on {Uncertainty} in {Artificial} {Intelligence}},
	author = {Verma, T. and Pearl, J.},
	year = {1988},
	keywords = {graphical\_model},
	pages = {352--359},
}

@article{vaurio_theory_1994,
	title = {The theory and quantification of common cause shock events for redundant standby systems},
	volume = {43},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/0951832094900345},
	doi = {http://dx.doi.org/10.1016/0951-8320(94)90034-5},
	abstract = {System unavailability equations are obtained for redundant standby safety systems with alternative testing strategies. The model is based on time-related general multiple failure rates rather than probabilities per demand. Common cause failures (CCF) of all multiplicities are accounted for. The results indicate that while staggering the tests is advantageous to most systems, extra testing rules may be beneficial in addition for certain types of redundancy. The results for common parallel redundancy systems are used to obtain proper quantitative values for the basic events of general fault tree models of large systems. Several parametric \{CCF\} models, their hidden assumptions and mutual relationships are critically evaluated. A certain class of component-caused \{CCFs\} is introduced. It turns out that most commonly used models are valid only for the subset of external shocks that are independent of the number of components in a system. Estimation methods are presented for multiple failure rates and other model parameters. Coupling equations between model parameters cause correlations between the parameters and lead to constrained estimation. Numerical examples are used to illustrate the methods and to demonstrate the significance of various aspects of the models.},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Vaurio, Jussi K.},
	year = {1994},
	keywords = {CCF},
	pages = {289 -- 305},
	file = {Vaurio1994Theory_Quanti_CCF.pdf:/home/roland/Zotero/storage/Y8GU74KA/Vaurio1994Theory_Quanti_CCF.pdf:application/pdf},
}

@article{vaurio_uncertainties_2005,
	title = {Uncertainties and quantification of common cause failure rates and probabilities for system analyses},
	volume = {90},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832004002674},
	doi = {http://dx.doi.org/10.1016/j.ress.2004.10.014},
	abstract = {Simultaneous failures of multiple components due to common causes at random times are modelled by constant multiple-failure rates. A procedure is described for quantification of common cause failure (CCF) basic event probabilities for system models using plant-specific and multiple-plant failure-event data. Methodology is presented for estimating CCF-rates from event data contaminated with assessment uncertainties. Generalised impact vectors determine the moments for the rates of individual systems or plants. These moments determine the effective numbers of events and observation times to be input to a Bayesian formalism to obtain plant-specific posterior CCF-rates. The rates are used to determine plant-specific common cause event probabilities for the basic events of explicit fault tree models depending on test intervals, test schedules and repair policies. Three methods are presented to determine these probabilities such that the correct time-average system unavailability can be obtained with single fault tree quantification. Recommended numerical values are given and examples illustrate different aspects of the methodology.},
	number = {2–3},
	journal = {Reliability Engineering \& System Safety},
	author = {Vaurio, Jussi K.},
	year = {2005},
	keywords = {Common cause failures},
	pages = {186 -- 195},
}

@article{vaurio_implicit_1998,
	title = {An implicit method for incorporating common-cause failures in system analysis},
	volume = {47},
	issn = {0018-9529},
	doi = {10.1109/24.722285},
	abstract = {A general procedure incorporates common-cause (CC) failures into system analysis by an implicit method; i.e., after first solving the system probability equation without CC failures. Components of subsets are assumed to be equally vulnerable to CC of any particular multiplicity. The method allows for age-dependent hazard rates, repairable and nonrepairable components, systems with multiple CC groups, and systems where not all components are statistically-identical or subject to CC failures. Key equations are given both for reliability block-diagrams and fault-trees (success and failure models), considering the system reliability, availability and failure intensity functions. Initial failures and certain human errors are included, mainly for standby-system applications. The implicit method can dramatically simplify the Boolean manipulation and quantification of fault trees. Possible limitations and extensions are discussed},
	number = {2},
	journal = {Reliability, IEEE Transactions on},
	author = {Vaurio, Jussi K},
	month = jun,
	year = {1998},
	keywords = {reliability theory, Availability, age-dependent hazard rates, Boolean algebra, Boolean manipulation, Boolean quantification, common-cause failures incorporation, Electric shock, Equations, Failure analysis, failure intensity functions, failure models, fault trees, Fault trees, fault-trees, Hazards, human errors, Humans, Humidity, implicit method, initial failures, Lightning, maintenance engineering, nonrepairable components, probability, probability equation, Reliability, reliability block-diagrams, repairable components, subsets components, success models, system analysis, system availability, system reliability},
	pages = {173--180},
	file = {Vaurio1998Implicit_CCF.pdf:/home/roland/Zotero/storage/45FQKTVA/Vaurio1998Implicit_CCF.pdf:application/pdf},
}

@article{vaurio_consistent_2007,
	title = {Consistent mapping of common cause failure rates and alpha factors},
	volume = {92},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832006000664},
	doi = {http://dx.doi.org/10.1016/j.ress.2006.02.008},
	abstract = {The problem addressed is how to combine event experience data from multiple source plants to estimate common cause failure (CCF) rates for a target plant. Alternative models are considered for transforming \{CCF\} parameters from systems with different numbers of similar components to obtain CCF-rates for a specific group of components. Two sets of rules are reviewed and compared for transforming rates and assessment uncertainties from larger to smaller systems, i.e. mapping down. Mapping down equations are presented also for the alpha-factors and for the variances of \{CCF\} rates. Consistent rules are developed for mapping up CCF-rates and uncertainties from smaller to larger systems. These mapping up rules are not limited to a binomial \{CCF\} model. It is shown how consistency requirements set certain limits to possible parametric values. Empirical alpha factors are used to estimate robust mapping parameters, and mapping up equations are derived for alpha factors as well. An assessment uncertainty procedure is presented for treating incomplete or vague information when estimating CCF-rates. Numerical studies illustrate mapping rules and procedures. Recommendations are made for practical applications.},
	number = {5},
	journal = {Reliability Engineering \& System Safety},
	author = {Vaurio, Jussi K.},
	year = {2007},
	keywords = {Alpha-factors},
	pages = {628 -- 645},
	file = {Vaurio2007Mapping.pdf:/home/roland/Zotero/storage/H4I4AV72/Vaurio2007Mapping.pdf:application/pdf},
}

@article{vatn_overall_1996,
	title = {An overall model for maintenance optimization},
	volume = {51},
	url = {http://dx.doi.org/10.1016/0951-8320(95)00055-0},
	doi = {10.1016/0951-8320(95)00055-0},
	abstract = {This paper presents an approach for identifying the optimal maintenance schedule for the components of a production system. Safety, health and environment objectives, maintenance costs and costs of lost production are all taken into consideration, and maintenance is thus optimized with respect to multiple objectives. Such a global approach to maintenance optimization requires expertise from various fields, e.g., decision theory, risk analysis and reliability and maintenance modelling. Further, a close co-operation between management, maintenance personnel and analysts is required to achieve a successful result. In the past this has been a major obstacle to the extensive use of proper maintenance optimization methods in practice, and techniques to promote the communication between the involved parties of the optimization process is an essential element in the suggested approach. A simple step by step presentation of the required modelling is provided. Contrary to most current methods of RCM (Reliability Centered Maintenance), the approach is based on an analytic model, and therefore gives a sound framework for carrying out a proper maintenance optimization. The approach is also flexible as it can be carried out at various levels of detail, e.g., adopted to available resources and to the managements willingness to give detailed priorities with respect to objectives on safety vs production loss.},
	number = {3},
	journal = {Maintenance and reliability},
	author = {Vatn, Jorn and Hokstad, Per and Bodsberg, Lars},
	month = mar,
	year = {1996},
	keywords = {graphical\_model, maintenance, optimization, methodology},
	pages = {241--257},
}

@article{vatn_maintenance_1997,
	title = {Maintenance optimisation from a decision theoretical point of view},
	volume = {58},
	url = {http://dx.doi.org/10.1016/S0951-8320(97)00025-2},
	doi = {10.1016/S0951-8320(97)00025-2},
	abstract = {Maintenance optimisation is rarely discussed from a decision theoretical point of view. It is believed that maintenance programmes may benefit from using decision theory in a more formal manner. In decision theory there is a sharp line of demarcation between establishing requirements and preferences on one side, and methods for seeking an optimal solution in accordance with the requirements and preferences on the other side. We discuss requirements and preferences concerning maintenance, and how to model these by value and utility functions. Next we discuss how to choose the optimum set of maintenance actions. Influence diagrams are introduced to visualise the relation between maintenance actions, system characteristics and value functions. Finally an illustrative example is given.},
	number = {2},
	journal = {ESREL '95},
	author = {Vatn, Jorn},
	month = nov,
	year = {1997},
	keywords = {graphical\_model, maintenance, optimization, weibull},
	pages = {119--126},
}

@article{vassiliadis_maintenance-based_2000,
	title = {Maintenance-based strategies for environmental risk minimization in the process industries},
	volume = {71},
	url = {http://dx.doi.org/10.1016/S0304-3894(99)00095-3},
	doi = {10.1016/S0304-3894(99)00095-3},
	abstract = {Industry, environmental agencies and the scientific community have all emphasized the need to include environmental impact considerations next to profitability objectives in the design phase of modern chemical processes, responding to the increasing social concern over environmental degradation in the past years. Most environmental impact assessment and minimization approaches, however, are rather qualitative, providing general guidelines. In this work, to overcome their limitations and rigorously represent the defining elements of environmental risk, i.e. the mechanism of occurrence of unexpected events usually related to equipment failure and the severity of their consequences, detailed process, reliability and maintenance characteristics are incorporated within a process optimization framework. The objective concerns the optimization of overall process performance defined as a system effectiveness vector characterized by both the environmental and the profitability functions of the system. Implementation of the framework on a process example identifies the optimal combination of process design and operation as well as preventive maintenance strategies that accomplish the conflicting environmental and profitability targets and quantifies the existing trade-offs between them.},
	number = {1-3},
	journal = {Journal of Hazardous Materials},
	author = {Vassiliadis, C. G. and Pistikopoulos, E. N.},
	month = jan,
	year = {2000},
	pages = {481--501},
}

@article{vassiliadis_maintenance_2001,
	title = {Maintenance scheduling and process optimization under uncertainty},
	volume = {25},
	url = {http://dx.doi.org/10.1016/S0098-1354(00)00647-5},
	doi = {10.1016/S0098-1354(00)00647-5},
	abstract = {In this paper, we describe an optimization framework for (i) deriving optimal maintenance policies in continuous process operations in the presence of parametric uncertainty and (ii) analyzing and quantifying the impact of uncertainty on optimal maintenance schedules. A systems effectiveness measure is introduced which depends on expected process profitability and process and reliability/maintenance characteristics. A mixed integer nonlinear optimization model is proposed which aims at identifying the number of maintenance (preventive or corrective) actions required over a given time horizon of interest, the time instants and sequence of these maintenance actions on the various components of the process system, so that the system effectiveness is maximized. By introducing the concept of availability threshold values, it is shown that an efficient solution strategy can be established which requires the solution of much smaller nonlinear optimization problems. The application of the proposed framework to an example problem highlights the important interactions between process operation and maintenance scheduling in the presence of uncertainty.},
	number = {2-3},
	journal = {Computers \& Chemical Engineering},
	author = {Vassiliadis, C. G. and Pistikopoulos, E. N.},
	month = mar,
	year = {2001},
	pages = {217--236},
}

@article{van_remaining_2012,
	title = {Remaining useful life ({RUL}) based maintenance decision making for deteriorating systems},
	volume = {45},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667015338970},
	doi = {https://doi.org/10.3182/20121122-2-ES-4026.00029},
	abstract = {This paper deals with a remaining useful life (RUL) based maintenance policy for single unit deteriorating systems whose condition are periodically monitored. Thank to the RUL information, which is estimated from the available monitoring information, various decisions related to inspections, logistic support, maintenance opportunities and maintenance actions are considered in the proposed policy. To illustrate the use and the advantages of the proposed maintenance policy, different sensitivity studies are analyzed and discussed through a numerical example. A comparaison with a classical condition based maintenance policy is also investigated.},
	number = {31},
	journal = {IFAC Proceedings Volumes},
	author = {VAN, Phuc DO and LEVRAT, Eric and VOISIN, Alexandre and IUNG, Benoit},
	year = {2012},
	keywords = {Condition based maintenance, decision-making, deteriorating system, Gamma process, opportunity, predictive Maintenance, remaining useful life},
	pages = {66 -- 72},
	file = {DoVan_al2012RUL_Maintenance.pdf:/home/roland/Zotero/storage/3W4XZ6T5/DoVan_al2012RUL_Maintenance.pdf:application/pdf},
}

@book{tian_dbn_2004,
	title = {A {DBN} inference algorithm using junction tree},
	volume = {5},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1342309},
	abstract = {Dynamic bayesian networks (DBNs) is a compact representation of complex stochastic processes and has been used for many purposes, whose practical application is based on the inference in them. In this paper, we define an optimal node set to d-separate the last slice from the next slice in DBNs, forward interface. Based on that, we present a simple and efficient algorithm - interface algorithm, which implements the forwards and backwards operators using the junction tree algorithm. The interface algorithm uses the junction tree structure constructed from a modified two-slice temporal Bayes net. Finally, we perform complexity analysis for the interface algorithm and give out the lower and upper bounds on the complexity of the interface algorithm.},
	author = {Tian, Fengzhan and Lu, Yuchang},
	year = {2004},
	keywords = {graphical\_model, inference, dynamic},
}

@article{thiesson_accelerating_2001,
	title = {Accelerating {EM} for large databases},
	volume = {45},
	journal = {Machine Learning},
	author = {Thiesson, B. and Meek, C. and Heckerman, D.},
	year = {2001},
	keywords = {em, learning},
	pages = {279--299},
}

@article{thangamani_generalized_2012,
	title = {Generalized {Stochastic} {Petri} {Nets} for {Reliability} {Analysis} of {Lube} {Oil} {System} with {Common}-{Cause} {Failures}},
	volume = {2},
	number = {4},
	journal = {American Journal of Computational and Applied Mathematics},
	author = {Thangamani, G},
	year = {2012},
	pages = {152--158},
}

@article{taylor_forecasting_2017,
	title = {Forecasting at {Scale}},
	abstract = {There are a variety of challenges that come with producing a large number of forecasts across a variety time series. Our approach to forecasting at scale is a combination of configurable models and thorough analyst-in-the-loop performance analysis. We present a forecasting approach based on a decomposable model with interpretable parameters that can be intuitively adjusted by the analyst. We describe performance analysis that we use compare and evaluate forecasting procedures, as well as automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable forecasting of a large variety of business time series.},
	author = {Taylor, Sean J. and Letham, Benjamin},
	month = jan,
	year = {2017},
}

@article{tang_mean_1999,
	title = {Mean residual life of lifetime distributions},
	volume = {48},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=765930},
	abstract = {This paper characterizes the general behaviors of the MRL (mean residual lives) for both continuous and discrete lifetime distributions, with respect to their failure rates. For the continuous lifetime distribution with failure rates with only one or two change-points, the characteristic of the MRL depends only on its mean and failure rate at time zero. For failure rates with \&ldquo;roller coaster\&rdquo; behavior, the subsequent behavior of the MRL depends on its MRL and failure-rates at the change points. Using the characterization, their behaviors for the: Weibull; lognormal; Birnbaum-Saunders; inverse Gaussian; and bathtub failure rate distributions are tabulated in terms of their shape parameters. For discrete lifetime distributions, for upside-down bathtub failure rate with only one change point, the characteristic of the MRL depends only on its mean and the probability mass function at time zero},
	number = {1},
	journal = {Reliability, IEEE Transactions on},
	author = {Tang, L. C. and Lu, Y. and Chew, E. P.},
	year = {1999},
	keywords = {reliability, discrete\_time, weibull},
	pages = {73--78},
}

@inproceedings{tang_integrated_2004,
	title = {An integrated method for incorporating common cause failures in system analysis},
	booktitle = {Reliability and {Maintainability}, 2004 {Annual} {Symposium}-{RAMS}},
	publisher = {IEEE},
	author = {Tang, Zhihua and Bechta Dugan, Joanne},
	year = {2004},
	pages = {610--614},
	file = {Tang_Dugan2004Integrated.pdf:/home/roland/Zotero/storage/9ZNG5Q23/Tang_Dugan2004Integrated.pdf:application/pdf},
}

@article{susto_machine_2015,
	title = {Machine learning for predictive maintenance: {A} multiple classifier approach},
	volume = {11},
	number = {3},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Susto, Gian Antonio and Schirru, Andrea and Pampuri, Simone and McLoone, Seán and Beghi, Alessandro},
	year = {2015},
	keywords = {Predictive maintenance, Machine Learning},
	pages = {812--820},
	file = {Susto_al2015ML_PM.pdf:/home/roland/Zotero/storage/2WFIAMDA/Susto_al2015ML_PM.pdf:application/pdf},
}

@techreport{stamatelatos_probabilistic_2011,
	title = {Probabilistic {Risk} {Assessment} {Procedures} {Guide} for {NASA} {Managers} and {Practitioners}},
	url = {http://www.osti.gov/scitech/servlets/purl/6573115},
	number = {NASA/SP-2011-3421},
	institution = {U.S. Nuclear Regulatory Commission},
	author = {Stamatelatos, Michael and Dezfuli, Homayoon},
	month = dec,
	year = {2011},
	file = {Stamatelatos_Dezfuli2001NASA_PRA_Guide.pdf:/home/roland/Zotero/storage/2AJHQ6LN/Stamatelatos_Dezfuli2001NASA_PRA_Guide.pdf:application/pdf},
}

@book{spirtes_causation_1993,
	title = {Causation, prediction, and search},
	publisher = {Springer-Verlag},
	author = {Spirtes, P. and Glymour, C. and Scheines, R.},
	year = {1993},
}

@inproceedings{srinivas_generalization_1993,
	address = {San Francisco, CA, USA},
	series = {{UAI}'93},
	title = {A {Generalization} of the {Noisy}-or {Model}},
	isbn = {1-55860-306-9},
	url = {http://dl.acm.org/citation.cfm?id=2074473.2074499},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Srinivas, Sampath},
	year = {1993},
	note = {event-place: Washihgton, DC},
	pages = {208--215},
	file = {Srinivas1993NoisyOr.pdf:/home/roland/Zotero/storage/BN2Z96UB/Srinivas1993NoisyOr.pdf:application/pdf},
}

@article{smyth_belief_1997,
	title = {Belief networks, hidden {Markov} models, and {Markov} random fields: a unifying view},
	volume = {18},
	journal = {Pattern Recognition Letters},
	author = {Smyth, Padhraic},
	editor = {{Padhraic}},
	year = {1997},
	keywords = {graphical\_model},
	pages = {1261--1268},
}

@inproceedings{sipos_log-based_2014,
	address = {New York, NY, USA},
	series = {{KDD} '14},
	title = {Log-based {Predictive} {Maintenance}},
	isbn = {978-1-4503-2956-9},
	url = {http://doi.acm.org/10.1145/2623330.2623340},
	doi = {10.1145/2623330.2623340},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Sipos, Ruben and Fradkin, Dmitriy and Moerchen, Fabian and Wang, Zhuang},
	year = {2014},
	note = {event-place: New York, New York, USA},
	keywords = {predictive maintenance, crisp-dm, log mining, machine learning},
	pages = {1867--1876},
}

@article{shachter_probabilistic_1988,
	title = {Probabilistic {Inference} and {Influence} {Diagrams}},
	volume = {36},
	issn = {0030364X},
	url = {http://dx.doi.org/10.2307/171139},
	doi = {10.2307/171139},
	abstract = {An influence diagram is a network representation for probabilistic and decision analysis models. The nodes correspond to variables which can be constants, uncertain quantities, decisions, or objectives. The arcs reveal the probabilistic dependence of the uncertain quantities and the information available at the time of the decisions. The detailed data about the variables are stored within the nodes, so the diagram graph is compact and focuses attention on the relationships among the variables. Influence diagrams are effective communication tools and recent developments also allow them to be used for analysis. We develop algorithms to address questions of inference within a probabilistic model represented as an influence diagram. We use the conditional independence implied by the diagram's structure to determine the information needed to solve a given problem. When there is enough information we can solve it, exploiting that conditional independence. These same results are applied to problems of decision analysis. This methodology allows the construction of computer tools to maintain and evaluate complex models.},
	number = {4},
	journal = {Operations Research},
	author = {Shachter, R. D.},
	year = {1988},
	keywords = {influence\_diagrams},
	pages = {589--604},
}

@inproceedings{shachter_bayes-ball:_1998,
	address = {San Francisco, CA, USA},
	series = {{UAI}'98},
	title = {Bayes-ball: {Rational} {Pastime} (for {Determining} {Irrelevance} and {Requisite} {Information} in {Belief} {Networks} and {Influence} {Diagrams})},
	isbn = {1-55860-555-X},
	url = {http://dl.acm.org/citation.cfm?id=2074094.2074151},
	booktitle = {Proceedings of the {Fourteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Shachter, Ross D.},
	year = {1998},
	note = {event-place: Madison, Wisconsin},
	keywords = {belief networks, dseparation, independence, influence diagrams, irrelevance, requisite information},
	pages = {480--487},
}

@book{schling_boost_2011,
	title = {The {Boost} {C}++ {Libraries}},
	isbn = {0-9822191-9-9 978-0-9822191-9-5},
	publisher = {XML Press},
	author = {Schling, Boris},
	year = {2011},
}

@article{schwartz_estimating_1978,
	title = {Estimating the dimension of a model},
	volume = {6},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Schwartz, G},
	year = {1978},
	pages = {461--464},
}

@book{saporta_probabilites_2006,
	edition = {Seconde},
	title = {Probabilités, analyse des données et statistique},
	isbn = {978-2-7108-0814-5},
	publisher = {Éditions Technip},
	author = {Saporta, G.},
	month = jun,
	year = {2006},
	keywords = {statistics},
}

@article{sand-jensen_how_2007,
	title = {How to write consistently boring scientific literature},
	volume = {116},
	issn = {0030-1299},
	url = {http://dx.doi.org/10.1111/j.2007.0030-1299.15674.x},
	doi = {10.1111/j.2007.0030-1299.15674.x},
	number = {5},
	journal = {Oikos},
	author = {Sand-Jensen, Kaj},
	month = may,
	year = {2007},
	keywords = {writing},
	pages = {723--727},
}

@techreport{saint_raymond_adoption_2002,
	title = {Adoption de la règle fondamentale de sûreté no. 2002-01},
	number = {DGSNR/SD2/N° 1361 / 2003},
	institution = {Autorité de Sûreté Nucléaire},
	author = {Saint Raymond, Philippe},
	month = dec,
	year = {2002},
}

@phdthesis{rodrigues_de_morais_bayesian_2009,
	type = {{PhD} {Thesis}},
	title = {Bayesian {Network} structure learning with applications in feature selection},
	school = {INSA de Lyon},
	author = {Rodrigues de Morais, Sérgio},
	month = nov,
	year = {2009},
	keywords = {graphical\_model, structure\_learning},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	url = {http://dx.doi.org/10.1093/biomet/63.3.581},
	doi = {10.1093/biomet/63.3.581},
	abstract = {When making sampling distribution inferences about the parameter of the data, theta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random' and the observed data are observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about theta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is distinct' from theta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences. 10.1093/biomet/63.3.581},
	number = {3},
	journal = {Biometrika},
	author = {Rubin, D. B.},
	month = dec,
	year = {1976},
	keywords = {em, learning},
	pages = {581--592},
}

@inproceedings{renault_kb3:_1999,
	title = {{KB3}: computer program for automatic generation of fault trees},
	booktitle = {Proceedings of {Reliability} and {Maintainability} {Symposium}},
	publisher = {IEEE},
	author = {Renault, I and Pilliere, M and Villatte, N and Mouttapa, P},
	year = {1999},
	pages = {389--395},
}

@book{rausand_system_2004,
	title = {System reliability theory: models, statistical methods, and applications},
	volume = {396},
	publisher = {John Wiley \& Sons},
	author = {Rausand, Marvin and Høyland, Arnljot},
	year = {2004},
}

@inproceedings{rocklin_dask:_2015,
	title = {Dask: {Parallel} computation with blocked algorithms and task scheduling},
	booktitle = {Proceedings of the 14th {Python} in {Science} {Conference}},
	publisher = {Citeseer},
	author = {Rocklin, Matthew},
	year = {2015},
	pages = {130--136},
}

@book{rao_probability_2006,
	title = {Probability {Theory} with {Applications}},
	publisher = {Springer},
	author = {Rao, M. M. and Swift, R. J.},
	year = {2006},
	keywords = {probability},
}

@inproceedings{ramoni_learning_1997,
	title = {Learning {Bayesian} {Networks} from {Incomplete} {Databases}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.3385},
	abstract = {Bayesian approaches to learn the graphical structure of Bayesian Belief Networks (bbns) from databases share the assumption that the database is complete, that is, no entry is reported as unknown. Attempts to relax this assumption often involve the use of expensive iterative methods to discriminate among different structures. This paper introduces a deterministic method to learn the graphical structure of a bbn from a possibly incomplete database. Experimental evaluations show a significant...},
	author = {Ramoni, Marco and Sebastiani, Paola},
	year = {1997},
	keywords = {graphical\_model, learning},
	pages = {401--408},
}

@article{rabinowitz_algorithm_1995,
	title = {Algorithm 744: a stochastic algorithm for global optimization with constraints},
	volume = {21},
	issn = {0098-3500},
	url = {http://dx.doi.org/10.1145/203082.203090},
	doi = {10.1145/203082.203090},
	number = {2},
	journal = {ACM Trans. Math. Softw.},
	author = {Rabinowitz, Michael F.},
	month = jun,
	year = {1995},
	pages = {194--213},
}

@book{rabiner_fundamentals_1993,
	series = {Signal {Processing} {Series}},
	title = {Fundamentals of {Speech} {Recognition}},
	publisher = {Prentice Hall},
	author = {Rabiner, L. and Juang, B. H.},
	year = {1993},
	keywords = {hmm, speech},
}

@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
	volume = {77},
	issn = {0018-9219},
	url = {http://dx.doi.org/10.1109/5.18626},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Rabiner, L. R.},
	month = feb,
	year = {1989},
	pages = {257--286},
}

@book{puterman_markov_2005,
	title = {Markov {Decision} {Processes} : {Discrete} {Stochastic} {Dynamic} {Programming}},
	publisher = {Wiley},
	author = {Puterman, M. L.},
	year = {2005},
	keywords = {markov, process, decision},
}

@inproceedings{t._prosvirnova_altarica_2013,
	address = {York (Great Britain)},
	title = {The {AltaRica} 3.0 project for {Model}-{Based} {Safety} {Assessment}},
	booktitle = {Proceedings of 4th {IFAC} {Workshop} on {Dependable} {Control} of {Discrete} {Systems}, {DCDS} 2013},
	publisher = {IFAC},
	author = {{T. Prosvirnova} and {M. Batteux} and {P.-A. Brameret} and {A. Cherfi} and {T. Friedlhuber} and {J.-M. Roussel} and {A. Rauzy}},
	month = sep,
	year = {2013},
}

@inproceedings{pradhan_knowledge_1994,
	address = {San Francisco, CA, USA},
	series = {{UAI}'94},
	title = {Knowledge {Engineering} for {Large} {Belief} {Networks}},
	isbn = {1-55860-332-8},
	url = {http://dl.acm.org/citation.cfm?id=2074394.2074456},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Pradhan, Malcolm and Provan, Gregory and Middleton, Blackford and Henrion, Max},
	year = {1994},
	note = {event-place: Seattle, WA},
	pages = {484--490},
	file = {Pradhan_al1994Knowledge.pdf:/home/roland/Zotero/storage/EFSPR59I/Pradhan_al1994Knowledge.pdf:application/pdf},
}

@article{pons_estimation_2004,
	title = {Estimation in a model for a semi-{Markov} process with covariates under right-censoring},
	volume = {38},
	url = {http://www.ingentaconnect.com/content/tandf/gsta/2004/00000038/00000004/art00001},
	abstract = {A semi-Markov model with covariates is proposed for a multi-state process with a finite number of states such that the transition probabilities between the states and the distribution functions of the duration times between the occurrence of two states depend on a discrete covariate. The hazard rates for the time elapsed between two successive states depend on the covariate through a proportional hazards model involving a set of regression parameters, while the transition probabilities depend on the covariate in an unspecified way. We propose estimators for these parameters and for the cumulative hazard functions of the sojourn times. A difficulty comes from the fact that when a sojourn time in a state is right-censored, the next state is unknown. We prove that our estimators are consistent and asymptotically Gaussian under the model constraints.},
	number = {4},
	journal = {Statistics},
	author = {Pons, Odile},
	month = aug,
	year = {2004},
	keywords = {reliability, markov, process, censoring, cox\_model},
	pages = {273--293},
}

@phdthesis{point_altarica:_2000,
	type = {{PhD} {Thesis}},
	title = {{AltaRica}: {Contribution} à l'unification des méthodes formelles et de la sûreté de fonctionnement},
	school = {Université Sciences et Technologies-Bordeaux I},
	author = {Point, Gérald},
	year = {2000},
	file = {Point2000TheseAltaRica.pdf:/home/roland/Zotero/storage/CA96L65Z/Point2000TheseAltaRica.pdf:application/pdf},
}

@article{podofillini_risk-informed_2006,
	title = {Risk-informed optimisation of railway tracks inspection and maintenance procedures},
	volume = {91},
	issn = {0951-8320},
	journal = {Reliability Engineering and System Safety},
	author = {Podofillini, L. and Zio, E. and Vatn, J.},
	year = {2006},
	keywords = {railway, maintenance, optimization, continuous\_time, markov, process, algorithm, genetic, tracks\_inspection},
	pages = {20--35},
}

@book{plauger_c++_2000,
	address = {Upper Saddle River, NJ, USA},
	edition = {1st},
	title = {C++ {Standard} {Template} {Library}},
	isbn = {0-13-437633-1},
	publisher = {Prentice Hall PTR},
	author = {Plauger, P.J. and Lee, Meng and Musser, David and Stepanov, Alexander A.},
	year = {2000},
}

@article{platz_markov_1984,
	title = {A {Markov} model for common-cause failures},
	volume = {9},
	issn = {0143-8174},
	url = {http://www.sciencedirect.com/science/article/pii/0143817484900040},
	doi = {http://dx.doi.org/10.1016/0143-8174(84)90004-0},
	number = {1},
	journal = {Reliability Engineering},
	author = {Platz, Ole},
	year = {1984},
	pages = {25 -- 31},
}

@inproceedings{pietre-cambacedes_relations_2012,
	address = {Tours, France},
	title = {Des relations entre sûreté et sécurité},
	booktitle = {18th {IMdR} annual {Conference} on {Dependability} ({Lambda}-{Mu})},
	author = {Piètre-Cambacédès, Ludovic and Bouissou, Marc and Chaudet, Claude},
	month = oct,
	year = {2012},
	keywords = {Safety, cybercriminality, Security},
	file = {PietreCambacedes_al2012Relations.pdf:/home/roland/Zotero/storage/CBT4BVEX/PietreCambacedes_al2012Relations.pdf:application/pdf},
}

@article{pedregal_rcm2_2004,
	title = {{RCM2} predictive maintenance of railway systems based on unobserved components models},
	volume = {83},
	url = {http://dx.doi.org/10.1016/j.ress.2003.09.020},
	doi = {10.1016/j.ress.2003.09.020},
	abstract = {Turnouts are probably the most important infrastructure elements of the railway system because of its effect on the system safety, reliability and quality of the service. In this paper, a predictive maintenance system in point mechanism, called RCM2, has been implemented for increasing the quality service. RCM2 is based on the integration of the two other types of maintenance techniques, namely Reliability Centred Maintenance (RCM1) and Remote Condition Monitoring (RCM2). The core of the system consists of an Unobserved Components model set-up in a State Space framework, in which the unknown elements of the system are estimated by Maximum Likelihood. The detection of faults in the system is based on the correlation estimate between a curve free from faults (that is, continuously updated as new curves are incorporated in the data base) with the current curve data. If the correlation falls far from one, a fault is at hand. The detection system is tested on a set of 476 experiments carried out by the Universities of Sheffield and Castilla-La Mancha.},
	number = {1},
	journal = {Reliability Engineering \& System Safety},
	author = {Pedregal, Diego J. and Garcia, Fausto P. and Schmid, Felix},
	month = jan,
	year = {2004},
	keywords = {maintenance, continuous\_time, state\_space, turnouts},
	pages = {103--110},
}

@phdthesis{pietre-cambacedes_relations_2010,
	type = {{PhD} {Thesis}},
	title = {Des relations entre sûreté et sécurité},
	school = {Télécom ParisTech},
	author = {Piètre-Cambacédès, Lu},
	month = nov,
	year = {2010},
	file = {PietreCambacedes2010These.pdf:/home/roland/Zotero/storage/EI8K7T3X/PietreCambacedes2010These.pdf:application/pdf},
}

@techreport{pearl_graphoids_1985,
	title = {{GRAPHOIDS} : {A} graph based logic for reasonning about relevance relations},
	number = {850038 (R-53-L)},
	institution = {Cognitive Systems Laboratory, University of California, Los Angeles},
	author = {Pearl, J. and Paz, A.},
	year = {1985},
	keywords = {graphical\_model},
}

@inproceedings{pearl_reverend_1982,
	address = {Pittsburgh, PA},
	title = {Reverend {Bayes} on inference engines: {A} distributed hierarchical approach},
	booktitle = {Proceedings of the {American} {Association} of {Artificial} {Intelligence} {National} {Conference} on {AI}},
	author = {Pearl, J.},
	year = {1982},
	keywords = {graphical\_model, inference},
	pages = {133--136},
}

@inproceedings{pearl_theory_1991,
	address = {San Mateo, California},
	title = {A {Theory} of {Inferred} {Causation}},
	booktitle = {{KR}'91: {Principles} of {Knowledge} {Representation} and {Reasoning}},
	publisher = {Morgan Kaufmann},
	author = {Pearl, J. and Verma, T.S.},
	editor = {Allen, James F. and Fikes, Richard and Sandewall, Erik},
	year = {1991},
	pages = {441--452},
}

@book{pearl_probabilistic_1988,
	edition = {1},
	title = {Probabilistic {Reasoning} in {Intelligent} {Systems}: {Networks} of {Plausible} {Inference}},
	isbn = {1-55860-479-0},
	url = {http://www.worldcat.org/isbn/1558604790},
	abstract = {{\textless}p{\textgreater}{\textless}I{\textgreater}Probabilistic Reasoning in Intelligent Systems{\textless}/I{\textgreater} is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic.{\textless}br{\textgreater}{\textless}p{\textgreater}The author distinguishes syntactic and semantic approaches to uncertainty–and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition–in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.{\textless}/p{\textgreater}{\textless}br{\textgreater}{\textless}p{\textgreater}{\textless}I{\textgreater}Probabilistic Reasoning in Intelligent Systems{\textless}/I{\textgreater} will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.{\textless}/p{\textgreater}},
	publisher = {Morgan Kaufmann},
	author = {Pearl, J.},
	month = sep,
	year = {1988},
	note = {Published: Paperback},
	keywords = {graphical\_model, inference},
}

@article{pearl_fusion_1986,
	title = {Fusion, propagation, and structuring in belief networks},
	volume = {29},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/000437028690072X},
	doi = {http://dx.doi.org/10.1016/0004-3702(86)90072-X},
	abstract = {Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called “hidden causes.” It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.},
	number = {3},
	journal = {Artificial Intelligence},
	author = {Pearl, Judea},
	year = {1986},
	pages = {241 -- 288},
	file = {Pearl1986Fusion.pdf:/home/roland/Zotero/storage/G34U293F/Pearl1986Fusion.pdf:application/pdf},
}

@book{panos_handbook_2002,
	title = {Handbook of {Applied} {Optimization}},
	url = {#},
	publisher = {Oxford University Press},
	author = {Panos, M. P. and Mauricio, G. C.},
	month = jan,
	year = {2002},
	keywords = {optimization},
}

@article{orringer_rail_1988,
	title = {Rail testing : strategies for safe and economical rail quality assurance},
	volume = {1174},
	issn = {0361-1981},
	journal = {Transportation Research Record : Rail Replacement and Maintenance Management},
	author = {Orringer, O.},
	year = {1988},
	keywords = {railway},
	pages = {28--42},
}

@book{nowlan_reliability-centered_1978,
	address = {San Francisco, CA},
	title = {Reliability-{Centered} {Maintenance}},
	abstract = {This book explains basic concepts, principles, definitions, and applications of a logical discipline for development of efficient scheduled (preventive) maintenance programs for complex equipment, and the on-going management of such programs. Such programs are called reliability-centered maintenance (RCM) programs because they are centered on achieving the inherent safety and reliability capabilities of equipment at a minimum cost. A U.S. Department of Defense objective in sponsoring preparation of this document was that it serve as a guide for application to a wide range of different types of military equipment. There are essentially only four types of tasks in a scheduled mainenance program: (1) Inspect an item to detect a potential failure; (2) Rework an item before a maximum permissible age is exceeded; (3) Discard an item before a maximum permissible age is exceeded; (4) Inspect an item to find failures that have already occurred but were not evident to the equipment operating crew. A central problem addressed in this book is how to determine which types of scheduled maintenance tasks, if any, should be applied to an item and how frequently assigned tasks should be accomplished. The use of a decision diagram as an aid in this analysis is illustrated. The net result is a structured, systematic blend of experience, judgment, and operational data/ information to identify and anlayze which type of maintnence task is both applicable and effective for each significant item as it relates to a particular type of equipment.},
	publisher = {United Air Lines Inc.},
	author = {Nowlan, F. S. and Heap, H. F.},
	year = {1978},
	keywords = {reliability, maintenance},
}

@misc{noauthor_google_nodate,
	title = {Google {Actualités}},
	url = {https://news.google.com},
	abstract = {Informations complètes et à jour, compilées par Google Actualités à partir de sources d'actualités du monde entier},
	language = {fr},
	urldate = {2022-03-02},
	journal = {Google Actualités},
	file = {Snapshot:/home/roland/Zotero/storage/URJ7YA7A/topstories.html:text/html},
}

@misc{stachtchenko_manuel_2022,
	title = {Manuel de survie dans la jungle des poncifs anti-{Bitcoin} (version longue)},
	url = {https://medium.com/@AlexStach/manuel-de-survie-dans-la-jungle-des-poncifs-anti-bitcoin-version-longue-523e381745ff},
	abstract = {Le problème des erreurs et inexactitudes, c’est qu’elles prennent beaucoup moins de temps à être énoncées qu’elles n’en prennent à être…},
	language = {en},
	urldate = {2022-02-08},
	journal = {Medium},
	author = {Stachtchenko, Alexandre},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/roland/Zotero/storage/VAM2MP5T/manuel-de-survie-dans-la-jungle-des-poncifs-anti-bitcoin-version-longue-523e381745ff.html:text/html},
}

@article{ortu_technical_2021,
	title = {On {Technical} {Trading} and {Social} {Media} {Indicators} in {Cryptocurrencies}' {Price} {Classification} {Through} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2102.08189},
	abstract = {This work aims to analyse the predictability of price movements of cryptocurrencies on both hourly and daily data observed from January 2017 to January 2021, using deep learning algorithms. For our experiments, we used three sets of features: technical, trading and social media indicators, considering a restricted model of only technical indicators and an unrestricted model with technical, trading and social media indicators. We verified whether the consideration of trading and social media indicators, along with the classic technical variables (such as price's returns), leads to a significative improvement in the prediction of cryptocurrencies price's changes. We conducted the study on the two highest cryptocurrencies in volume and value (at the time of the study): Bitcoin and Ethereum. We implemented four different machine learning algorithms typically used in time-series classification problems: Multi Layers Perceptron (MLP), Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) neural network and Attention Long Short Term Memory (ALSTM). We devised the experiments using the advanced bootstrap technique to consider the variance problem on test samples, which allowed us to evaluate a more reliable estimate of the model's performance. Furthermore, the Grid Search technique was used to find the best hyperparameters values for each implemented algorithm. The study shows that, based on the hourly frequency results, the unrestricted model outperforms the restricted one. The addition of the trading indicators to the classic technical indicators improves the accuracy of Bitcoin and Ethereum price's changes prediction, with an increase of accuracy from a range of 51-55\% for the restricted model, to 67-84\% for the unrestricted model.},
	urldate = {2022-02-06},
	journal = {arXiv:2102.08189 [cs, q-fin, stat]},
	author = {Ortu, Marco and Uras, Nicola and Conversano, Claudio and Destefanis, Giuseppe and Bartolucci, Silvia},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.08189},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Statistics - Applications, \_tablet},
	file = {arXiv.org Snapshot:/home/roland/Zotero/storage/HJH4LKPT/2102.html:text/html;Ortu et al_2021_On Technical Trading and Social Media Indicators in Cryptocurrencies' Price.pdf:/home/roland/Zotero/storage/J484T5UM/Ortu et al_2021_On Technical Trading and Social Media Indicators in Cryptocurrencies' Price.pdf:application/pdf},
}

@article{huang_predicting_2019,
	title = {Predicting bitcoin returns using high-dimensional technical indicators},
	volume = {5},
	issn = {2405-9188},
	url = {https://www.sciencedirect.com/science/article/pii/S2405918818300928},
	doi = {10.1016/j.jfds.2018.10.001},
	abstract = {There has been much debate about whether returns on financial assets, such as stock returns or commodity returns, are predictable; however, few studies have investigated cryptocurrency return predictability. In this article we examine whether bitcoin returns are predictable by a large set of bitcoin price-based technical indicators. Specifically, we construct a classification tree-based model for return prediction using 124 technical indicators. We provide evidence that the proposed model has strong out-of-sample predictive power for narrow ranges of daily returns on bitcoin. This finding indicates that using big data and technical analysis can help predict bitcoin returns that are hardly driven by fundamentals.},
	language = {en},
	number = {3},
	urldate = {2022-02-06},
	journal = {The Journal of Finance and Data Science},
	author = {Huang, Jing-Zhi and Huang, William and Ni, Jun},
	month = sep,
	year = {2019},
	keywords = {Bitcoin, Bitcoin return prediction, CART, Cryptocurrency, Decision tree classification, High-dimensional classification, Technical indicators},
	pages = {140--155},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/EQ22L2T8/S2405918818300928.html:text/html},
}

@article{neely_forecasting_2014,
	title = {Forecasting the {Equity} {Risk} {Premium}: {The} {Role} of {Technical} {Indicators}},
	volume = {60},
	issn = {0025-1909},
	shorttitle = {Forecasting the {Equity} {Risk} {Premium}},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2013.1838},
	doi = {10.1287/mnsc.2013.1838},
	abstract = {Academic research relies extensively on macroeconomic variables to forecast the U.S. equity risk premium, with relatively little attention paid to the technical indicators widely employed by practitioners. Our paper fills this gap by comparing the predictive ability of technical indicators with that of macroeconomic variables. Technical indicators display statistically and economically significant in-sample and out-of-sample predictive power, matching or exceeding that of macroeconomic variables. Furthermore, technical indicators and macroeconomic variables provide complementary information over the business cycle: technical indicators better detect the typical decline in the equity risk premium near business-cycle peaks, whereas macroeconomic variables more readily pick up the typical rise in the equity risk premium near cyclical troughs. Consistent with this behavior, we show that combining information from both technical indicators and macroeconomic variables significantly improves equity risk premium forecasts versus using either type of information alone. Overall, the substantial countercyclical fluctuations in the equity risk premium appear well captured by the combined information in technical indicators and macroeconomic variables.

Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2013.1838.

This paper was accepted by Wei Jiang, finance.},
	number = {7},
	urldate = {2022-02-06},
	journal = {Management Science},
	author = {Neely, Christopher J. and Rapach, David E. and Tu, Jun and Zhou, Guofu},
	month = jul,
	year = {2014},
	note = {Publisher: INFORMS},
	keywords = {asset allocation, business cycle, equity risk premium predictability, macroeconomic variables, momentum, moving averages, out-of-sample forecasts, sentiment, volume},
	pages = {1772--1791},
}

@inproceedings{aupetit_improving_2015,
	address = {Zürich, Switzerland},
	title = {Improving performances of the {AltaRica} 3.0 stochastic simulator},
	url = {https://hal.archives-ouvertes.fr/hal-01239379},
	doi = {10.1201/b19094-236},
	abstract = {This article presents the performance improvements we obtained on the AltaRica 3.0 stochastic simulator by using profiling techniques and testing it on a benchmark of models. This analysis showed that evaluating guards is one of the most time-consuming part of the execution. A selective update of the guards made it possible to improve significantly the performance. This work is a part of the OpenAltaRica project, which aims at developing a complete set of tools for the high level modeling language AltaRica 3.0.},
	urldate = {2022-01-11},
	booktitle = {Safety and {Reliability} of {Complex} {Engineered} {Systems}: {ESREL} 2015},
	author = {Aupetit, Benjamin and Batteux, Michel Batteux and Rauzy, Antoine and Roussel, Jean-Marc},
	month = sep,
	year = {2015},
	file = {Aupetit et al. - 2015 - Improving performances of the AltaRica 3.0 stochas.pdf:/home/roland/Zotero/storage/TRJNEVN3/Aupetit et al. - 2015 - Improving performances of the AltaRica 3.0 stochas.pdf:application/pdf},
}

@article{rauzy_guarded_2008,
	title = {Guarded transition systems: {A} new states/events formalism for reliability studies},
	volume = {222},
	issn = {1748-006X},
	shorttitle = {Guarded transition systems},
	url = {https://doi.org/10.1243/1748006XJRR177},
	doi = {10.1243/1748006XJRR177},
	abstract = {States/events formalisms, such as Markov graphs or Petri nets, are widely used in reliability engineering studies. They have proved to be a very powerful tool both from conceptual and practical viewpoints. This article introduces a new states/events formalism, the so-called guarded transition system. The guarded transition system generalizes both block diagrams and Petri nets. It also makes it possible to handle looped systems, which no existing formalism is able to handle smoothly. Its use is illustrated by means of examples and several important issues such as composition and graphical representations are discussed. It is shown that current assessment methods for Markov graphs and Petri nets can be improved to a guarded transition system without a significant change of complexity.},
	language = {en},
	number = {4},
	urldate = {2022-01-11},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
	author = {Rauzy, A B},
	month = dec,
	year = {2008},
	note = {Publisher: SAGE Publications},
	keywords = {modelling languages, system engineering, transition systems},
	pages = {495--505},
	file = {Rauzy - 2008 - Guarded transition systems A new statesevents fo.pdf:/home/roland/Zotero/storage/9JEVAXJI/Rauzy - 2008 - Guarded transition systems A new statesevents fo.pdf:application/pdf},
}

@inproceedings{zing_methodologie_2020,
	address = {Le Havre},
	title = {Méthodologie permettant de réaliser une étude {FMD} au niveau d’une ligne de métro complète},
	abstract = {L’arrivée des métros sans conducteurs s’accompagne de changements dans la gestion des risques d’indisponibilité. Ces évolutions mettent en évidence la nécessité de maîtriser tous les points critiques, d’un point de vue de la disponibilité, de ces systèmes ferroviaires complexes.},
	author = {Zing, Christophe and Ikhlef, Sabrina and Dufresne, Michel and Iningoue, Vanessa},
	month = oct,
	year = {2020},
	file = {ZING et al. - 2020 - Méthodologie permettant de réaliser une étude FMD .pdf:/home/roland/Zotero/storage/CZZGUA2X/ZING et al. - 2020 - Méthodologie permettant de réaliser une étude FMD .pdf:application/pdf},
}

@article{bao_intelligent_2008,
	title = {Intelligent stock trading system by turning point confirming and probabilistic reasoning},
	volume = {34},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417406003125},
	doi = {10.1016/j.eswa.2006.09.043},
	abstract = {Financial engineering such as trading decision is an emerging research area and also has great commercial potentials. A successful stock buying/selling generally occurs near price trend turning point. Traditional technical analysis relies on some statistics (i.e. technical indicators) to predict turning point of the trend. However, these indicators can not guarantee the accuracy of prediction in chaotic domain. In this paper, we propose an intelligent ﬁnancial trading system through a new approach: learn trading strategy by probabilistic model from high-level representation of time series – turning points and technical indicators. The main contributions of this paper are two-fold. First, we utilize high-level representation (turning point and technical indicators). High-level representation has several advantages such as insensitive to noise and intuitive to human being. However, it is rarely used in past research. Technical indicator is the knowledge from professional investors, which can generally characterize the market. Second, by combining high-level representation with probabilistic model, the randomness and uncertainty of chaotic system is further reduced. In this way, we achieve great results (comprehensive experiments on S\&P500 components) in a chaotic domain in which the prediction is thought impossible in the past.},
	language = {en},
	number = {1},
	urldate = {2021-12-29},
	journal = {Expert Systems with Applications},
	author = {Bao, Depei and Yang, Zehong},
	month = jan,
	year = {2008},
	pages = {620--627},
	file = {Bao et Yang - 2008 - Intelligent stock trading system by turning point .pdf:/home/roland/Zotero/storage/LAMUUN6C/Bao et Yang - 2008 - Intelligent stock trading system by turning point .pdf:application/pdf},
}

@article{chu_high_2020,
	title = {High frequency momentum trading with cryptocurrencies},
	volume = {52},
	issn = {02755319},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0275531919308062},
	doi = {10.1016/j.ribaf.2019.101176},
	abstract = {Over the past few years, cryptocurrencies have increasingly been discussed as alternatives to traditional fiat currencies. These digital currencies have garnered significant interest from investment banks and portfolio managers as a potential option to diversify the financial risk from investing in other assets. This interest has also extended to the general public who have seen cryptocurrencies as a way of making a quick profit. This paper provides a first insight into the applicability of high frequency momentum trading strategies for cryptocurrencies. We implemented two variations of a signal-based momentum trading strategy: (i) a time series method; (ii) a cross sectional method. These strategies were tested on a selection of seven of the largest cryptocurrencies ranked by market capitalization. The results show that there exists potential for the momentum strategy to be used successfully for cryptocurrency trading in a high frequency setting. A comparison with a passive portfolio strategy is proposed, which shows abnormal returns when compared with the momentum strategies. Furthermore, the robustness of our results are checked through the application of the momentum strategies other sample periods. We also compare the performances of the signal-based momentum strategies with returns-based versions of the strategies. It is shown that the signal-based strategy outperforms the returns-based strategy. However, there appears to be no single parameterization of the signal-based strategies that can generate the greatest cumulative return over all sample periods.},
	language = {en},
	urldate = {2021-12-29},
	journal = {Research in International Business and Finance},
	author = {Chu, Jeffrey and Chan, Stephen and Zhang, Yuanyuan},
	month = apr,
	year = {2020},
	pages = {101176},
	file = {Chu et al. - 2020 - High frequency momentum trading with cryptocurrenc.pdf:/home/roland/Zotero/storage/27CJPZ5V/Chu et al. - 2020 - High frequency momentum trading with cryptocurrenc.pdf:application/pdf},
}

@article{hassanniakalager_trading_2021,
	title = {Trading the foreign exchange market with technical analysis and {Bayesian} {Statistics}},
	volume = {63},
	issn = {09275398},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927539821000542},
	doi = {10.1016/j.jempfin.2021.07.006},
	abstract = {In this study, the profitability of technical analysis and Bayesian Statistics in trading the EUR/USD, GBP/USD, and USD/JPY exchange rates are examined. For this purpose, seven thousand eight hundred forty-six technical rules are generated, and their profitability is assessed through a data snooping procedure. Then, the most promising rules are combined with a Naïve Bayes, a Relevance Vector Machine, a Dynamic Model Averaging, a Dynamic Model Selection and a Bayesian regularized Neural Network model. The findings show that technical analysis has value in foreign exchange trading, but the profit margins are small. On the other hand, Bayesian Statistics seems to increase the profitability of technical rules up to five times.},
	language = {en},
	urldate = {2021-12-29},
	journal = {Journal of Empirical Finance},
	author = {Hassanniakalager, Arman and Sermpinis, Georgios and Stasinakis, Charalampos},
	month = sep,
	year = {2021},
	pages = {230--251},
	file = {Hassanniakalager et al. - 2021 - Trading the foreign exchange market with technical.pdf:/home/roland/Zotero/storage/5KRCQHQ3/Hassanniakalager et al. - 2021 - Trading the foreign exchange market with technical.pdf:application/pdf},
}

@article{batteux_altarica_2017,
	title = {{AltaRica} 3.0 assertions: {The} whys and wherefores},
	volume = {231},
	issn = {1748-006X},
	shorttitle = {{AltaRica} 3.0 assertions},
	url = {https://doi.org/10.1177/1748006X17728209},
	doi = {10.1177/1748006X17728209},
	abstract = {In discrete event simulations, the system is assumed to change of state when and only when an event occurs. This change of state can be more or less sophisticated depending on the modeling formalism. In this article, we discuss the whys and wherefores of the fixpoint assertion mechanism introduced in AltaRica 3.0 to perform changes of states. We show how it can be used to handle complex phenomena such as change in flow directions depending on the states of components. We propose an efficient implementation of this mechanism, thanks to ideas stemmed in theoretical computer science and artificial intelligence. We compare the AltaRica 3.0 approach with alternative ones, including those of the previous versions of the language.},
	language = {en},
	number = {6},
	urldate = {2021-12-29},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
	author = {Batteux, Michel and Prosvirnova, Tatiana and Rauzy, Antoine},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications},
	keywords = {AltaRica, assertions, Discrete event simulation},
	pages = {691--700},
}

@article{jimenez_semi-nonparametric_2022,
	title = {Semi-nonparametric risk assessment with cryptocurrencies},
	volume = {59},
	issn = {02755319},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0275531921001884},
	doi = {10.1016/j.ribaf.2021.101567},
	abstract = {This paper establishes a brand-new perspective of analyzing the risk of crypto assets through a semi-nonparametric approach, discussing its theoretical advantages and testing its performance compared to parametric approaches and in terms of backtesting techniques and different risk measures: Value-at-Risk, Expected Shortfall and Median Shortfall. Our comprehensive analysis for six cryptocurrencies shows that flexible semi-nonparametric approaches outperform risk mea­ sures of most crypto assets (particularly Bitcoin) and tend to provide the most conservative risk assessment. Furthermore, we propose the Median Shortfall as a robust-to-outliers and reliable risk measure for cryptocurrencies and discuss on the choice of the appropriate probability levels ac­ cording to the assumed distribution. The evidence supports that Median Shortfall at 98.31 \% and 98.51 \% confidence levels as accurate alternatives to Value-at-Risk at 99 \% and Expected Shortfall at 97.5 \%.},
	language = {en},
	urldate = {2021-12-28},
	journal = {Research in International Business and Finance},
	author = {Jiménez, Inés and Mora-Valencia, Andrés and Perote, Javier},
	month = jan,
	year = {2022},
	keywords = {\_tablet},
	pages = {101567},
	file = {Jiménez et al_2022_Semi-nonparametric risk assessment with cryptocurrencies.pdf:/home/roland/Zotero/storage/6TC6Z63P/Jiménez et al_2022_Semi-nonparametric risk assessment with cryptocurrencies.pdf:application/pdf},
}

@article{bendtsen_gated_2016,
	title = {Gated {Bayesian} networks for algorithmic trading},
	volume = {69},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X15001619},
	doi = {10.1016/j.ijar.2015.11.002},
	abstract = {This paper introduces a new probabilistic graphical model called gated Bayesian network (GBN). This model evolved from the need to represent processes that include several distinct phases. In essence, a GBN is a model that combines several Bayesian networks (BNs) in such a manner that they may be active or inactive during queries to the model. We use objects called gates to combine BNs, and to activate and deactivate them when predefined logical statements are satisfied. In this paper we also present an algorithm for semi-automatic learning of GBNs. We use the algorithm to learn GBNs that output buy and sell decisions for use in algorithmic trading systems. We show how the learnt GBNs can substantially lower risk towards invested capital, while they at the same time generate similar or better rewards, compared to the benchmark investment strategy buy-and-hold. We also explore some differences and similarities between GBNs and other related formalisms.},
	language = {en},
	urldate = {2021-12-27},
	journal = {International Journal of Approximate Reasoning},
	author = {Bendtsen, Marcus and Peña, Jose M.},
	month = feb,
	year = {2016},
	keywords = {Bayesian networks, Algorithmic trading, Decision support, Probabilistic graphical models},
	pages = {58--80},
	file = {Version soumise:/home/roland/Zotero/storage/P3JC9AHI/Bendtsen et Peña - 2016 - Gated Bayesian networks for algorithmic trading.pdf:application/pdf},
}

@article{zuo_updown_nodate,
	title = {Up/{Down} {Analysis} of {Stock} {Index} by {Using} {Bayesian} {Network}},
	abstract = {Bayesian network is the graphical model which can represent the stochastic dependency of the random variables via the acyclic directed graph. In this study, Bayesian network is applied for the up/down analysis of the stock index. The up/down rates of the daily stock indexes in three major markets are taken as the network nodes and then, the network is determined by K2 algorithm with the K2 metric as the prediction accuracy of the network. The present algorithm is applied for predicting the up/down analysis of the daily stock indeies in 2007 and the results are compared with the traditional algorithms; Psychological line and trend estimation, which are popular algorithms which are well-known by the traders. Their accuracy comparison shows that the average correction rate of the present algorithm is almost 60\%, which is almost equal or higher than them of the traditional algorithms such as the psychological line (50-59\%) and the trend estimation (50-52\%). Moreover, the vertical trading results reveal that the profit of the present algorithm is much greater than the others.},
	journal = {Engineering Management Research, Issue},
	author = {Zuo, Yi and Kita, Eisuke},
	keywords = {\_tablet\_modified},
	pages = {46--52},
	file = {Citeseer - Snapshot:/home/roland/Zotero/storage/9JLQ8IC9/summary.html:text/html;Zuo_Kita_Up-Down Analysis of Stock Index by Using Bayesian Network.pdf:/home/roland/Zotero/storage/ZM7GZEYS/Zuo_Kita_Up-Down Analysis of Stock Index by Using Bayesian Network.pdf:application/pdf},
}

@inproceedings{chang_market_2015,
	title = {Market analysis and trading strategies with {Bayesian} networks},
	abstract = {This paper examines the application of data fusion and probabilistic reasoning for investment decision and its performance evaluation. Specifically, Bayesian networks are used to model the qualitative and quantitative relationships between various factors that affect the dynamics of equity index (S\&P 500) for predictive analysis. The resulting assessments are applied to trading decisions utilizing derivatives such as S\&P futures and options. The simulated trading performance results demonstrate the effectiveness of the Bayesian network approach.},
	booktitle = {2015 18th {International} {Conference} on {Information} {Fusion} ({Fusion})},
	author = {Chang, K C and Tian, Zhi},
	month = jul,
	year = {2015},
	keywords = {Predictive models, Bayesian networks, Market research, Bayes methods, Data models, financial markets, futures options trading, Indexes, Investment, Portfolios, return and risk analysis, S\&P futures},
	pages = {1922--1929},
	file = {IEEE Xplore Abstract Record:/home/roland/Zotero/storage/UWTZYU2Y/7266790.html:text/html},
}

@article{bouri_trading_2019,
	title = {Trading volume and the predictability of return and volatility in the cryptocurrency market},
	volume = {29},
	issn = {1544-6123},
	url = {https://www.sciencedirect.com/science/article/pii/S1544612318303519},
	doi = {10.1016/j.frl.2018.08.015},
	abstract = {We extend our limited understanding on the Granger causality from trading volume to the returns and volatility in the cryptocurrency market via a copula-quantile causality approach. Using daily data of seven leading cryptocurrencies (Bitcoin, Ripple, Ethereum, Litecoin, Nem, Dash, and Stellar), results show that trading volume Granger causes extreme negative and positive returns of all cryptocurrencies under study. However, volume Granger causes return volatility for only three cryptocurrencies (Litecoin, NEM, and Dash) when the volatility is low. However, this latter result only holds when squared returns are used as a proxy of volatility and not when GARCH volatility is employed.},
	language = {en},
	urldate = {2021-12-27},
	journal = {Finance Research Letters},
	author = {Bouri, Elie and Lau, Chi Keung Marco and Lucey, Brian and Roubaud, David},
	month = jun,
	year = {2019},
	keywords = {Cryptocurrency, Copula-quantile causality, Return, Trading volume, Volatility},
	pages = {340--346},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/RANGBU8R/S1544612318303519.html:text/html;Version soumise:/home/roland/Zotero/storage/77DC6AG3/Bouri et al. - 2019 - Trading volume and the predictability of return an.pdf:application/pdf},
}

@article{balcilar_can_2017,
	title = {Can volume predict {Bitcoin} returns and volatility? {A} quantiles-based approach},
	volume = {64},
	issn = {0264-9993},
	shorttitle = {Can volume predict {Bitcoin} returns and volatility?},
	url = {https://www.sciencedirect.com/science/article/pii/S0264999317304558},
	doi = {10.1016/j.econmod.2017.03.019},
	abstract = {Prior studies on the price formation in the Bitcoin market consider the role of Bitcoin transactions at the conditional mean of the returns distribution. This study employs in contrast a non-parametric causality-in-quantiles test to analyse the causal relation between trading volume and Bitcoin returns and volatility, over the whole of their respective conditional distributions. The nonparametric characteristics of our test control for misspecification due to nonlinearity and structural breaks, two features of our data that cover 19th December 2011 to 25th April 2016. The causality-in-quantiles test reveals that volume can predict returns – except in Bitcoin bear and bull market regimes. This result highlights the importance of modelling nonlinearity and accounting for the tail behaviour when analysing causal relationships between Bitcoin returns and trading volume. We show, however, that volume cannot help predict the volatility of Bitcoin returns at any point of the conditional distribution.},
	language = {en},
	urldate = {2021-12-27},
	journal = {Economic Modelling},
	author = {Balcilar, Mehmet and Bouri, Elie and Gupta, Rangan and Roubaud, David},
	month = aug,
	year = {2017},
	keywords = {Bitcoin, Volatility, C22, G15, Nonparametric quantile causality, Returns, Volume},
	pages = {74--81},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/MVZM4HF8/S0264999317304558.html:text/html;Version soumise:/home/roland/Zotero/storage/VC46FE6F/Balcilar et al. - 2017 - Can volume predict Bitcoin returns and volatility.pdf:application/pdf},
}

@article{peter_chung_predictability_1996,
	title = {The predictability of stock returns – a nonparametric approach},
	volume = {15},
	issn = {0747-4938},
	url = {https://doi.org/10.1080/07474939608800357},
	doi = {10.1080/07474939608800357},
	abstract = {This paper reexamines the predictability of stock returns with a nonparametric model. We first identify, through a set of diagnostic tests, five lagged predictive factors from a linear model. Using these factors, we predict one-month-ahead stock index returns with a nonparametric approach. We find that our nonparametricmodel. We first identify, through a set of diagnostic tests, five lagged predictive factors from a linear model. Using these factors, we predict on -month-ahead stock index returns with a nonparametric approach. We find that our nonparametric model can correctly predict about 74\% of stock index return signs. With various ex ante trading rules based on nonparametric predictions and transaction cost schedules, we then compare the performance of "managed" portfolios with that of the buy and hold portfolios. We fmd that the managed portfolios are mean-variance dominant over the buy-and-hold strategies when no or low transaction costs are assumed. When high transaction costs are assumed instead, the mean-variance dominance diminishes However,the Sharpe index of risk-adjusted portfolio performanceindicates that the managed portfolios significantly outperform the buy-and-hold strategies even for the high-transaction cost scenario. We show that the difference in performance between the managed portfolios and the buy-and-hold strategies can be partially explained by the January effect or the small firm effect. In sum, this paper demonstrates the merits of using a nonparametric approach for predicting stock returns and testing market efficiency.},
	number = {3},
	urldate = {2021-12-27},
	journal = {Econometric Reviews},
	author = {Peter Chung, Y. and Zhou, Zhong-guo},
	month = jan,
	year = {1996},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07474939608800357},
	keywords = {Market Efficiency, Nonparametric Prediction, Stock Return},
	pages = {299--330},
	file = {Snapshot:/home/roland/Zotero/storage/NAB29JSW/07474939608800357.html:text/html},
}

@article{abramov_probabilistic_2008,
	title = {A probabilistic analysis of the trading the line strategy},
	volume = {8},
	issn = {1469-7688},
	url = {https://doi.org/10.1080/14697680701489427},
	doi = {10.1080/14697680701489427},
	abstract = {We provide analytic models for which the appropriate statistics of the trading the line strategy, N h , can be derived in closed form. In particular, we provide closed-form expressions concerning the average duration of the open position, E(N h ), the variance of the open duration, Var(N h ), the average of the stopped log price, E(S N h ), the variance of the stopped log price, Var(S N h ), the correlation, Corr(N h , S N h ), and the Laplace transform, E(e−s N h ). These results are obtained, in discrete time settings, for binomial and other price scenarios. Furthermore, when analytic results are not possible, such as the case of a normal distribution for log returns, we show by simulation that our general conclusions still hold. Using these statistics we point out some of the subtle features of the trailing stops strategy.},
	number = {5},
	urldate = {2021-12-27},
	journal = {Quantitative Finance},
	author = {Abramov, V. and Khan, M. K. and Khan, R. A.},
	month = aug,
	year = {2008},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/14697680701489427},
	keywords = {Binomial model, Cumulative sum procedure, Financial securities, Geometric random walk, Long position, Short position, SPRT, Trailing stops strategy, Trinomial model},
	pages = {499--512},
	file = {Snapshot:/home/roland/Zotero/storage/MX4MXDQB/14697680701489427.html:text/html;Version soumise:/home/roland/Zotero/storage/AVHY7V4Q/Abramov et al. - 2008 - A probabilistic analysis of the trading the line s.pdf:application/pdf},
}

@misc{hsv-porky_energy_2021,
	title = {The {Energy} {Web} {Foundation}, {Rocky} {Mountain} {Institute} \& {The} {Alliance} {For} {Innovative} {Regulation} {Have}…},
	url = {https://pedroporky.medium.com/the-energy-web-foundation-join-the-crypto-climate-accord-to-decarbonize-the-cryptocurrency-industry-b1bb9911d41e},
	abstract = {Some Highlights From The Announcement},
	language = {en},
	urldate = {2021-11-29},
	journal = {Medium},
	author = {HSV-PORKY},
	month = apr,
	year = {2021},
	file = {Snapshot:/home/roland/Zotero/storage/HEMYTAM7/the-energy-web-foundation-join-the-crypto-climate-accord-to-decarbonize-the-cryptocurrency-indu.html:text/html},
}

@misc{noauthor_solution_2021,
	title = {Solution durable ou catastrophe climatique ? {Les} dangers et les promesses de la technologie des crypto-monnaies},
	shorttitle = {Solution durable ou catastrophe climatique ?},
	url = {https://news.un.org/fr/story/2021/06/1099092},
	abstract = {L'impact environnemental négatif des crypto-monnaies telles que Bitcoin a été largement couvert par la presse au cours des dernières semaines et des derniers mois, et leur volatilité a également été signalée comme une source de préoccupation.},
	language = {fr},
	urldate = {2021-11-29},
	journal = {ONU Info},
	month = jun,
	year = {2021},
	file = {Snapshot:/home/roland/Zotero/storage/6LSNWPGI/1099092.html:text/html},
}

@article{snow_machine_2020,
	title = {Machine {Learning} in {Asset} {Management} - {Part} 1 : {Portfolio} {Construction} - {Trading} {Strategies}},
	volume = {2},
	issn = {2640-3943},
	shorttitle = {Machine {Learning} in {Asset} {Management}— \textit{{Part} 1}},
	url = {http://jfds.pm-research.com/lookup/doi/10.3905/jfds.2019.1.021},
	doi = {10.3905/jfds.2019.1.021},
	abstract = {This is the first in a series of articles dealing with machine learning in asset management. Asset management can be broken into the following tasks: (1) portfolio construction, (2) risk management, (3) capital management, (4) infrastructure and deployment, and (5) sales and marketing. This article focuses on portfolio construction using machine learning. Historically, algorithmic trading could be more narrowly defined as the automation of sell-side trade execution, but since the introduction of more advanced algorithms, the definition has grown to include idea generation, alpha factor design, asset allocation, position sizing, and the testing of strategies. Machine learning, from the vantage of a decision-making tool, can help in all these areas.},
	language = {en},
	number = {1},
	urldate = {2021-11-14},
	journal = {The Journal of Financial Data Science},
	author = {Snow, Derek},
	month = jan,
	year = {2020},
	pages = {10--23},
	file = {Snow - 2020 - Machine Learning in Asset Management— Part 1i.pdf:/home/roland/Zotero/storage/Z66PWDTQ/Snow - 2020 - Machine Learning in Asset Management— Part 1i.pdf:application/pdf},
}

@inproceedings{pokhrel_digital_2020,
	address = {New York, NY, USA},
	series = {{ICSEW}'20},
	title = {Digital {Twin} for {Cybersecurity} {Incident} {Prediction}: {A} {Multivocal} {Literature} {Review}},
	isbn = {978-1-4503-7963-2},
	shorttitle = {Digital {Twin} for {Cybersecurity} {Incident} {Prediction}},
	url = {https://doi.org/10.1145/3387940.3392199},
	doi = {10.1145/3387940.3392199},
	abstract = {The advancements in the field of internet of things, artificial intelligence, machine learning, and data analytics has laid the path to the evolution of digital twin technology. The digital twin is a high-fidelity digital model of a physical system or asset that can be used e.g. to optimize operations and predict faults of the physical system. To understand different use cases of digital twin and its potential for cybersecurity incident prediction, we have performed a Systematic Literature Review (SLR). In this paper, we summarize the definition of digital twin and state-of-the-art on the development of digital twin including reported work on the usability of a digital twin for cybersecurity. Existing tools and technologies for developing digital twin is discussed.},
	urldate = {2021-11-14},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {Association for Computing Machinery},
	author = {Pokhrel, Abhishek and Katta, Vikash and Colomo-Palacios, Ricardo},
	month = jun,
	year = {2020},
	keywords = {Cybersecurity, Digital Twin, Fault detection, Incident prediction, IoT, Multivocal literature review},
	pages = {671--678},
}

@article{suhail_blockchain-based_2021,
	title = {Blockchain-based {Digital} {Twins}: {Research} {Trends}, {Issues}, and {Future} {Challenges}},
	shorttitle = {Blockchain-based {Digital} {Twins}},
	url = {http://arxiv.org/abs/2103.11585},
	abstract = {Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross-validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, Digital Twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use-cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We highlight the role of Artificial Intelligence (AI) in blockchain-based DTs. Furthermore, we discuss current and future research and deployment challenges of blockchain-supported DTs that require further investigation.},
	urldate = {2021-11-14},
	journal = {arXiv:2103.11585 [cs]},
	author = {Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Matulevičius, Raimundas and Hong, Choong Seon},
	month = sep,
	year = {2021},
	note = {arXiv: 2103.11585},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/roland/Zotero/storage/Q2B8VXCI/Suhail et al. - 2021 - Blockchain-based Digital Twins Research Trends, I.pdf:application/pdf;arXiv.org Snapshot:/home/roland/Zotero/storage/3MKMNE4D/2103.html:text/html},
}

@inproceedings{holmes_digital_2021,
	title = {Digital {Twins} and {Cyber} {Security} -solution or challenge?},
	doi = {10.1109/SEEDA-CECNSM53056.2021.9566277},
	abstract = {Digital twin technology today is diverse and emerging and its full potential is not yet widely understood. The concept of a digital twin allows for the analysis, design, opti-misation and evolution of systems to take place fully digital, or in conjunction with a cyber-physical system to improve speed, accuracy and efficiency when compared to traditional engineering approaches. Digital twins continue to be a technology that enables new paradigms, such as Industry 4.0 and Factories of the Future as well as generating improved efficiencies within existing systems. The development of digital twin technology in traditional industries such as manufacturing, construction, the automotive industry, agriculture and transportation has highlighted its potential, but often insufficiently explored the risks associated with their integration. In this paper we explore risks relating to the cyber-security of systems employing digital twin technology and also consider the opportunities for digital twins themselves to mitigate cyber-security risks and become an integral part of a security in-depth defence.},
	author = {Holmes, David and Papathanasaki, Maria and Maglaras, Leandros and Ferrag, Mohamed Amine and Nepal, Surya and Janicke, Helge},
	month = aug,
	year = {2021},
	file = {Full Text PDF:/home/roland/Zotero/storage/TL9KDB28/Holmes et al. - 2021 - Digital Twins and Cyber Security -solution or chal.pdf:application/pdf},
}

@article{thompson_digital_nodate,
	title = {Digital twinning of ship structural fatigue: state of the art review and strategic research agenda},
	language = {en},
	author = {Thompson, Ian},
	pages = {39},
	file = {Thompson - Digital twinning of ship structural fatigue state.pdf:/home/roland/Zotero/storage/LWR2QD5J/Thompson - Digital twinning of ship structural fatigue state.pdf:application/pdf},
}

@book{sharma_digital_2020,
	title = {Digital {Twins}: {State} of the {Art} {Theory} and {Practice}, {Challenges}, and {Open} {Research} {Questions}},
	shorttitle = {Digital {Twins}},
	abstract = {Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation and forecasting. However, the theoretical framework and practical implementations of digital twins (DT) are still far from this vision. Although successful implementations exist, sufficient implementation details are not publicly available, therefore it is difficult to assess their effectiveness, draw comparisons and jointly advance the DT methodology. This work explores the various DT features and current approaches, the shortcomings and reasons behind the delay in the implementation and adoption of digital twin. Advancements in machine learning, internet of things and big data have contributed hugely to the improvements in DT with regards to its real-time monitoring and forecasting properties. Despite this progress and individual company-based efforts, certain research gaps exist in the field, which have caused delay in the widespread adoption of this concept. We reviewed relevant works and identified that the major reasons for this delay are the lack of a universal reference framework, domain dependence, security concerns of shared data, reliance of digital twin on other technologies, and lack of quantitative metrics. We define the necessary components of a digital twin required for a universal reference framework, which also validate its uniqueness as a concept compared to similar concepts like simulation, autonomous systems, etc. This work further assesses the digital twin applications in different domains and the current state of machine learning and big data in it. It thus answers and identifies novel research questions, both of which will help to better understand and advance the theory and practice of digital twins.},
	author = {Sharma, Angira and Kosasih, Edward and Zhang, Jie and Brintrup, Alexandra and Calinescu, Anisoara},
	month = nov,
	year = {2020},
	file = {Full Text PDF:/home/roland/Zotero/storage/4X2XJRUU/Sharma et al. - 2020 - Digital Twins State of the Art Theory and Practic.pdf:application/pdf},
}

@book{fuller_digital_2020,
	title = {Digital {Twin}: {Enabling} {Technologies}, {Challenges} and {Open} {Research}},
	shorttitle = {Digital {Twin}},
	abstract = {Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins.

This article has been accepted for publication in a future issue of IEEE Access. Citation information: DOI 10.1109/ACCESS.2020.2998358, IEEE Access},
	author = {Fuller, Aidan and Fan, Zhong and Day, Charles},
	month = may,
	year = {2020},
	file = {Full Text PDF:/home/roland/Zotero/storage/K7VP3U9Z/Fuller et al. - 2020 - Digital Twin Enabling Technologies, Challenges an.pdf:application/pdf},
}

@article{wagner_challenges_2019,
	series = {29th {CIRP} {Design} {Conference} 2019, 08-10 {May} 2019, {Póvoa} de {Varzim}, {Portgal}},
	title = {Challenges and {Potentials} of {Digital} {Twins} and {Industry} 4.0 in {Product} {Design} and {Production} for {High} {Performance} {Products}},
	volume = {84},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827119308637},
	doi = {10.1016/j.procir.2019.04.219},
	abstract = {Digital twins offer great opportunities in various domains of the product engineering process. However, current approaches to the use of digital twins only focus on different separated disciplines. In contrast to that, it is expected that the holistic use of digital twin models in product development and production will dominate future product generations, because they allow to create high-performance products competitively. This paper explores important challenges and future potentials of digital twins and Industry 4.0 for the seamless integration of product specification and production. In this regard, approaches of linking digital twins to other domains open up new possibilities in tolerance allocation and production integration. Thereby, the most efficient product specifications in technical and economic terms are achieved for the manufacturer. In addition, the connectivity of Industry 4.0 broadens the scope and enables the evaluation of alternative approaches in production planning and control. Approaches at the organizational level, product functions with specifications beyond the technological limits and production control strategies (e.g. order dispatching) ensure high performance operations. Simulations with a digital production twin with integrated digital product twin allow early estimations even before the actual ramp-up of the product. The future challenge addressed in this paper is to define a consistent framework for the holistic use of digital twins in the entire product development process, which requires the integration of product designers and production planner concepts.},
	language = {en},
	urldate = {2021-11-14},
	journal = {Procedia CIRP},
	author = {Wagner, Raphael and Schleich, Benjamin and Haefner, Benjamin and Kuhnle, Andreas and Wartzack, Sandro and Lanza, Gisela},
	month = jan,
	year = {2019},
	keywords = {Production planning, Information, Product development},
	pages = {88--93},
	file = {ScienceDirect Full Text PDF:/home/roland/Zotero/storage/SL97IRE8/Wagner et al. - 2019 - Challenges and Potentials of Digital Twins and Ind.pdf:application/pdf},
}

@article{cohen_optimizing_2021,
	title = {Optimizing {Algorithmic} {Strategies} for {Trading} {Bitcoin}},
	volume = {57},
	doi = {10.1007/s10614-020-09972-6},
	abstract = {This research tries to establish to what extent three popular algorithmic systems for trading financial assets: the relative strength index, the moving average convergence diversion (MACD) and the pivot reversal (PR), are suitable for Bitcoin trading. Using data about daily Bitcoin prices from the beginning of April 2013 until the end of October 2018, we explored these strategies through particle swarm optimization. Our results demonstrate that the relative strength index produced poorer results than the buy and hold strategy. In contrast, the MACD and PR strategies dramatically outperformed the buy and hold strategy. However, our optimizing process produced even better results.},
	journal = {Computational Economics},
	author = {Cohen, Gil},
	month = feb,
	year = {2021},
}

@article{pricope_deep_2021,
	title = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}: {A} {Review}},
	shorttitle = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}},
	url = {http://arxiv.org/abs/2106.00123},
	abstract = {Algorithmic stock trading has become a staple in today’s ﬁnancial market, the majority of trades being now fully automated. Deep Reinforcement Learning (DRL) agents proved to be to a force to be reckon with in many complex games like Chess and Go. We can look at the stock market historical price series and movements as a complex imperfect information environment in which we try to maximize return - proﬁt and minimize risk. This paper reviews the progress made so far with deep reinforcement learning in the subdomain of AI in ﬁnance, more precisely, automated low-frequency quantitative stock trading. Many of the reviewed studies had only proof-of-concept ideals with experiments conducted in unrealistic settings and no real-time trading applications. For the majority of the works, despite all showing statistically signiﬁcant improvements in performance compared to established baseline strategies, no decent proﬁtability level was obtained. Furthermore, there is a lack of experimental testing in real-time, online trading platforms and a lack of meaningful comparisons between agents built on diﬀerent types of DRL or human traders. We conclude that DRL in stock trading has showed huge applicability potential rivalling professional traders under strong assumptions, but the research is still in the very early stages of development.},
	language = {en},
	urldate = {2021-11-14},
	journal = {arXiv:2106.00123 [cs, q-fin]},
	author = {Pricope, Tidor-Vlad},
	month = may,
	year = {2021},
	note = {arXiv: 2106.00123},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Trading and Market Microstructure},
}

@book{pricope_deep_2021-1,
	title = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}: {A} {Review}},
	shorttitle = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}},
	abstract = {Algorithmic stock trading has become a staple in today's financial market, the majority of trades being now fully automated. Deep Reinforcement Learning (DRL) agents proved to be to a force to be reckon with in many complex games like Chess and Go. We can look at the stock market historical price series and movements as a complex imperfect information environment in which we try to maximize return - profit and minimize risk. This paper reviews the progress made so far with deep reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading. Many of the reviewed studies had only proof-of-concept ideals with experiments conducted in unrealistic settings and no real-time trading applications. For the majority of the works, despite all showing statistically significant improvements in performance compared to established baseline strategies, no decent profitability level was obtained. Furthermore, there is a lack of experimental testing in real-time, online trading platforms and a lack of meaningful comparisons between agents built on different types of DRL or human traders. We conclude that DRL in stock trading has showed huge applicability potential rivalling professional traders under strong assumptions, but the research is still in the very early stages of development.},
	author = {Pricope, Tidor-Vlad},
	month = may,
	year = {2021},
	keywords = {\_tablet},
	file = {Pricope_2021_Deep Reinforcement Learning in Quantitative Algorithmic Trading.pdf:/home/roland/Zotero/storage/BCAN5RRA/Pricope_2021_Deep Reinforcement Learning in Quantitative Algorithmic Trading.pdf:application/pdf},
}

@article{de_nicola_intraday_2021,
	title = {On the {Intraday} {Behavior} of {Bitcoin}},
	volume = {6},
	issn = {2379-5980},
	url = {http://ledger.pitt.edu/ojs/ledger/article/view/213},
	doi = {10.5195/ledger.2021.213},
	abstract = {We analyze the intraday time series of Bitcoin, comparing its features with those of traditional ﬁnancial assets such as stocks and exchange rates. The results shed light on similarities as well as signiﬁcant deviations from the standard patterns. In particular, our most interesting ﬁnding is the unusual presence of signiﬁcant negative ﬁrst-order autocorrelation of returns calculated on medium-frequency timeframes, such as one, two and four hours, signaling the presence of systematic mean reversion. It is also found that larger price movements lead to stronger reversals, in percentage terms. We ﬁnally point out the potential exploitability of the phenomenon by implementing a basic algorithmic trading strategy and retroactively applying it to the data. We explain the ﬁndings mainly through (i) investor and trader overreaction, (ii) excess volatility and (iii) cascading liquidations due to excessive use of leverage by market participants.},
	language = {en},
	urldate = {2021-11-14},
	journal = {Ledger},
	author = {De Nicola, Giacomo},
	month = jul,
	year = {2021},
	file = {De Nicola - 2021 - On the Intraday Behavior of Bitcoin.pdf:/home/roland/Zotero/storage/CEHKF296/De Nicola - 2021 - On the Intraday Behavior of Bitcoin.pdf:application/pdf},
}

@book{hansen_periodicity_2021,
	title = {Periodicity in {Cryptocurrency} {Volatility} and {Liquidity}},
	abstract = {We study recurrent patterns in volatility and volume for major cryptocurrencies, Bitcoin and Ether, using data from two centralized exchanges (Coinbase Pro and Binance) and a decentralized exchange (Uniswap V2). We find systematic patterns in both volatility and volume across day-of-the-week, hour-of-the-day, and within the hour. These patterns have grown stronger over the years and can be related to algorithmic trading and funding times in futures markets. We also document that price formation mainly takes place on the centralized exchanges while price adjustments on the decentralized exchanges can be sluggish.},
	author = {Hansen, Peter and Kim, Chan and Kimbrough, Wade},
	month = sep,
	year = {2021},
	file = {Hansen et al. - 2021 - Periodicity in Cryptocurrency Volatility and Liqui.pdf:/home/roland/Zotero/storage/G8TIE3JW/Hansen et al. - 2021 - Periodicity in Cryptocurrency Volatility and Liqui.pdf:application/pdf},
}

@article{cohen_optimizing_2021-1,
	title = {Optimizing candlesticks patterns for {Bitcoin}'s trading systems},
	volume = {57},
	doi = {10.1007/s11156-021-00973-6},
	abstract = {In this research we make the first attempt to construct automated Bitcoin trading systems that are based on classical candlesticks patterns. We than tried to alter the classical formations for better trading results. Our data consists Bitcoins prices from the beginning of 2012 till the end of July 2020. WE found that that out of the tree classical candlesticks pattern only Engulfing has been fertile in predicting Bitcoin's price trends shifts. The classical Engulfing pattern generated Profit Factor (PF) of 3.54 and \$38,349 Net Profit (NP). We also found that using a strength proxy of 0.9\% may improve the classical pattern results. The classical Harami formation failed to produce positive trading results and therefore does not fit Bitcoin trading platforms. On the other hand, a reversed Harami was proven to be a fertile Bitcoin trading strategy. Our research also finds that classical four bars Kicker signals seldom appears and therefore cannot help Bitcoin traders. On the other hand, a reversed Kicker pattern has found to be a winning strategy with relatively low risk. This strategy fits particularly long positions with 74.36\% Percent of Profitable (PP) trades and 6.92 PF. We also find that all the examined candlesticks patterns better predict long trends than short trends.},
	journal = {Review of Quantitative Finance and Accounting},
	author = {Cohen, Gil},
	month = oct,
	year = {2021},
}

@article{min_systemic_2021,
	title = {Systemic failures and organizational risk management in algorithmic trading: {Normal} accidents and high reliability in financial markets},
	shorttitle = {Systemic failures and organizational risk management in algorithmic trading},
	doi = {10.1177/03063127211048515},
	abstract = {This article examines algorithmic trading and some key failures and risks associated with it, including so-called algorithmic 'flash crashes'. Drawing on documentary sources, 189 interviews with market participants, and fieldwork conducted at an algorithmic trading firm, we argue that automated markets are characterized by tight coupling and complex interactions, which render them prone to large-scale technological accidents, according to Perrow's normal accident theory. We suggest that the implementation of ideas from research into high-reliability organizations offers a way for trading firms to curb some of the technological risk associated with algorithmic trading. Paradoxically, however, certain systemic conditions in markets can allow individual firms' high-reliability practices to exacerbate market instability, rather than reduce it. We therefore conclude that in order to make automated markets more stable (and curb the impact of failures), it is important to both widely implement reliability-enhancing practices in trading firms and address the systemic risks that follow from the tight coupling and complex interactions of markets.},
	journal = {Social studies of science},
	author = {Min, Bo and Borch, Christian},
	month = oct,
	year = {2021},
	pages = {3063127211048515},
	file = {Min et Borch - 2021 - Systemic failures and organizational risk manageme.pdf:/home/roland/Zotero/storage/V2UKEN6A/Min et Borch - 2021 - Systemic failures and organizational risk manageme.pdf:application/pdf},
}

@book{crone_exploration_2021,
	title = {Exploration of {Algorithmic} {Trading} {Strategies} for the {Bitcoin} {Market}},
	abstract = {Bitcoin is firmly becoming a mainstream asset in our global society. Its highly volatile nature has traders and speculators flooding into the market to take advantage of its significant price swings in the hope of making money. This work brings an algorithmic trading approach to the Bitcoin market to exploit the variability in its price on a day-to-day basis through the classification of its direction. Building on previous work, in this paper, we utilise both features internal to the Bitcoin network and external features to inform the prediction of various machine learning models. As an empirical test of our models, we evaluate them using a real-world trading strategy on completely unseen data collected throughout the first quarter of 2021. Using only a binary predictor, at the end of our three-month trading period, our models showed an average profit of 86{\textbackslash}\%, matching the results of the more traditional buy-and-hold strategy. However, after incorporating a risk tolerance score into our trading strategy by utilising the model's prediction confidence scores, our models were 12.5{\textbackslash}\% more profitable than the simple buy-and-hold strategy. These results indicate the credible potential that machine learning models have in extracting profit from the Bitcoin market and act as a front-runner for further research into real-world Bitcoin trading.},
	author = {Crone, Nathan and Brophy, Eoin and Ward, Tomas},
	month = oct,
	year = {2021},
	file = {Crone et al. - 2021 - Exploration of Algorithmic Trading Strategies for .pdf:/home/roland/Zotero/storage/NHXCHAVZ/Crone et al. - 2021 - Exploration of Algorithmic Trading Strategies for .pdf:application/pdf},
}

@article{borch_machine_2021,
	title = {Machine learning and social theory: {Collective} machine behaviour in algorithmic trading},
	shorttitle = {Machine learning and social theory},
	doi = {10.1177/13684310211056010},
	abstract = {This article examines what the rise in machine learning (ML) systems might mean for social theory. Focusing on financial markets, in which algorithmic securities trading founded on ML-based decision-making is gaining traction, I discuss the extent to which established sociological notions remain relevant or demand a reconsideration when applied to an ML context. I argue that ML systems have some capacity for agency and for engaging in forms of collective machine behaviour, in which ML systems interact with other machines. However, ML-based collective machine behaviour is irreducible to human decision-making and thereby challenges established sociological notions of financial markets (including that of embeddedness). I argue that such behaviour can nonetheless be analysed through an adaptation of sociological theories of interaction and collective behaviour.},
	journal = {European Journal of Social Theory},
	author = {Borch, Christian},
	month = nov,
	year = {2021},
	pages = {136843102110560},
	file = {Borch - 2021 - Machine learning and social theory Collective mac.pdf:/home/roland/Zotero/storage/XTY99WC5/Borch - 2021 - Machine learning and social theory Collective mac.pdf:application/pdf},
}

@article{noauthor_pdf_nodate,
	title = {({PDF}) {Machine} learning and social theory: {Collective} machine behaviour in algorithmic trading},
	shorttitle = {({PDF}) {Machine} learning and social theory},
	url = {https://www.researchgate.net/publication/355850025_Machine_learning_and_social_theory_Collective_machine_behaviour_in_algorithmic_trading},
	doi = {10.1177/13684310211056010},
	abstract = {PDF {\textbar} This article examines what the rise in machine learning (ML) systems might mean for social theory. Focusing on financial markets, in which... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2021-11-14},
	journal = {ResearchGate},
	file = {Snapshot:/home/roland/Zotero/storage/VNCF2HRL/355850025_Machine_learning_and_social_theory_Collective_machine_behaviour_in_algorithmic_tradin.html:text/html;Texte intégral:/home/roland/Zotero/storage/7MNR7RBK/(PDF) Machine learning and social theory Collecti.pdf:application/pdf},
}

@article{park_practical_2021,
	title = {Practical {Algorithmic} {Trading} {Using} {State} {Representation} {Learning} and {Imitative} {Reinforcement} {Learning}},
	volume = {PP},
	doi = {10.1109/ACCESS.2021.3127209},
	abstract = {Algorithmic trading allows investors to avoid emotional and irrational trading decisions and helps them make profits using modern computer technology. In recent years, reinforcement learning has yielded promising results for algorithmic trading. Two prominent challenges in algorithmic trading with reinforcement learning are (1) extracting robust features and (2) learning a profitable trading policy. Another challenge is that it was previously often assumed that both long and short positions are always possible in stock trading; however, taking a short position is risky or sometimes impossible in practice. We propose a practical algorithmic trading method, SIRL-Trader, which achieves good profit using only long positions. SIRL-Trader uses offline/online state representation learning (SRL) and imitative reinforcement learning. In offline SRL, we apply dimensionality reduction and clustering to extract robust features whereas, in online SRL, we co-train a regression model with a reinforcement learning model to provide accurate state information for decision-making. In imitative reinforcement learning, we incorporate a behavior cloning technique with the twin-delayed deep deterministic policy gradient (TD3) algorithm and apply multistep learning and dynamic delay to TD3. The experimental results show that SIRL-Trader yields higher profits and offers superior generalization ability compared with state-of-the-art methods.},
	journal = {IEEE Access},
	author = {Park, Deog-Yeong and Lee, Ki-Hoon},
	month = nov,
	year = {2021},
	pages = {1--1},
	file = {Park et Lee - 2021 - Practical Algorithmic Trading Using State Represen.pdf:/home/roland/Zotero/storage/NT4Y58EJ/Park et Lee - 2021 - Practical Algorithmic Trading Using State Represen.pdf:application/pdf},
}

@article{nehemya_taking_2021,
	title = {Taking {Over} the {Stock} {Market}: {Adversarial} {Perturbations} {Against} {Algorithmic} {Traders}},
	shorttitle = {Taking {Over} the {Stock} {Market}},
	url = {http://arxiv.org/abs/2010.09246},
	abstract = {In recent years, machine learning has become prevalent in numerous tasks, including algorithmic trading. Stock market traders utilize machine learning models to predict the market's behavior and execute an investment strategy accordingly. However, machine learning models have been shown to be susceptible to input manipulations called adversarial examples. Despite this risk, the trading domain remains largely unexplored in the context of adversarial learning. In this study, we present a realistic scenario in which an attacker influences algorithmic trading systems by using adversarial learning techniques to manipulate the input data stream in real time. The attacker creates a universal perturbation that is agnostic to the target model and time of use, which, when added to the input stream, remains imperceptible. We evaluate our attack on a real-world market data stream and target three different trading algorithms. We show that when added to the input stream, our perturbation can fool the trading algorithms at future unseen data points, in both white-box and black-box settings. Finally, we present various mitigation methods and discuss their limitations, which stem from the algorithmic trading domain. We believe that these findings should serve as an alert to the finance community about the threats in this area and promote further research on the risks associated with using automated learning models in the trading domain.},
	urldate = {2021-11-14},
	journal = {arXiv:2010.09246 [cs, q-fin]},
	author = {Nehemya, Elior and Mathov, Yael and Shabtai, Asaf and Elovici, Yuval},
	month = sep,
	year = {2021},
	note = {arXiv: 2010.09246
version: 2},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Quantitative Finance - Trading and Market Microstructure},
	file = {arXiv Fulltext PDF:/home/roland/Zotero/storage/VJY6I2QR/Nehemya et al. - 2021 - Taking Over the Stock Market Adversarial Perturba.pdf:application/pdf;arXiv.org Snapshot:/home/roland/Zotero/storage/UGRWYBRU/2010.html:text/html},
}

@article{petukhina_rise_2021,
	title = {Rise of the {Machines}? {Intraday} {High}-{Frequency} {Trading} {Patterns} of {Cryptocurrencies}},
	volume = {27},
	issn = {1351-847X, 1466-4364},
	shorttitle = {Rise of the {Machines}?},
	url = {http://arxiv.org/abs/2009.04200},
	doi = {10.1080/1351847X.2020.1789684},
	abstract = {This research analyses high-frequency data of the cryptocurrency market in regards to intraday trading patterns related to algorithmic trading and its impact on the European cryptocurrency market. We study trading quantitatives such as returns, traded volumes, volatility periodicity, and provide summary statistics of return correlations to CRIX (CRyptocurrency IndeX), as well as respective overall high-frequency based market statistics with respect to temporal aspects. Our results provide mandatory insight into a market, where the grand scale employment of automated trading algorithms and the extremely rapid execution of trades might seem to be a standard based on media reports. Our findings on intraday momentum of trading patterns lead to a new quantitative view on approaching the predictability of economic value in this new digital market.},
	number = {1-2},
	urldate = {2021-11-14},
	journal = {The European Journal of Finance},
	author = {Petukhina, Alla A. and Reule, Raphael C. G. and Härdle, Wolfgang Karl},
	month = jan,
	year = {2021},
	note = {arXiv: 2009.04200
version: 1},
	keywords = {Quantitative Finance - Trading and Market Microstructure},
	pages = {8--30},
	file = {arXiv Fulltext PDF:/home/roland/Zotero/storage/JDU6LZCF/Petukhina et al. - 2021 - Rise of the Machines Intraday High-Frequency Trad.pdf:application/pdf;arXiv.org Snapshot:/home/roland/Zotero/storage/4SLF4K9H/2009.html:text/html},
}

@article{ponomarev_using_2019,
	title = {Using {Reinforcement} {Learning} in the {Algorithmic} {Trading} {Problem}},
	volume = {64},
	issn = {1064-2269, 1555-6557},
	url = {http://arxiv.org/abs/2002.11523},
	doi = {10.1134/S1064226919120131},
	abstract = {The development of reinforced learning methods has extended application to many areas including algorithmic trading. In this paper trading on the stock exchange is interpreted into a game with a Markov property consisting of states, actions, and rewards. A system for trading the fixed volume of a financial instrument is proposed and experimentally tested; this is based on the asynchronous advantage actor-critic method with the use of several neural network architectures. The application of recurrent layers in this approach is investigated. The experiments were performed on real anonymized data. The best architecture demonstrated a trading strategy for the RTS Index futures (MOEX:RTSI) with a profitability of 66\% per annum accounting for commission. The project source code is available via the following link: http://github.com/evgps/a3c\_trading.},
	number = {12},
	urldate = {2021-11-14},
	journal = {Journal of Communications Technology and Electronics},
	author = {Ponomarev, Evgeny and Oseledets, Ivan and Cichocki, Andrzej},
	month = dec,
	year = {2019},
	note = {arXiv: 2002.11523
version: 1},
	keywords = {Quantitative Finance - Trading and Market Microstructure, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Neural and Evolutionary Computing, \_tablet\_modified},
	pages = {1450--1457},
	file = {arXiv.org Snapshot:/home/roland/Zotero/storage/FKRDC9N7/2002.html:text/html;Ponomarev et al_2019_Using Reinforcement Learning in the Algorithmic Trading Problem.pdf:/home/roland/Zotero/storage/QTIIP8G3/Ponomarev et al_2019_Using Reinforcement Learning in the Algorithmic Trading Problem.pdf:application/pdf},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Using} {Reinforcement} {Learning} in the {Algorithmic} {Trading} {Problem}},
	url = {https://paperswithcode.com/paper/using-reinforcement-learning-in-the},
	abstract = {Implemented in one code library.},
	language = {en},
	urldate = {2021-11-14},
	file = {Snapshot:/home/roland/Zotero/storage/DW9SES22/using-reinforcement-learning-in-the.html:text/html},
}

@book{shen_stochastic_2020,
	title = {A {Stochastic} {LQR} {Model} for {Child} {Order} {Placement} in {Algorithmic} {Trading}},
	abstract = {Modern Algorithmic Trading ("Algo") allows institutional investors and traders to liquidate or establish big security positions in a fully automated or low-touch manner. Most existing academic or industrial Algos focus on how to "slice" a big parent order into smaller child orders over a given time horizon. Few models rigorously tackle the actual placement of these child orders. Instead, placement is mostly done with a combination of empirical signals and heuristic decision processes. A self-contained, realistic, and fully functional Child Order Placement (COP) model may never exist due to all the inherent complexities, e.g., fragmentation due to multiple venues, dynamics of limit order books, lit vs. dark liquidity, different trading sessions and rules. In this paper, we propose a reductionism COP model that focuses exclusively on the interplay between placing passive limit orders and sniping using aggressive takeout orders. The dynamic programming model assumes the form of a stochastic linear-quadratic regulator (LQR) and allows closed-form solutions under the backward Bellman equations. Explored in detail are model assumptions and general settings, the choice of state and control variables and the cost functions, and the derivation of the closed-form solutions.},
	author = {Shen, Jackie},
	month = apr,
	year = {2020},
	keywords = {\_tablet\_modified},
	file = {Shen_2020_A Stochastic LQR Model for Child Order Placement in Algorithmic Trading.pdf:/home/roland/Zotero/storage/XVM7RRME/Shen_2020_A Stochastic LQR Model for Child Order Placement in Algorithmic Trading.pdf:application/pdf},
}

@article{shen_pre-trade_2013,
	title = {A {Pre}-{Trade} {Algorithmic} {Trading} {Model} {Under} {Given} {Volume} {Measures} and {Generic} {Price} {Dynamics}},
	doi = {10.1093/amrx/abu007},
	abstract = {We make several improvements to the mean–variance framework for optimal pre-trade algorithmic execution, by working with volume
measures and generic price dynamics. Volume measures are the continuum analogies for discrete volume profiles commonly implemented in the execution industry. Execution then becomes an absolutely continuous measure over such a measure space, and its Radon–Nikodym derivative is commonly known as the participation of volume (PoV) function. The four impact cost components are all consistently built upon the PoV function. Some novel efforts are
made for these linear impact models by having market signals more properly expressed. For the opportunistic cost, we go beyond
the conventional Brownian-type motions. By working directly with the auto-covariances of the price dynamics, we remove the
Markovian restriction associated with Brownians and thus allow potential memory effects. In combination, the final execution
model becomes a constrained quadratic programming problem in infinite-dimensional Hilbert spaces. Important linear constraints
such as participation capping are all permissible. Uniqueness and existence of optimal solutions are established via the theory
of positive compact operators in Hilbert spaces. Several typical numerical examples explain both the behavior and versatility
of the model.},
	journal = {Applied Mathematics Research eXpress},
	author = {Shen, Jackie},
	month = sep,
	year = {2013},
	file = {Full Text PDF:/home/roland/Zotero/storage/PUBSC4FE/Shen - 2013 - A Pre-Trade Algorithmic Trading Model Under Given .pdf:application/pdf},
}

@book{shen_nine_2021,
	title = {Nine {Challenges} in {Modern} {Algorithmic} {Trading} and {Controls}},
	abstract = {This editorial article partially informs the algorithmic trading community about launching of the new journal "Algorithmic Trading and Controls" (ATC). ATC is an online open-access journal that publishes novel works on algorithmic trading and its control methodologies. In this inaugural article, we discuss nine major challenges that contemporary Algo trading faces. There is nothing superstitiously magical about the number "nine," but so is any other one. Several of these challenges are at the strategy level, including for example, trading of illiquid securities or optimal portfolio execution. Others are more at the level of risk management and controls, such as on how to develop automated controls, testing and simulations. The editorial views could be inevitably personal and biased, but have been explored with the most innocent intention of contributing to this important field in modern financial services and technologies.},
	author = {Shen, Jackie},
	month = jan,
	year = {2021},
	file = {Shen - 2021 - Nine Challenges in Modern Algorithmic Trading and .pdf:/home/roland/Zotero/storage/ZAL3N4LS/Shen - 2021 - Nine Challenges in Modern Algorithmic Trading and .pdf:application/pdf},
}

@article{tao_digital_2019,
	title = {Digital {Twins} and {Cyber}–{Physical} {Systems} toward {Smart} {Manufacturing} and {Industry} 4.0: {Correlation} and {Comparison}},
	volume = {5},
	issn = {2095-8099},
	shorttitle = {Digital {Twins} and {Cyber}–{Physical} {Systems} toward {Smart} {Manufacturing} and {Industry} 4.0},
	url = {https://www.sciencedirect.com/science/article/pii/S209580991830612X},
	doi = {10.1016/j.eng.2019.01.014},
	abstract = {State-of-the-art technologies such as the Internet of Things (IoT), cloud computing (CC), big data analytics (BDA), and artificial intelligence (AI) have greatly stimulated the development of smart manufacturing. An important prerequisite for smart manufacturing is cyber–physical integration, which is increasingly being embraced by manufacturers. As the preferred means of such integration, cyber–physical systems (CPS) and digital twins (DTs) have gained extensive attention from researchers and practitioners in industry. With feedback loops in which physical processes affect cyber parts and vice versa, CPS and DTs can endow manufacturing systems with greater efficiency, resilience, and intelligence. CPS and DTs share the same essential concepts of an intensive cyber–physical connection, real-time interaction, organization integration, and in-depth collaboration. However, CPS and DTs are not identical from many perspectives, including their origin, development, engineering practices, cyber–physical mapping, and core elements. In order to highlight the differences and correlation between them, this paper reviews and analyzes CPS and DTs from multiple perspectives.},
	language = {en},
	number = {4},
	urldate = {2021-11-14},
	journal = {Engineering},
	author = {Tao, Fei and Qi, Qinglin and Wang, Lihui and Nee, A. Y. C.},
	month = aug,
	year = {2019},
	keywords = {Correlation and comparison, Cyber–physical systems (CPS), Digital twin (DT), Smart manufacturing},
	pages = {653--661},
	file = {ScienceDirect Full Text PDF:/home/roland/Zotero/storage/CKFPUEQI/Tao et al. - 2019 - Digital Twins and Cyber–Physical Systems toward Sm.pdf:application/pdf},
}

@article{flammini_digital_2021,
	title = {Digital twins as run-time predictive models for the resilience of cyber-physical systems: a conceptual framework},
	volume = {379},
	shorttitle = {Digital twins as run-time predictive models for the resilience of cyber-physical systems},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0369},
	doi = {10.1098/rsta.2020.0369},
	abstract = {Digital twins (DT) are emerging as an extremely promising paradigm for run-time modelling and performability prediction of cyber-physical systems (CPS) in various domains. Although several different definitions and industrial applications of DT exist, ranging from purely visual three-dimensional models to predictive maintenance tools, in this paper, we focus on data-driven evaluation and prediction of critical dependability attributes such as safety. To that end, we introduce a conceptual framework based on autonomic systems to host DT run-time models based on a structured and systematic approach. We argue that the convergence between DT and self-adaptation is the key to building smarter, resilient and trustworthy CPS that can self-monitor, self-diagnose and—ultimately—self-heal. The conceptual framework eases dependability assessment, which is essential for the certification of autonomous CPS operating with artificial intelligence and machine learning in critical applications.

This article is part of the theme issue ‘Towards symbiotic autonomous systems’.},
	number = {2207},
	urldate = {2021-11-14},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Flammini, Francesco},
	month = oct,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {cyber-physical systems, digital twins, resilience, run-time models, self-healing, trustworthy autonomy},
	pages = {20200369},
	file = {Full Text PDF:/home/roland/Zotero/storage/MW3APTG2/Flammini - 2021 - Digital twins as run-time predictive models for th.pdf:application/pdf},
}

@article{tao_digital_2019-1,
	title = {Digital {Twins} and {Cyber}–{Physical} {Systems} toward {Smart} {Manufacturing} and {Industry} 4.0: {Correlation} and {Comparison}},
	volume = {5},
	issn = {2095-8099},
	shorttitle = {Digital {Twins} and {Cyber}–{Physical} {Systems} toward {Smart} {Manufacturing} and {Industry} 4.0},
	url = {https://www.sciencedirect.com/science/article/pii/S209580991830612X},
	doi = {10.1016/j.eng.2019.01.014},
	abstract = {State-of-the-art technologies such as the Internet of Things (IoT), cloud computing (CC), big data analytics (BDA), and artificial intelligence (AI) have greatly stimulated the development of smart manufacturing. An important prerequisite for smart manufacturing is cyber–physical integration, which is increasingly being embraced by manufacturers. As the preferred means of such integration, cyber–physical systems (CPS) and digital twins (DTs) have gained extensive attention from researchers and practitioners in industry. With feedback loops in which physical processes affect cyber parts and vice versa, CPS and DTs can endow manufacturing systems with greater efficiency, resilience, and intelligence. CPS and DTs share the same essential concepts of an intensive cyber–physical connection, real-time interaction, organization integration, and in-depth collaboration. However, CPS and DTs are not identical from many perspectives, including their origin, development, engineering practices, cyber–physical mapping, and core elements. In order to highlight the differences and correlation between them, this paper reviews and analyzes CPS and DTs from multiple perspectives.},
	language = {en},
	number = {4},
	urldate = {2021-11-11},
	journal = {Engineering},
	author = {Tao, Fei and Qi, Qinglin and Wang, Lihui and Nee, A. Y. C.},
	month = aug,
	year = {2019},
	keywords = {Correlation and comparison, Cyber–physical systems (CPS), Digital twin (DT), Smart manufacturing},
	pages = {653--661},
	file = {ScienceDirect Full Text PDF:/home/roland/Zotero/storage/NTTQRX6V/Tao et al. - 2019 - Digital Twins and Cyber–Physical Systems toward Sm.pdf:application/pdf},
}

@techreport{osler_support_2006,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Support for {Resistance}: {Technical} {Analysis} and {Intraday} {Exchange} {Rates}},
	shorttitle = {Support for {Resistance}},
	url = {https://papers.ssrn.com/abstract=888805},
	abstract = {"Support" and "resistance" levels - points at which an exchange rate trend may be interrupted and reversed - are widely used for short-term exchange rate forecasting. Nevertheless, the levels' ability to predict intraday trend interruptions has never been rigorously evaluated. This article undertakes such an analysis, using support and resistance levels provided to customers by six firms active in the foreign exchange market. The author offers strong evidence that the levels help to predict intraday trend interruptions. However, the levels' predictive power is found to vary across the exchange rates and firms examined.},
	language = {en},
	number = {ID 888805},
	urldate = {2021-07-02},
	institution = {Social Science Research Network},
	author = {Osler, Carol L.},
	month = mar,
	year = {2006},
	keywords = {bootstrap, exchange rates, high-frequency, resistance, support, tecnhical analysis},
	file = {Snapshot:/home/roland/Zotero/storage/8W5KKWWF/papers.html:text/html},
}

@book{podofillini_safety_2015,
	title = {Safety and {Reliability} of {Complex} {Engineered} {Systems}: {ESREL} 2015},
	isbn = {978-1-138-02879-1 978-1-315-64841-5},
	shorttitle = {Safety and {Reliability} of {Complex} {Engineered} {Systems}},
	url = {http://www.crcnetbase.com/doi/book/10.1201/b19094},
	abstract = {This paper addresses the integration of common cause failures (CCFs or CCF events) in Probabilistic Risk Assessment (PRA) for dynamic ﬁnite-state systems. CCF events are simultaneous failures of multiple identical components due to a common cause. While CCF modelling in static PRA relies mainly on fault tree analysis that has been extensively studied since the 80’s, no common methodology has arisen to address CCFs in dynamic PRA. The challenge is that many modelling techniques can be applied in dynamic PRA: Markov chains, Petri nets, BDMP (Boolean logic Driven Markov Processes), other speciﬁc ”smart component” approaches etc. and currently no generic CCF support is available for such a variety of techniques. The ﬁrst contribution of this paper concerns the extension of the CCF Basic Parametric Model (BPM) to the case of dynamic systems. The main interest of this extension is that BPM parameters estimates for static analyses can be directly used in dynamic analyses. This point is particularly important from an operational point of view because the feedback data analysis campaigns needed to estimate the reliability parameters of a real complex system are costly. The second contribution of this work is the demonstration of this CCF modelling approach with an implementation in the Figaro modelling language that was developed at EDF R\&D in the 90’s. Figaro is a general object oriented modelling language designed to engineer knowledge bases for static and dynamic PRA. The paper describes a procedure (that can be automated) able to augment any Figaro library in order to represent the CCF events in a dynamic context. Thanks to this procedure, both the computational and combinatorial aspects of CCF modelling remain transparent to the analyst. Finally, this methodology is applied to the popular BDMP formalism, and its beneﬁts are exempliﬁed by the comparison of a BDMP including CCF added manually and via the method described in the paper.},
	language = {en},
	urldate = {2020-09-18},
	publisher = {CRC Press},
	editor = {Podofillini, Luca and Sudret, Bruno and Stojadinovic, Bozidar and Zio, Enrico and Kröger, Wolfgang},
	month = sep,
	year = {2015},
	doi = {10.1201/b19094},
	file = {Podofillini et al. - 2015 - Safety and Reliability of Complex Engineered Syste.pdf:/home/roland/Zotero/storage/UN8ZGUHP/Podofillini et al. - 2015 - Safety and Reliability of Complex Engineered Syste.pdf:application/pdf},
}

@article{donat_dynamic_2010,
	series = {Bayesian {Networks} / {Design} and {Application} of {Neural} {Networks} and {Intelligent} {Learning} {Systems} ({KES} 2008 / {Bio}-inspired {Computing}: {Theories} and {Applications} ({BIC}-{TA} 2007)},
	title = {A dynamic {Bayesian} network to represent discrete duration models},
	volume = {73},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231209003713},
	doi = {10.1016/j.neucom.2009.09.012},
	abstract = {Originally devoted to specific applications such as biology, medicine and demography, duration models are now widely used in economy, finance or reliability. Recent works in various fields of application have shown the relevancy of using Bayesian networks to model complex systems, namely stochastic systems with an underlying distribution that does not fit to a well-known parametric form. In this paper, the description of a specific dynamic Bayesian network, referred to as Graphical Duration Model (GDM), is given. A GDM aims to represent a wide range of duration models. Its structure allows especially to fit multi-state systems featuring complex sojourn-time distributions and contextual dependencies. To that end, a duration variable is explicitly introduced to the state transition model which is classically represented by a Markov chain. A recursive algorithm efficiently perform inference in this model is derived along with its proof of correctness and space and time complexity studies. Finally, this approach is illustrated with an application in survival analysis in which the proposed model is compared with the commonly used Markov chain modelling.},
	language = {en},
	number = {4},
	urldate = {2020-09-18},
	journal = {Neurocomputing},
	author = {Donat, Roland and Leray, Philippe and Bouillaut, Laurent and Aknin, Patrice},
	month = jan,
	year = {2010},
	keywords = {Discrete duration models, Dynamic Bayesian networks, Graphical duration models, Inference, Survival analysis},
	pages = {570--577},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/8LWP924M/S0925231209003713.html:text/html},
}

@misc{rauzy_altarica_2014,
	address = {Ecole Centrale de Paris},
	title = {The {AltaRica} 3.0 {Project}, formalisms comparison of “{Expected} {Properties}”},
	author = {Rauzy, Antoine},
	year = {2014},
}

@inproceedings{batteux_altarica_2018,
	address = {Reims, France},
	title = {{ALTARICA} {WIZARD}: {AN} {INTEGRATED} {MODELING} {AND} {SIMULATION} {ENVIRONMENT} {FOR} {ALTARICA} 3.0},
	shorttitle = {{ALTARICA} {WIZARD}},
	url = {https://hal.archives-ouvertes.fr/hal-01945932},
	urldate = {2020-09-17},
	booktitle = {Congrés {Lambda} {Mu} 21 “ {Maîtrise} des risques et transformation numérique : opportunités et menaces ”},
	author = {Batteux, Michel Batteux and Prosvirnova, Tatiana and Rauzy, Antoine},
	month = oct,
	year = {2018},
	file = {HAL PDF Full Text:/home/roland/Zotero/storage/7YBS6F3Y/Batteux et al. - 2018 - ALTARICA WIZARD AN INTEGRATED MODELING AND SIMULA.pdf:application/pdf},
}

@misc{gautier_apport_nodate,
	address = {Dassault Aviation},
	title = {Apport des {Méthodes} formelles pour la certification du {Falcon} {7X}},
	author = {Gautier},
}

@phdthesis{prosvirnova_altarica_2014,
	type = {Theses},
	title = {{AltaRica} 3.0: a {Model}-{Based} approach for {Safety} {Analyses}},
	shorttitle = {{AltaRica} 3.0},
	url = {https://pastel.archives-ouvertes.fr/tel-01119730},
	urldate = {2020-09-17},
	school = {Ecole Polytechnique},
	author = {Prosvirnova, Tatiana},
	month = nov,
	year = {2014},
	keywords = {Safety, Reliability, AltaRica, Analyse du risque, approche orientée modèles, Fiabilité, Model-Based approach, Sûreté de Fonctionnement},
	file = {HAL PDF Full Text:/home/roland/Zotero/storage/5GUGTYJQ/Prosvirnova - 2014 - AltaRica 3.0 a Model-Based approach for Safety An.pdf:application/pdf},
}

@article{cha_fundamentals_nodate,
	title = {Fundamentals of {Modeling} and {Analyzing} {Engineering} {Systems}},
	language = {en},
	author = {Cha, Philip D and Rosenberg, James J and Dym, Clive L and Thompson, Charles},
	pages = {4},
	file = {Cha et al. - Fundamentals of Modeling and Analyzing Engineering.pdf:/home/roland/Zotero/storage/I2MVADCP/Cha et al. - Fundamentals of Modeling and Analyzing Engineering.pdf:application/pdf},
}

@article{berrebi_131_2012,
	title = {1.3.1 {How} to use systems architecture to specify the operational perimeter of an innovative product line},
	volume = {22},
	copyright = {© 2012 The Authors},
	issn = {2334-5837},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2334-5837.2012.tb01323.x},
	doi = {10.1002/j.2334-5837.2012.tb01323.x},
	abstract = {In industrial companies, it can be observed that research and development departments encounter difficulties during the innovation phase. Indeed, searchers have a tendency to optimize technologies in an unorganized way. As a result, they often end up with products that do not fit with the industrial needs and are stored on shelves. The emergence of new products in an industrial perimeter should be the result of a careful study of the context, environment constrains, and stakeholder's needs which have to be translated into requirements building a frame for innovation. This article's aim is to propose a suitable methodology (system architecture) to manage and supervise innovation in order to create or improve a product. We will apply this methodology to a wireless sensor network system which is an innovative product research field launch by the rising aeronautic need to lighten aircrafts. As this system's final purpose is to be installed on aircrafts, we will try to build a family of modular systems with common basic module and specific compatible modules.},
	language = {en},
	number = {1},
	urldate = {2020-09-17},
	journal = {INCOSE International Symposium},
	author = {Berrebi, Johanna and Krob, Daniel},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2334-5837.2012.tb01323.x},
	keywords = {reliability, safety, aircraft, commonality, complex system, module, need, product family, requirement, security, sensor, SysML operational and functional analysis, wireless network},
	pages = {84--99},
	file = {Snapshot:/home/roland/Zotero/storage/Y7TNS227/j.2334-5837.2012.tb01323.html:text/html},
}

@book{krob_cesam_2017,
	title = {{CESAM}: {CESAMES} {Systems} {Architecting} {Method} - {A} {Pocket} {Guide}},
	shorttitle = {{CESAM}},
	url = {https://hal.archives-ouvertes.fr/hal-02561111},
	urldate = {2020-09-17},
	author = {Krob, Daniel},
	year = {2017},
	file = {HAL PDF Full Text:/home/roland/Zotero/storage/K4WXH2D6/Krob - 2017 - CESAM CESAMES Systems Architecting Method - A Poc.pdf:application/pdf},
}

@phdthesis{legendre_ingenierie_2017,
	type = {phdthesis},
	title = {Ingénierie système et {Sûreté} de fonctionnement : {Méthodologie} de synchronisation des modèles d'architecture et d'analyse de risques},
	shorttitle = {Ingénierie système et {Sûreté} de fonctionnement},
	url = {https://tel.archives-ouvertes.fr/tel-01730329},
	abstract = {L'organisation classique en silos disciplinaires des industries atteint ses limites pour maîtriser la complexité. Les problèmes sont découverts trop tard et le manque de communication entre les experts empêche l'émergence précoce de solutions. C'est pourquoi, il est urgent de fournir de nouvelles approches collaboratives et des moyens d' interactions entre les disciplines d'ingénierie, au début et tout au long du cycle de développement. Dans ce contexte, nous avons étudié l'approche synchronisation de modèles entre deux domaines d'ingénierie : la conception d'architecture de systèmes et la sûreté de fonctionnement. Elle a pour but de construire et maintenir la cohérence entre les modèles.Ces travaux proposent, étudient et analysent une démarche collaborative de synchronisation de modèles. Ils tiennent compte des contextes d’études, des processus, des méthodes appliqués et des points de vue produits par les ingénieurs. Les contributions répondent à des problématiques au niveau des pratiques, des concepts, de la mise en œuvre, des applications et l’implémentation de la synchronisation de modèles.},
	language = {fr},
	urldate = {2020-09-17},
	school = {Université Paris-Saclay},
	author = {Legendre, Anthony},
	month = dec,
	year = {2017},
	file = {Full Text PDF:/home/roland/Zotero/storage/FM4UE9AP/Legendre - 2017 - Ingénierie système et Sûreté de fonctionnement  M.pdf:application/pdf;Snapshot:/home/roland/Zotero/storage/4R5UYAVW/tel-01730329.html:text/html},
}

@inproceedings{yanar_system_1998-1,
	title = {System structuring for risk analysis using object oriented methodology},
	volume = {1},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Probabilistic} {Safety} {Assessment} and {Management} ({PSAM} {IV}), {New} {York}},
	author = {Yanar, DK},
	year = {1998},
	pages = {227--32},
}

@book{tuffery_data_2012,
	title = {Data {Mining} et statistique décisionnelle: {L}'intelligence des données},
	isbn = {978-2-7108-1017-9},
	shorttitle = {Data {Mining} et statistique décisionnelle},
	abstract = {Le data mining et la statistique sont de plus en plus  répandus dans les entreprises et les organisations soucieuses d’extraire  l’information pertinente de leurs bases de données, qu’elles peuvent  utiliser pour expliquer et prévoir les phénomènes qui les concernent  (risques, consommation, fidélisation...).Cette quatrième édition, actualisée et augmentée de 120 pages, fait le point sur le data mining, ses fondements théoriques, ses méthodes, ses outils et ses applications, qui vont du scoring jusqu’au web mining et au text mining.  Nombre de ses outils appartiennent à l’analyse des données et la  statistique \&quot;classique\&quot; (analyse factorielle, classification  automatique, analyse discriminante, régression logistique, modèles  linéaires généralisés, régression pénalisées...) mais certains sont plus  spécifiques au data mining, comme les arbres de décision, les  réseaux de neurones, les SVM, l’agrégation de modèles et la détection  des règles d’associations. Ces outils sont disponibles dans des  logiciels de plus en plus puissants et conviviaux, aptes à exécuter de  nombreux algorithmes sur de grands volumes de données. Un chapitre de  l’ouvrage aide le lecteur à se diriger dans cette offre logicielle et  détaille les fonctionnalités des trois principaux logiciels : R, SAS,  IBM et SPSS. Ces logiciels sont aussi utilisés pour illustrer par des  exemples de nombreuses explications théoriques : une partie de 50 pages  est consacrée à une étude de cas complète de credit scoring, qui va de l’exploration des données jusqu’à l’élaboration de la grille de score.Les  aspects méthodologiques vont de la conduite des projets jusqu’aux  facteurs de réussite et aux pièges à éviter, en passant par l’évaluation  et la comparaison des modèles, leur intégration dans les processus  opérationnels, sans oublier les contraintes juridiques dès que l’on  traite des données à caractère personnel.         Table des matières :1. Panorama du data  mining. 2. Le déroulement d’une étude de data mining. 3. L’exploration  et la préparation des données. 4. L’utilisation des données commerciales  et géodémographiques. 5. Les logiciels de statistique et de data  mining. 6. Panorama des méthodes de data mining. 7. L’analyse  factorielle. 8. Les réseaux de neurones. 9. Les techniques de  classification automatique. 10. La recherche des règles d’associations.  11. Les techniques de classement et de prédiction. 12. L’analyse  discriminante linéraire et ses généralisations. 13. Le modèle linéaire  et ses généralisations. 14. Le modèle logistique et ses généralisations.  15. Les autres modèles prédictifs. 16. L’agrégation de modèles. 17. Une  application du data mining : le scoring. 18. Les facteurs de succès  d’un projet de data mining. 19. Le text mining. 20. Le web mining.  Annexes. Bibliographie. Index.},
	language = {fr},
	publisher = {Editions TECHNIP},
	author = {Tufféry, Stéphane},
	month = aug,
	year = {2012},
	note = {Google-Books-ID: Cs2fCgAAQBAJ},
}

@article{miller_modelling_2017,
	title = {Modelling weather effects for impact analysis of residential time-of-use electricity pricing},
	doi = {http://dx.doi.org/10.1016/j.enpol.2017.03.015},
	abstract = {Analyzing the impact of pricing policies such as time-of-use (TOU) is challenging in the presence of confounding factors such as weather. Motivated by a lack of consensus and model selection details in prior work, we present a methodology for modelling the eﬀect of weather on residential electricity demand. The best model is selected according to explanatory power, out-of-sample prediction accuracy, goodness of ﬁt and interpretability. We then evaluate the eﬀect of mandatory TOU pricing in a local distribution company in southwestern Ontario, Canada. We use a smart meter dataset of over 20,000 households which is particularly suited to our analysis: it contains data from the summer before and after the implementation of TOU pricing in November 2011, and all customers transitioned from tiered rates to TOU rates at the same time. We ﬁnd that during the summer rate season, TOU pricing results in electricity conservation across all price periods. The average demand change during on-peak and mid-peak periods is −2.6\% and −2.4\% respectively. Changes during oﬀ-peak periods are not statistically signiﬁcant. These TOU pricing eﬀects are less pronounced compared to previous studies, underscoring the need for clear, reproducible impact analyses which include full details about the model selection process.},
	language = {en},
	journal = {Energy Policy},
	author = {Miller, Reid},
	year = {2017},
	keywords = {energy, forecasting, weather},
	pages = {13},
}

@article{zhang_time_2016,
	title = {Time series forecasting for building energy consumption using weighted {Support} {Vector} {Regression} with differential evolution optimization technique},
	volume = {126},
	issn = {03787788},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378778816303899},
	doi = {10.1016/j.enbuild.2016.05.028},
	abstract = {Electricity load forecasting is crucial for effective operation and management of buildings. Support Vector Regression (SVR) have been successfully used in solving nonlinear regression and time series problems related to building energy consumption forecasting. As the performance of SVR heavily depends on the selection of its parameters, differential evolution (DE) algorithm is employed in this study to solve this problem. The forecasting model is developed using weighted SVR models with nu-SVR and epsilonSVR. The DE algorithm is again used to determine the weights corresponding to each model. A case of time series energy consumption data from an institutional building in Singapore is used to elucidate the performance of the proposed model. The proposed model can be used to forecast both, half-hourly and daily electricity consumption time series data for the same building. The results show that for halfhourly data, the model exhibits higher weight for nu-SVR, whereas for daily data, a higher weight for epsilon-SVR is observed. The mean absolute percentage error (MAPE) for daily energy consumption data is 5.843 and that for half-hourly energy consumption is 3.767 respectively. A detailed comparison with other evolutionary algorithms show that the proposed model yields higher accuracy for building energy consumption forecasting.},
	language = {en},
	urldate = {2020-01-25},
	journal = {Energy and Buildings},
	author = {Zhang, Fan and Deb, Chirag and Lee, Siew Eang and Yang, Junjing and Shah, Kwok Wei},
	month = aug,
	year = {2016},
	keywords = {energy, forecasting, SVM},
	pages = {94--103},
}

@inproceedings{zhang_forecasting_2018,
	address = {Orlando, FL},
	title = {Forecasting {Residential} {Energy} {Consumption}: {Single} {Household} {Perspective}},
	isbn = {978-1-5386-6805-4},
	shorttitle = {Forecasting {Residential} {Energy} {Consumption}},
	url = {https://ieeexplore.ieee.org/document/8614049/},
	doi = {10.1109/ICMLA.2018.00024},
	abstract = {With the development of smart electricity metering technologies, huge amount of consumption data can be retrieved on daily and hourly basis. Energy consumption forecasting facilitates electricity demand management and utilities load planning. Most of the prior researches are focused on commercial customers or residential building-level energy consumption, or use behavioral and occupancy sensor data to experiment on individual household’s electrical consumption. This paper investigates ﬁfteen anonymous individual household’s electricity consumption forecasting using support vector regression(SVR) modelling approach, applied to both daily and hourly data granularity. The electricity usage dataset was collected from ﬁfteen households by London Hydro, a local utility company, from 2014 to 2016. Exploratory data analysis (EDA) is adopted for data visualization and feature selection. Our analysis demonstrates that forecasting residential energy consumption by weather, calendar and Timeof-usage price is feasible and reliable with sufﬁcient accuracy for some individual residential uses in either daily or hourly prediction.},
	language = {en},
	urldate = {2020-01-25},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Zhang, Xiaoou Monica and Grolinger, Katarina and Capretz, Miriam A. M. and Seewald, Luke},
	month = dec,
	year = {2018},
	keywords = {energy, forecasting, SVM},
	pages = {110--117},
}

@article{melzi_dedicated_2017,
	title = {A {Dedicated} {Mixture} {Model} for {Clustering} {Smart} {Meter} {Data}: {Identification} and {Analysis} of {Electricity} {Consumption} {Behaviors}},
	volume = {10},
	issn = {1996-1073},
	shorttitle = {A {Dedicated} {Mixture} {Model} for {Clustering} {Smart} {Meter} {Data}},
	url = {http://www.mdpi.com/1996-1073/10/10/1446},
	doi = {10.3390/en10101446},
	abstract = {The large amount of data collected by smart meters is a valuable resource that can be used to better understand consumer behavior and optimize electricity consumption in cities. This paper presents an unsupervised classiﬁcation approach for extracting typical consumption patterns from data generated by smart electric meters. The proposed approach is based on a constrained Gaussian mixture model whose parameters vary according to the day type (weekday, Saturday or Sunday). The proposed methodology is applied to a real dataset of Irish households collected by smart meters over one year. For each cluster, the model provides three consumption proﬁles that depend on the day type. In the ﬁrst instance, the model is applied on the electricity consumption of users during one month to extract groups of consumers who exhibit similar consumption behaviors. The clustering results are then crossed with contextual variables available for the households to show the close links between electricity consumption and household socio-economic characteristics. At the second instance, the evolution of the consumer behavior from one month to another is assessed through variations of cluster sizes over time. The results show that the consumer behavior evolves over time depending on the contextual variables such as temperature ﬂuctuations and calendar events.},
	language = {en},
	number = {10},
	urldate = {2020-01-25},
	journal = {Energies},
	author = {Melzi, Fateh and Same, Allou and Zayani, Mohamed and Oukhellou, Latifa},
	month = sep,
	year = {2017},
	keywords = {clustering, electricity consumption},
	pages = {1446},
}

@article{jadhav_forecasting_nodate,
	title = {Forecasting {Energy} {Consumption} using {Machine} {Learning}.},
	abstract = {Energy consumption forecasting is a tricky task given the presence of complex linear as well as nonlinear patterns in the energy consumption timeseries. While the orthodox method of auto-regressive integrated moving average (ARIMA) performs well to diagnose the linear aspects within the timeseries it fails to account for its non-linear aspects. On the other hand, neural network architecture accounts for the non-linear aspects of the timeseries but often ignores the linear aspects. In this research, we employ and compare the performance of the statistical method of auto-regressive integrated moving average (ARIMA), the econometric method of vector autoregression (VAR) and the machine learning method of artiﬁcial neural networks (ANN RNN LSTM) in forecasting energy consumption. Ultimately we devise and test a hybrid model based on VAR and ANN to capture both linear as well as non-linear aspects of the energy timeseries. We observe that the ANN (RNN LSTM) model outperforms all other models in terms of accuracy when the accuracy is measured using mean absolute percentage error (MAPE).},
	language = {en},
	author = {Jadhav, Vinit and Ligay, Vladislav},
	keywords = {forecasting, ARIMA, neural networks, VAR},
	pages = {16},
}

@inproceedings{neapolitan_learning_2007,
	address = {San Jose, California, USA},
	series = {{KDD} '07},
	title = {Learning {Bayesian} networks},
	isbn = {978-1-59593-609-7},
	url = {https://doi.org/10.1145/1327942.1327961},
	doi = {10.1145/1327942.1327961},
	urldate = {2020-03-16},
	booktitle = {Proceedings of the 13th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Neapolitan, Richard E.},
	month = aug,
	year = {2007},
	pages = {1},
}

@book{nielsen_bayesian_2009,
	title = {Bayesian {Networks} and {Decision} {Graphs}},
	isbn = {978-0-387-68282-2},
	abstract = {Probabilistic graphical models and decision graphs are powerful modeling tools for reasoning and decision making under uncertainty. As modeling languages they allow a natural specification of problem domains with inherent uncertainty, and from a computational perspective they support efficient algorithms for automatic construction and query answering. This includes belief updating, finding the most probable explanation for the observed evidence, detecting conflicts in the evidence entered into the network, determining optimal strategies, analyzing for relevance, and performing sensitivity analysis. The book introduces probabilistic graphical models and decision graphs, including Bayesian networks and influence diagrams. The reader is introduced to the two types of frameworks through examples and exercises, which also instruct the reader on how to build these models.  The book is a new edition of Bayesian Networks and Decision Graphs by Finn V. Jensen. The new edition is structured into two parts. The first part focuses on probabilistic graphical models. Compared with the previous book, the new edition also includes a thorough description of recent extensions to the Bayesian network modeling language, advances in exact and approximate belief updating algorithms, and methods for learning both the structure and the parameters of a Bayesian network. The second part deals with decision graphs, and in addition to the frameworks described in the previous edition, it also introduces Markov decision processes and partially ordered decision problems. The authors also      provide a well-founded practical introduction to Bayesian networks, object-oriented Bayesian networks, decision trees, influence diagrams (and variants hereof), and Markov decision processes.   give practical advice on the construction of Bayesian networks, decision trees, and influence diagrams from domain knowledge.  give several examples and exercises exploiting computer systems for dealing with Bayesian networks and decision graphs.   present a thorough introduction to state-of-the-art solution and analysis algorithms.   The book is intended as a textbook, but it can also be used for self-study and as a reference book.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Nielsen, Thomas Dyhre and JENSEN, FINN VERNER},
	month = mar,
	year = {2009},
	note = {Google-Books-ID: 37CAgCykQaAC},
	keywords = {Computers / Intelligence (AI) \& Semantics, Computers / Mathematical \& Statistical Software, Mathematics / Discrete Mathematics, Mathematics / Probability \& Statistics / General, Science / Life Sciences / Botany},
}

@incollection{dechter_bucket_1998,
	address = {Dordrecht},
	series = {{NATO} {ASI} {Series}},
	title = {Bucket {Elimination}: {A} {Unifying} {Framework} for {Probabilistic} {Inference}},
	isbn = {978-94-011-5014-9},
	shorttitle = {Bucket {Elimination}},
	url = {https://doi.org/10.1007/978-94-011-5014-9_4},
	abstract = {Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem’s structure.},
	language = {en},
	urldate = {2020-03-16},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Dechter, R.},
	editor = {Jordan, Michael I.},
	year = {1998},
	doi = {10.1007/978-94-011-5014-9_4},
	keywords = {Conditional Probability Table, Elimination Algorithm, Influence Diagram, Probabilistic Inference, Utility Component},
	pages = {75--104},
}

@incollection{pradhan_knowledge_1994-1,
	address = {San Francisco (CA)},
	title = {Knowledge {Engineering} for {Large} {Belief} {Networks}},
	isbn = {978-1-55860-332-5},
	url = {http://www.sciencedirect.com/science/article/pii/B9781558603325500663},
	abstract = {We present several techniques for knowledge engineering of large belief networks (BNs) based on the our experiences with a network derived from a large medical knowledge base. The noisy-MAX, a generalization of the noisy-OR gate, is used to model causal independence in a BN with multivalued variables. We describe the use of leak probabilities to enforce the closed-world assumption in our model. We present Netview, a visualization tool based on causal independence and the use of leak probabilities. The Netview software allows knowledge engineers to dynamically view subnetworks for knowledge engineering, and it provides version control for editing a BN. Netview generates sub-networks in which leak probabilities are dynamically updated to reflect the missing portions of the network.},
	language = {en},
	urldate = {2020-03-16},
	booktitle = {Uncertainty {Proceedings} 1994},
	publisher = {Morgan Kaufmann},
	author = {Pradhan, Malcolm and Provan, Gregory and Middleton, Blackford and Henrion, Max},
	editor = {de Mantaras, Ramon Lopez and Poole, David},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-1-55860-332-5.50066-3},
	pages = {484--490},
}

@misc{noauthor_transparency_2019,
	title = {Transparency in {Algorithmic} {Decision} {Making}},
	year = {2019},
	keywords = {ethic, explainable, transparency},
}

@article{giurgiu_explainable_2019,
	title = {Explainable {Failure} {Predictions} with {RNN} {Classifiers} based on {Time} {Series} {Data}},
	url = {http://arxiv.org/abs/1901.08554},
	abstract = {Given key performance indicators collected with ﬁne granularity as time series, our aim is to predict and explain failures in storage environments. Although explainable predictive modeling based on spiky telemetry data is key in many domains, current approaches cannot tackle this problem. Deep learning methods suitable for sequence modeling and learning temporal dependencies, such as RNNs, are effective, but opaque from an explainability perspective. Our approach ﬁrst extracts the anomalous spikes from time series as events and then builds an RNN classiﬁer with attention mechanisms to embed the irregularity and frequency of these events. A preliminary evaluation on real world storage environments shows that our approach can predict failures within a 3-day prediction window with comparable accuracy as traditional RNN-based classiﬁers. At the same time it can explain the predictions by returning the key anomalous events which led to those failure predictions.},
	language = {en},
	urldate = {2019-10-01},
	journal = {arXiv:1901.08554 [cs, stat]},
	author = {Giurgiu, Ioana and Schumann, Anika},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08554},
	keywords = {neural networks, explainable, failure, IBM, time series},
}

@article{huang_probability_2008,
	title = {Probability based vehicle fault diagnosis: {Bayesian} network method},
	volume = {19},
	issn = {1572-8145},
	shorttitle = {Probability based vehicle fault diagnosis},
	url = {https://doi.org/10.1007/s10845-008-0083-7},
	doi = {10.1007/s10845-008-0083-7},
	abstract = {Fault diagnostics are increasingly important for ensuring vehicle safety and reliability. One of the issues in vehicle fault diagnosis is the difficulty of successful interpretation of failure symptoms to correctly diagnose the real root cause. This paper presents an innovative Bayesian Network based method for guiding off-line vehicle fault diagnosis. By using a vehicle infotainment system as a case study, a number of Bayesian diagnostic models have been established for fault cases with single and multiple symptoms. Particular considerations are given to the design of the Bayesian model structure, determination of prior probabilities of root causes, and diagnostic procedure. In order to unburden the computation, an object oriented model structure has been adopted to prevent the model from overly large. It is shown that the proposed method is capable of guiding vehicle diagnostics in a probabilistic manner. Furthermore, the method features a multiple-symptoms-orientated troubleshooting strategy, and is capable of diagnosing multiple symptoms optimally and simultaneously.},
	language = {en},
	number = {3},
	urldate = {2020-03-16},
	journal = {Journal of Intelligent Manufacturing},
	author = {Huang, Yingping and McMurran, Ross and Dhadyalla, Gunwant and Peter Jones, R.},
	month = jun,
	year = {2008},
	pages = {301--311},
}

@article{krishnamurthi_expert_1992,
	title = {An expert system framework for machine fault diagnosis},
	volume = {22},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/036083529290034H},
	doi = {10.1016/0360-8352(92)90034-H},
	abstract = {This research focuses on two major issues related to the design, development, and implementation of machine fault diagnosis expert systems: (1) investigation of the actual cognitive process of human diagnostic experts, and (2) analysis of the current practices in the development of machine fault diagnosis expert systems. The investigation of the human diagnostic reasoning process has resulted in the abstraction and capturing of the human ability to learn, understand, and diagnose different machinery belonging to a particular class. The captured abstraction of human diagnostic expertise have been integrated with the expert system development expertise of knowledge engineers to provide a customized expert system shell for developing machine fault diagnosis expert systems. The designed machine fault diagnosis shell reduces the development time, effort and skill making use of generalized modules for knowledge acquisition, knowledge verification, application system generation, learning, explanation, and eliminates the burden of designing and developing each application diagnosis expert system separately. The developed shell has been validated by generating a prototype fault diagnosis expert system for a Cincinnati Milacron 786 robot.},
	language = {en},
	number = {1},
	urldate = {2020-03-16},
	journal = {Computers \& Industrial Engineering},
	author = {Krishnamurthi, Murali and Phillips, Don T.},
	month = jan,
	year = {1992},
	pages = {67--84},
}

@inproceedings{han_universal_2008,
	title = {A {Universal} {Fault} {Diagnostic} {Expert} {System} {Based} on {Bayesian} {Network}},
	volume = {1},
	doi = {10.1109/CSSE.2008.946},
	abstract = {Fault diagnosis is an area of great concern of any industry to reduce maintenance cost and increase profitability in the mean time. But most of the researches tend to rely on sensor data and equipment structure, which are expensive because each category of equipment differs from the others. Thus developing a universal system remains a key challenge to be solved. A universal expert system is developed in this paper making full use of expertspsila knowledge to diagnose the possible root causes and the corresponding probabilities for maintenance decision making support. Bayesian network was chosen as the inference engine of the system through raw data analysis. Improved causal relationship questionnaire and probability scale method were applied to construct the Bayesian network. The system has been applied to the production line of a chipset factory and the results show that the system can support decision making for fault diagnosis promptly and correctly.},
	booktitle = {2008 {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	author = {Han, Ting and Li, Bo and Xu, Limei},
	month = dec,
	year = {2008},
	note = {ISSN: null},
	keywords = {Bayesian network, Bayesian methods, belief networks, Baysian Network, Costs, data analysis, Data analysis, Decision making, Diagnostic expert systems, Engines, equipment structure, expert systems, fault diagnosis, Fault diagnosis, Fault Diagnosis, maintenance decision making support, Production facilities, Production systems, Profitability, sensor data, Universal Expert System, universal fault diagnostic expert system},
	pages = {260--263},
}

@article{chen-fu_chien_using_2002,
	title = {Using {Bayesian} network for fault location on distribution feeder},
	volume = {17},
	issn = {1937-4208},
	doi = {10.1109/TPWRD.2002.1022804},
	abstract = {The Bayesian network is a probabilistic graphical model in which a problem is structured as a set of variables (parameters) and probabilistic relationships among them. The Bayesian network has been effectively used to incorporate expert knowledge and historical data for revising the prior belief in the light of new evidence in many fields. However, little research has been done to apply a Bayesian network for fault location in power delivery system. We construct a Bayesian network on the basis of expert knowledge and historical data for fault diagnosis on a distribution feeder in Taiwan. The experimental results validate the practical viability of the proposed approach.},
	number = {3},
	journal = {IEEE Transactions on Power Delivery},
	author = {Chen-Fu Chien and Shi-Lin Chen and Yih-Shin Lin},
	month = jul,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Power Delivery},
	keywords = {Bayesian network, Bayesian methods, belief networks, Fault detection, Diagnostic expert systems, fault diagnosis, Fault diagnosis, distribution feeder, expert knowledge, expert system, Expert systems, fault location, Fault location, Graphical models, historical data, Load flow, Load flow analysis, power delivery system, power distribution faults, power system analysis computing, Power system modeling, probabilistic graphical model, probabilistic relationships, Taiwan, Taiwan Power Company},
	pages = {785--793},
}

@article{cai_bayesian_2017,
	title = {Bayesian {Networks} in {Fault} {Diagnosis}},
	volume = {13},
	issn = {1941-0050},
	doi = {10.1109/TII.2017.2695583},
	abstract = {Fault diagnosis is useful in helping technicians detect, isolate, and identify faults, and troubleshoot. Bayesian network (BN) is a probabilistic graphical model that effectively deals with various uncertainty problems. This model is increasingly utilized in fault diagnosis. This paper presents bibliographical review on use of BNs in fault diagnosis in the last decades with focus on engineering systems. This work also presents general procedure of fault diagnosis modeling with BNs; processes include BN structure modeling, BN parameter modeling, BN inference, fault identification, validation, and verification. The paper provides series of classification schemes for BNs for fault diagnosis, BNs combined with other techniques, and domain of fault diagnosis with BN. This study finally explores current gaps and challenges and several directions for future research.},
	number = {5},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Cai, Baoping and Huang, Lei and Xie, Min},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {belief networks, Bayes methods, fault diagnosis, Fault diagnosis, probabilistic graphical model, Analytical models, bayesian network, Bayesian networks (BNs), BN inference, BN parameter modeling, BN structure modeling, classification schemes, directed graphs, fault identification, fault validation, fault verification, Inference algorithms, Learning systems, Mathematical model, Probability},
	pages = {2227--2240},
}

@inproceedings{chen_railway_2019,
	address = {Nanjing, China},
	title = {Railway {Vehicle} {Door} {Fault} {Diagnosis} {Method} with {Bayesian} {Network}},
	isbn = {978-1-72811-593-1},
	url = {https://ieeexplore.ieee.org/document/8724211/},
	doi = {10.1109/ICCRE.2019.8724211},
	urldate = {2020-04-06},
	booktitle = {2019 4th {International} {Conference} on {Control} and {Robotics} {Engineering} ({ICCRE})},
	publisher = {IEEE},
	author = {Chen, Ruwen and Zhu, Songqing and Hao, Fei and Zhu, Bin and Zhao, Zhendong and Xu, Youxiong},
	month = apr,
	year = {2019},
	pages = {70--74},
}

@inproceedings{zhang_fault_2017,
	address = {Beijing},
	title = {Fault detection and diagnosis using {Bayesian}-network inference},
	isbn = {978-1-5386-1127-2},
	url = {http://ieeexplore.ieee.org/document/8216872/},
	doi = {10.1109/IECON.2017.8216872},
	urldate = {2020-04-06},
	booktitle = {{IECON} 2017 - 43rd {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Zhang, Yuping and You, Liyu and Jia, Chunhua},
	month = oct,
	year = {2017},
	pages = {5049--5053},
}

@inproceedings{shi_fault_2006,
	address = {Hangzhou, Zhejiang, China},
	title = {Fault {Diagnosis} of {AUV} {Based} on {Bayesian} {Networks}},
	url = {http://ieeexplore.ieee.org/document/4673726/},
	doi = {10.1109/IMSCCS.2006.224},
	urldate = {2020-04-06},
	booktitle = {First {International} {Multi}-{Symposiums} on {Computer} and {Computational} {Sciences} ({IMSCCS}'06)},
	publisher = {IEEE},
	author = {Shi, Changting and Zhang, Rubo and Yang, Ge},
	month = jun,
	year = {2006},
	pages = {339--343},
}

@article{mhenni_safesyse_2018,
	title = {{SafeSysE}: {A} {Safety} {Analysis} {Integration} in {Systems} {Engineering} {Approach}},
	volume = {12},
	issn = {2373-7816},
	shorttitle = {{SafeSysE}},
	doi = {10.1109/JSYST.2016.2547460},
	abstract = {The main objective of this paper is the integration of safety analysis in a SysML-based systems engineering approach in order to make it more effective and efficient. It helps to ensure the consistency between safety analyses and system design and then to avoid late errors and to reduce system development time. To achieve this purpose, we tackled the following axes: 1) formalizing a SysML-based design methodology that will be the support for safety analyses; 2) providing an extension of SysML to enable the integration of specific needs for safety concepts in the system model; and 3) performing an automated exploration of the SysML models to generate necessary information to elaborate safety artifacts such as failure mode and effects analysis (FMEA) and fault tree analysis (FTA). The proposed methodology named safety integration in systems engineering (SafeSysE) is applied to a real case study from the aeronautics domain: electromechanical actuator (EMA).},
	number = {1},
	journal = {IEEE Systems Journal},
	author = {Mhenni, Faïda and Nguyen, Nga and Choley, Jean-Yves},
	month = mar,
	year = {2018},
	keywords = {Safety, safety, fault trees, Fault trees, Analytical models, Mathematical model, failure analysis, failure mode and effects analysis, Failure mode and effects analysis (FMEA), fault tree analysis, fault tree analysis (FTA), FMEA, FTA, MBSA, MBSE, model checking, model-based safety analysis (MBSA), model-based systems engineering (MBSE), SafeSysE, safety analysis, safety integration in systems engineering approach, Standards, Static, SysML, SysML models, system design, system development, systems engineering},
	pages = {161--172},
}

@article{mili_transformation-based_2019,
	title = {Transformation-{Based} {Approach} to {Security} {Verification} for {Cyber}-{Physical} {Systems}},
	volume = {13},
	issn = {2373-7816},
	doi = {10.1109/JSYST.2019.2923818},
	abstract = {The increasing complexity of cyber-physical systems motivates new modeling approaches to ensure system security right from the design process. In this paper, we present a model-based approach to formally validate communicating systems against cyber-attacks. Security requirements are modeled by using the unified modeling language (UML) extended attack tree profile with temporal logic operators. Moreover, to identify attack propagation, another UML profile, i.e., the connectivity profile, has been integrated to model interactions between system components. In order to carry out a formal verification of the system, a transformation platform that automatically generates a new symbolic model verifier code from systems modeling language (SysML) models for both static and dynamic aspects has been developed. The modeling and validation process is illustrated via two case studies on connected cars: 2014 Jeep Cherokee attack and 2016 Tesla Model S attack.},
	number = {4},
	journal = {IEEE Systems Journal},
	author = {Mili, Saoussen and Nguyen, Nga and Chelouah, Rachid},
	month = dec,
	year = {2019},
	keywords = {Security, cyber-physical systems, model checking, SysML, systems engineering, 2014 Jeep Cherokee attack, 2016 Tesla Model S attack, Attack modeling, attack propagation, attack tree profile, Biological system modeling, code generation, Computational modeling, connectivity profile, cyber-attacks, cyber-physical system, cyber-security, design process, Embedded systems, formal verification, model-based approach, Petri nets, security of data, security requirements, security verification, symbolic model verifier code, system components, system security, Systems Modeling Language (SysML), Systems Modeling Language models, temporal logic, temporal logic operators, transformation platform, transformation-based approach, trees (mathematics), UML profile, Unified modeling language, Unified Modeling Language, validation process},
	pages = {3989--4000},
}

@book{siddesh_cyber-physical_2015,
	title = {Cyber-{Physical} {Systems}: {A} {Computational} {Perspective}},
	shorttitle = {Cyber-{Physical} {Systems}},
	publisher = {CRC Press},
	author = {Siddesh, Gaddadevara Matt and Deka, Ganesh Chandra and Srinivasa, Krishnarajanagar GopalaIyengar and Patnaik, Lalit Mohan},
	year = {2015},
}

@article{kabir_dynamic_2018,
	title = {Dynamic system safety analysis in {HiP}-{HOPS} with {Petri} {Nets} and {Bayesian} {Networks}},
	volume = {105},
	issn = {0925-7535},
	url = {http://www.sciencedirect.com/science/article/pii/S0925753517314911},
	doi = {10.1016/j.ssci.2018.02.001},
	abstract = {Dynamic systems exhibit time-dependent behaviours and complex functional dependencies amongst their components. Therefore, to capture the full system failure behaviour, it is not enough to simply determine the consequences of different combinations of failure events: it is also necessary to understand the order in which they fail. Pandora temporal fault trees (TFTs) increase the expressive power of fault trees and allow modelling of sequence-dependent failure behaviour of systems. However, like classical fault tree analysis, TFT analysis requires a lot of manual effort, which makes it time consuming and expensive. This in turn makes it less viable for use in modern, iterated system design processes, which requires a quicker turnaround and consistency across evolutions. In this paper, we propose for a model-based analysis of temporal fault trees via HiP-HOPS, which is a state-of-the-art model-based dependability analysis method supported by tools that largely automate analysis and optimisation of systems. The proposal extends HiP-HOPS with Pandora, Petri Nets and Bayesian Networks and results to dynamic dependability analysis that is more readily integrated into modern design processes. The effectiveness is demonstrated via application to an aircraft fuel distribution system.},
	language = {en},
	urldate = {2020-02-20},
	journal = {Safety Science},
	author = {Kabir, Sohag and Walker, Martin and Papadopoulos, Yiannis},
	month = jun,
	year = {2018},
	keywords = {Bayesian Networks, Dynamic fault trees, Fault tree analysis, HiP-HOPS, Model-based safety analysis, Petri Nets, Reliability analysis, Temporal fault trees},
	pages = {55--70},
}

@inproceedings{saxena_metrics_2008,
	address = {Denver, CO, USA},
	title = {Metrics for evaluating performance of prognostic techniques},
	isbn = {978-1-4244-1935-7},
	url = {http://ieeexplore.ieee.org/document/4711436/},
	doi = {10.1109/PHM.2008.4711436},
	abstract = {Prognostics is an emerging concept in condition based maintenance (CBM) of critical systems. Along with developing the fundamentals of being able to confidently predict Remaining Useful Life (RUL), the technology calls for fielded applications as it inches towards maturation. This requires a stringent performance evaluation so that the significance of the concept can be fully exploited. Currently, prognostics concepts lack standard definitions and suffer from ambiguous and inconsistent interpretations. This lack of standards is in part due to the varied end-user requirements for different applications, time scales, available information, domain dynamics, etc. to name a few issues. Instead, the research community has used a variety of metrics based largely on convenience with respect to their respective requirements. Very little attention has been focused on establishing a common ground to compare different efforts. This paper surveys the metrics that are already used for prognostics in a variety of domains including medicine, nuclear, automotive, aerospace, and electronics. It also considers other domains that involve prediction-related tasks, such as weather and finance. Differences and similarities between these domains and health maintenance have been analyzed to help understand what performance evaluation methods may or may not be borrowed. Further, these metrics have been categorized in several ways that may be useful in deciding upon a suitable subset for a specific application. Some important prognostic concepts have been defined using a notational framework that enables interpretation of different metrics coherently. Last, but not the least, a list of metrics has been suggested to assess critical aspects of RUL predictions before they are fielded in real applications.},
	language = {en},
	urldate = {2020-06-27},
	booktitle = {2008 {International} {Conference} on {Prognostics} and {Health} {Management}},
	publisher = {IEEE},
	author = {Saxena, Abhinav and Celaya, Jose and Balaban, Edward and Goebel, Kai and Saha, Bhaskar and Saha, Sankalita and Schwabacher, Mark},
	month = oct,
	year = {2008},
	pages = {1--17},
	file = {Saxena et al. - 2008 - Metrics for evaluating performance of prognostic t.pdf:/home/roland/Zotero/storage/Y9NCQ3FC/Saxena et al. - 2008 - Metrics for evaluating performance of prognostic t.pdf:application/pdf},
}

@inproceedings{bordeau_modelisation_2018,
	address = {Reims, France},
	title = {Modélisation et analyse prédictive des précurseurs de dangers},
	copyright = {All rights reserved},
	url = {https://hal.archives-ouvertes.fr/hal-02075368},
	urldate = {2020-05-30},
	booktitle = {Congrès {Lambda} {Mu} 21 “ {Maîtrise} des risques et transformation numérique : opportunités et menaces ”},
	author = {Bordeau, Vianney and Donat, Roland},
	month = oct,
	year = {2018},
	file = {HAL PDF Full Text:/home/roland/Zotero/storage/VA2LV9UC/Bordeau et Donat - 2018 - MODELING AND PREDICTIVE ANALYSIS OF HAZARD PRECURS.pdf:application/pdf},
}

@article{ruz_sentiment_2020,
	title = {Sentiment analysis of {Twitter} data during critical events through {Bayesian} networks classifiers},
	volume = {106},
	issn = {0167-739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X19303322},
	doi = {10.1016/j.future.2020.01.005},
	abstract = {Sentiment analysis through machine learning using Twitter data has become a popular topic in recent years. Here we address the problem of sentiment analysis during critical events such as natural disasters or social movements. We consider Bayesian network classifiers to perform sentiment analysis on two datasets in Spanish: the 2010 Chilean earthquake and the 2017 Catalan independence referendum. In order to automatically control the number of edges that are supported by the training examples in the Bayesian network classifier, we adopt a Bayes factor approach for this purpose, yielding more realistic networks. The results show the effectiveness of using the Bayes factor measure as well as its competitive predictive results when compared to support vector machines and random forests, given a sufficient number of training examples. Also, the resulting networks allow to identify the relations amongst words, offering interesting qualitative information to historically and socially comprehend the main features of the event dynamics.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Future Generation Computer Systems},
	author = {Ruz, Gonzalo A. and Henríquez, Pablo A. and Mascareño, Aldo},
	month = may,
	year = {2020},
	keywords = {Bayes factor, Bayesian network classifiers, Random forests, Sentiment analysis, Support vector machines, Twitter data},
	pages = {92--104},
	file = {Ruz et al. - 2020 - Sentiment analysis of Twitter data during critical.pdf:/home/roland/Zotero/storage/H8MUPYCN/Ruz et al. - 2020 - Sentiment analysis of Twitter data during critical.pdf:application/pdf},
}

@article{li_risk_2020,
	title = {Risk assessment of gas explosion in coal mines based on fuzzy {AHP} and bayesian network},
	volume = {135},
	issn = {0957-5820},
	url = {http://www.sciencedirect.com/science/article/pii/S095758201931763X},
	doi = {10.1016/j.psep.2020.01.003},
	abstract = {Gas explosion is one of the most deadly hazards in underground coal mining. Risk assessment has played an effective role in avoiding gas explosions and revising coal mine regulations. However, the traditional methods are deficient in quantitative evaluation, dynamic control and dealing with uncertainty. In this paper, a method of quantitative assessment the risk of gas explosion in underground coal mine using Bayesian network was proposed. A fuzzy analytic hierarchy process (FAHP) method based on subjective and objective information of experts was developed in the process of fuzzification. Through the Bayesian inference, the probability of occurrence of potential risk events and the probability distribution of risk factors can be calculated in real time according to on prior knowledge and evidence updating. Meanwhile, the most likely potential causes of accidents can be determined. A sensitivity analysis technique was utilized to investigate the contribution rate of each risk factor to a risk event, so as to determine the most critical risk factor. Taking Babao Coal Mine in China as the case, this study conducted a gas explosion risk assessment. The results show that the mothed of fuzzy AHP and Bayesian Network is feasible and applicable. It can be used as a decision-making tool to prevent coal mine gas explosions and provide decision makers with a technical guide for managing the coal mine gas explosion risk.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Process Safety and Environmental Protection},
	author = {Li, Min and Wang, Hetang and Wang, Deming and Shao, Zhenlu and He, Shan},
	month = mar,
	year = {2020},
	keywords = {Bayesian network, Risk assessment, Fuzzy number, Gas explosion},
	pages = {207--218},
	file = {Li et al. - 2020 - Risk assessment of gas explosion in coal mines bas.pdf:/home/roland/Zotero/storage/F83IUKNG/Li et al. - 2020 - Risk assessment of gas explosion in coal mines bas.pdf:application/pdf},
}

@article{kammouh_probabilistic_2020,
	title = {Probabilistic framework to evaluate the resilience of engineering systems using {Bayesian} and dynamic {Bayesian} networks},
	volume = {198},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832019303333},
	doi = {10.1016/j.ress.2020.106813},
	abstract = {Resilience indicators are a convenient tool to assess the resilience of engineering systems. They are often used in preliminary designs or in the assessment of complex systems. This paper introduces a novel approach to assess the time-dependent resilience of engineering systems using resilience indicators. A Bayesian network (BN) approach is employed to handle the relationships among the indicators. BN is known for its capability of handling causal dependencies between different variables in probabilistic terms. However, the use of BN is limited to static systems that are in a state of equilibrium. Being at equilibrium is often not the case because most engineering systems are dynamic in nature as their performance fluctuates with time, especially after disturbing events (e.g. natural disasters). Therefore, the temporal dimension is tackled in this work using the Dynamic Bayesian Network (DBN). DBN extends the classical BN by adding the time dimension. It permits the interaction among variables at different time steps. It can be used to track the evolution of a system's performance given an evidence recorded at a previous time step. This allows predicting the resilience state of a system given its initial condition. A mathematical probabilistic framework based on the DBN is developed to model the resilience of dynamic engineering systems. Two illustrative examples are presented in the paper to demonstrate the applicability of the introduced framework. One example evaluates the resilience of Brazil. The other one evaluates the resilience of a transportation system.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Reliability Engineering \& System Safety},
	author = {Kammouh, Omar and Gardoni, Paolo and Cimellaro, Gian Paolo},
	month = jun,
	year = {2020},
	keywords = {Bayesian network, Critical infrastructure, Dynamic Bayesian network, Recovery, Resilience analysis, Resilience indicators},
	pages = {106813},
	file = {Kammouh et al. - 2020 - Probabilistic framework to evaluate the resilience.pdf:/home/roland/Zotero/storage/RAJ6R9AF/Kammouh et al. - 2020 - Probabilistic framework to evaluate the resilience.pdf:application/pdf},
}

@article{aghaabbasi_predicting_2020,
	title = {Predicting the use frequency of ride-sourcing by off-campus university students through random forest and {Bayesian} network techniques},
	volume = {136},
	issn = {0965-8564},
	url = {http://www.sciencedirect.com/science/article/pii/S0965856420305681},
	doi = {10.1016/j.tra.2020.04.013},
	abstract = {This study used a survey technique to investigate factors that motivate the adoption and the usage frequency of ride-sourcing among students in a Malaysia public university. Two of the most broadly used machine learning techniques, Random Forest technique and Bayesian network analysis were applied in this study. Random Forest was employed to establish the relationship between ride-sourcing usage frequency and students' socio-demographic related factors, built environment considerations, and attitudes towards ride-sourcing specific factors. Random Forest identified 10 most important factors influencing university students’ use of ride-sourcing for different travel purposes, including study-related, shopping, and leisure travel. These important predictors were found to be indicators of the target variables (i.e., ride-sourcing usage frequency) in Bayesian network analysis. Bayesian network analysis identified the students' age (0.15), safety perception (0.32), and neighbourhood facilities in a walkable distance (0.21) as the most important predictors of the use of ride-sourcing among students to get to school, shopping, and leisure, respectively.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Aghaabbasi, Mahdi and Shekari, Zohreh Asadi and Shah, Muhammad Zaly and Olakunle, Oloruntobi and Armaghani, Danial Jahed and Moeinaddini, Mehdi},
	month = jun,
	year = {2020},
	keywords = {Bayesian Network, Off-campus university students, Random Forest, Ride-sourcing use frequency},
	pages = {262--281},
	file = {Aghaabbasi et al. - 2020 - Predicting the use frequency of ride-sourcing by o.pdf:/home/roland/Zotero/storage/TXZV8N7T/Aghaabbasi et al. - 2020 - Predicting the use frequency of ride-sourcing by o.pdf:application/pdf},
}

@article{barton_multi-criteria_2020,
	title = {Multi-criteria decision analysis in {Bayesian} networks - {Diagnosing} ecosystem service trade-offs in a hydropower regulated river},
	volume = {124},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S136481521831291X},
	doi = {10.1016/j.envsoft.2019.104604},
	abstract = {The paper demonstrates the use of Bayesian networks in multicriteria decision analysis (MCDA) of environmental design alternatives for environmental flows (eflows) and physical habitat remediation measures in the Mandalselva River in Norway. We demonstrate how MCDA using multi-attribute value functions can be implemented in a Bayesian network with decision and utility nodes. An object-oriented Bayesian network is used to integrate impacts computed in quantitative sub-models of hydropower revenues and Atlantic salmon smolt production and qualitative judgement models of mesohabitat fishability and riverscape aesthetics. We show how conditional probability tables are useful for modelling uncertainty in value scaling functions, and variance in criteria weights due to different stakeholder preferences. While the paper demonstrates the technical feasibility of MCDA in a BN, we also discuss the challenges of providing decision-support to a real-world habitat remediation process.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Environmental Modelling \& Software},
	author = {Barton, David N. and Sundt, Håkon and Bustos, Ana Adeva and Fjeldstad, Hans-Petter and Hedger, Richard and Forseth, Torbjørn and {Berit Köhler} and Aas, Øystein and Alfredsen, Knut and Madsen, Anders L.},
	month = feb,
	year = {2020},
	keywords = {Angling, Atlantic salmon, Bayesian network (BN), Disproportionate cost, Good ecological potential, Multi-attribute valuation theory (MAVT), River aesthetics, Valuation, Water framework directive (WFD)},
	pages = {104604},
	file = {Barton et al. - 2020 - Multi-criteria decision analysis in Bayesian netwo.pdf:/home/roland/Zotero/storage/VZSE5DKP/Barton et al. - 2020 - Multi-criteria decision analysis in Bayesian netwo.pdf:application/pdf},
}

@article{kovacic_learning_2020,
	title = {Learning parameters of {Bayesian} networks from datasets with systematically missing data: {A} meta–analytic approach},
	volume = {141},
	issn = {0957-4174},
	shorttitle = {Learning parameters of {Bayesian} networks from datasets with systematically missing data},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417419306748},
	doi = {10.1016/j.eswa.2019.112956},
	abstract = {Previous research suggested that using additional data sources could improve parameter learning in Bayesian networks. However, when additional datasets do not include all network variables, neither standard Bayesian network learning techniques nor standard missing data methods can be applied. In such situations, the use of a meta–analytic approach is proposed. The performance of one such meta–analytic approach was evaluated by simulating several study results on two real–life biomedical examples (one discrete and one Gaussian Bayesian network). Regardless of the network type, the meta–analytic approach showed higher mean log–likelihood values, less sensitive to the presence of heterogeneity, than a single dataset analysis. The difference between the two methods was most pronounced when sample sizes were small (N=100). For the meta–analytic approach, the increase in log–likelihood was in most cases positively related to the number of nodes estimated with additional data. However, as in the case of single dataset analysis, care is needed when estimating rare event probabilities from small datasets due to the problems with unidentifiability and increased bias.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Expert Systems with Applications},
	author = {Kovačić, Jelena},
	month = mar,
	year = {2020},
	keywords = {Bayesian networks, Meta-analysis, Missing data},
	pages = {112956},
	file = {Kovačić - 2020 - Learning parameters of Bayesian networks from data.pdf:/home/roland/Zotero/storage/GBQ6NBKV/Kovačić - 2020 - Learning parameters of Bayesian networks from data.pdf:application/pdf},
}

@article{amin_dynamic_2018,
	title = {Dynamic availability assessment of safety critical systems using a dynamic {Bayesian} network},
	volume = {178},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832017314771},
	doi = {10.1016/j.ress.2018.05.017},
	abstract = {Availability analysis of safety critical systems is an integral part of ensuring safety both in onshore and offshore process operations. However, the availability assessment of these systems is complex due to their multistate failure scenarios (dormant failure and failure on demand) and multistate functionality (operational failure). In the present study, a dynamic Bayesian network (DBN)-based dynamic availability assessment technique is proposed. This approach offers much flexibility in representing different failure scenarios and the interdependence of failure causes. Sensitivity and importance analyses have also been performed to identify the most influential failure causes. This helps to design better management strategies and provides more realistic reliance on safety critical systems. Applications of the proposed methodology are demonstrated on two safety critical systems: a fire alarm system and a hazard scenario in a steam generation system. This methodology will offer a pivotal step forward in dynamic safety analysis.},
	language = {en},
	urldate = {2020-05-05},
	journal = {Reliability Engineering \& System Safety},
	author = {Amin, Md. Tanjin and Khan, Faisal and Imtiaz, Syed},
	month = oct,
	year = {2018},
	keywords = {Sensitivity analysis, Dynamic Bayesian network, Dynamic availability analysis, Safety analysis, Safety critical systems},
	pages = {108--117},
	file = {Amin et al. - 2018 - Dynamic availability assessment of safety critical.pdf:/home/roland/Zotero/storage/5SGRTJYY/Amin et al. - 2018 - Dynamic availability assessment of safety critical.pdf:application/pdf},
}

@article{morimoto_development_2020,
	title = {Development of a software for kinship analysis considering linkage and mutation based on a {Bayesian} network},
	volume = {47},
	issn = {1872-4973, 1878-0326},
	url = {https://www.fsigenetics.com/article/S1872-4973(20)30052-1/abstract},
	doi = {10.1016/j.fsigen.2020.102279},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}In forensic DNA testing, the number of tested short tandem repeat loci has increased owing to new multiplex kits with additional loci. Although this advancement provides improved discrimination power, the effects of linkage and mutation must be considered during kinship analysis. However, no software currently includes both of these effects. In this study, we developed new freeware called \textit{KinBN} for kinship analysis based on a Bayesian network. The software is graphical-user-interface-based and calculates the likelihood ratios (LRs) at multiple loci considering the effects of linkage and mutation. In addition, the software can simulate the LR distribution according to the specified relationship. We confirmed the accuracy of \textit{KinBN} by comparing its LRs with those of other software and evaluated the effects of linkage and mutation on the LRs. Our results indicate that \textit{KinBN} is a useful tool for kinship analysis, particularly if expanded locus sets are used for DNA testing.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2020-05-05},
	journal = {Forensic Science International: Genetics},
	author = {Morimoto, Chie and Tsujii, Hideaki and Manabe, Sho and Fujimoto, Shuntaro and Hirai, Eriko and Hamano, Yuya and Tamaki, Keiji},
	month = jul,
	year = {2020},
	pmid = {32289730},
	note = {Publisher: Elsevier},
	file = {Morimoto et al. - 2020 - Development of a software for kinship analysis con.pdf:/home/roland/Zotero/storage/3QXTLLPY/Morimoto et al. - 2020 - Development of a software for kinship analysis con.pdf:application/pdf},
}

@article{zhang_data-driven_2018,
	series = {10th {IFAC} {Symposium} on {Advanced} {Control} of {Chemical} {Processes} {ADCHEM} 2018},
	title = {Data-{Driven} {Fault} {Prognosis} {Based} on {Incomplete} {Time} {Slice} {Dynamic} {Bayesian} {Network1}⁎{This} work was supported in part by the {National} {Natural} {Science} {Foundation} of {China} ({No}. 61374047, {No}. 61202473) and the {Fundamental} {Research} {Funds} for {Central} {Universities} ({JUSRP111A49}).},
	volume = {51},
	issn = {2405-8963},
	url = {http://www.sciencedirect.com/science/article/pii/S2405896318319876},
	doi = {10.1016/j.ifacol.2018.09.306},
	abstract = {Based on a dynamic Bayesian network with an incomplete time slice and a mixture of the Gaussian outputs, a data-driven fault prognosis method for model-unknown processes is proposed in this article. First, according to the requirement of fault prognosis, an incomplete time slice Bayesian network with unknown future observed node is constructed. Moreover, the future states are described by the current measurements and his historic data in the form of conditional probability. Second, according to the completed part of historical data, a parameter-learning algorithm is used to obtain network parameters and the weight coefficients of distribution components. After that, using such weight coefficients as input-output data, the subspace identification method is employed to build a forecasting model which can predict weight coefficients at next sampling time. To achieve fault prognosis, an inference algorithm is developed to predict hidden faults based on the distribution of the measurements directly. Furthermore, the remaining useful life of process is estimated via iterative one-step ahead prognosis. As an example, the proposed method is applied to a continuous stirred tank reactor system. The results demonstrate that the proposed method can efficiently predict and identify the fault, and estimate the remaining useful life of process, even though the measurements are partly missing.},
	language = {en},
	number = {18},
	urldate = {2020-05-05},
	journal = {IFAC-PapersOnLine},
	author = {Zhang, Zhengdao and Dong, Feilong and Xie, Linbo},
	month = jan,
	year = {2018},
	keywords = {remaining useful life, data missing, dynamic Bayesian network, fault prognosis, subspace identification},
	pages = {239--244},
	file = {Zhang et al. - 2018 - Data-Driven Fault Prognosis Based on Incomplete Ti.pdf:/home/roland/Zotero/storage/DYGLFRLZ/Zhang et al. - 2018 - Data-Driven Fault Prognosis Based on Incomplete Ti.pdf:application/pdf},
}

@article{shin_cyber_2017,
	title = {Cyber {Security} {Risk} {Evaluation} of a {Nuclear} {I}\&{C} {Using} {BN} and {ET}},
	volume = {49},
	issn = {1738-5733},
	url = {http://www.sciencedirect.com/science/article/pii/S1738573316302935},
	doi = {10.1016/j.net.2016.11.004},
	abstract = {Cyber security is an important issue in the field of nuclear engineering because nuclear facilities use digital equipment and digital systems that can lead to serious hazards in the event of an accident. Regulatory agencies worldwide have announced guidelines for cyber security related to nuclear issues, including U.S. NRC Regulatory Guide 5.71. It is important to evaluate cyber security risk in accordance with these regulatory guides. In this study, we propose a cyber security risk evaluation model for nuclear instrumentation and control systems using a Bayesian network and event trees. As it is difficult to perform penetration tests on the systems, the evaluation model can inform research on cyber threats to cyber security systems for nuclear facilities through the use of prior and posterior information and backpropagation calculations. Furthermore, we suggest a methodology for the application of analytical results from the Bayesian network model to an event tree model, which is a probabilistic safety assessment method. The proposed method will provide insight into safety and cyber security risks.},
	language = {en},
	number = {3},
	urldate = {2020-05-05},
	journal = {Nuclear Engineering and Technology},
	author = {Shin, Jinsoo and Son, Hanseong and Heo, Gyunyoung},
	month = apr,
	year = {2017},
	keywords = {Bayesian Network, Activity–Quality, Architecture Analysis, Cyber Security, Reactor Protection System, Research Reactor},
	pages = {517--524},
	file = {Shin et al. - 2017 - Cyber Security Risk Evaluation of a Nuclear I&C Us.pdf:/home/roland/Zotero/storage/Z3BXCJ6J/Shin et al. - 2017 - Cyber Security Risk Evaluation of a Nuclear I&C Us.pdf:application/pdf},
}

@inproceedings{nodelman_expectation_2005,
	address = {Edinburgh, Scottland, UK},
	title = {Expectation {Maximization} and {Complex} {Duration} {Distributions} for {Continuous} {Time} {Bayesian} {Networks}},
	booktitle = {Proceedings of the {Twenty}-first {Conference} on {Uncertainty} in {AI} ({UAI})},
	author = {Nodelman, U. and Shelton, C. R. and Koller, D.},
	month = jul,
	year = {2005},
	pages = {421--430},
}

@book{neapolitan_learning_2003,
	edition = {illustrated edition},
	title = {Learning {Bayesian} {Networks}},
	isbn = {0-13-012534-2},
	url = {http://www.worldcat.org/isbn/0130125342},
	publisher = {Prentice Hall},
	author = {Neapolitan, Richard E.},
	month = apr,
	year = {2003},
	note = {Published: Paperback},
	keywords = {graphical\_model, learning},
}

@techreport{nea_international_2012,
	title = {International {Common} {Cause} {Failure} {Data} {Exchange} ({ICDE})},
	url = {http://www.osti.gov/scitech/servlets/purl/6573115},
	abstract = {In this document, the general coding guidelines for the OECD ICDE Project (International Common Cause Failure Data Exchange) are presented with explanations and appendices for each analysed component. The guide reflects the present experience with the already completed data collection. The following persons have significantly contributed to the preparation of the main guidelines by their personal effort, for which they deserve an acknowledgement: Mr. Gunnar Johanson (ES Konsult). Dr. Wolfgang Werner (SAC). Mrs. Marina Concepcion Capote (ES Konsult / Emarcon). Dr. Albert Kreuser (GRS). In addition, those persons who have contributed to the component specific guidelines are mentioned in the respective appendices ICDE CG 01-06 of this document. Finally, the ICDE Working Group and the people with whom they liaise in all participating countries are recognized as important contributors to the success of these guidelines.},
	number = {NEA/CSNI/R(2011)12},
	institution = {Nuclear Safety Agency},
	author = {{NEA}},
	month = feb,
	year = {2012},
	file = {NEA2012ICDE.pdf:/home/roland/Zotero/storage/8SR9VQRC/NEA2012ICDE.pdf:application/pdf},
}

@book{naim_reseaux_2007,
	address = {Paris},
	edition = {Troisième},
	series = {Collection {Algorithmes}},
	title = {Réseaux bayésiens},
	publisher = {Eyrolles},
	author = {Naïm, P. and Wuillemin, P. H. and Leray, Ph and Pourret, O. and Becker, A.},
	year = {2007},
	keywords = {graphical\_model},
}

@book{nachlas_reliability_2005,
	title = {Reliability {Engineering} - {Probabilistic} {Models} and {Maintenance} {Methods}},
	publisher = {Taylor \& Francis},
	author = {Nachlas, Joel A.},
	year = {2005},
	keywords = {reliability, maintenance},
}

@inproceedings{murphy_loopy_1999,
	title = {Loopy {Belief} {Propagation} for {Approximate} {Inference}: {An} {Empirical} {Study}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5538},
	abstract = {Recently, researchers have demonstrated that "loopy belief propagation" — the use of Pearl's polytree algorithm in a Bayesian network with loops — can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of "Turbo Codes" — codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. In this paper we ask: is there something special about the...},
	booktitle = {Proceedings of the 15th on {Uncertainty} in {Artificial} {Intelligence}},
	author = {Murphy, K. P. and Weiss, Y. and Jordan, M. I.},
	year = {1999},
	keywords = {graphical\_model, inference},
	pages = {467--475},
}

@inproceedings{murphy_factored_2001,
	title = {The {Factored} {Frontier} {Algorithm} for {Approximate} {Inference} in {DBNs}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.3738},
	abstract = {The Factored Frontier (FF) algorithm is a simple approximate inference algorithm for Dynamic Bayesian Networks (DBNs). It is very similar to the fully factorized version of the Boyen-Koller (BK) algorithm, but instead of doing an exact update at every step followed by marginalisation (projection), it always works with factored distributions. Hence it can be applied to models for which the exact update step is intractable. We show that FF is equivalent to (one iteration of) loopy belief propagation (LBP) on the original DBN, and that BK is equivalent (to one iteration of) LBP on a DBN where we cluster some of the nodes. We then show empirically that by iterating more than once, LBP can improve on the accuracy of both FF and BK. We compare these algorithms on two real-world DBNs: the rst is a model of a water treatment plant, and the second is a coupled HMM, used to model freeway trac. 1},
	booktitle = {Proceedings of the 17th {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	author = {Murphy, K. P. and Weiss, Y.},
	year = {2001},
	keywords = {graphical\_model, inference},
	pages = {378--385},
}

@phdthesis{murphy_dynamic_2002,
	type = {{PhD} {Thesis}},
	title = {Dynamic {Bayesian} {Networks} : {Representation}, {Inference} and {Learning}},
	school = {University of California, Berkeley},
	author = {Murphy, Kevin P.},
	year = {2002},
	keywords = {graphical\_model, inference, estimation, dynamic, representation},
}

@techreport{mosleh_procedures_1988,
	title = {Procedures for treating common cause failures in safety and reliability studies: {Volume} 2, {Analytic} background and techniques: {Final} report},
	url = {http://www.osti.gov/scitech/servlets/purl/6573115},
	abstract = {This report presents a framework for the inclusion of the impact of common cause failures in risk and reliability evaluations. Common cause failures are defined as that subset of dependent failures for which causes are not explicitly included in the logic model as basic events. The emphasis here is on providing procedures for a practical, systematic approach that can be used to perform and clearly document the analysis. The framework and the methods discussed for performing the different stages of the analysis integrate insights obtained from engineering assessments of the system and the historical evidence from multiple failure events into a systematic, reproducible, and defensible analysis. This document, Volume 2, contains a series of appendices that provide additional background and methodological detail on several important topics discussed in Volume 1.},
	number = {EPRI-NP-5613-Vol.2},
	institution = {Electric Power Research Institute},
	author = {Mosleh, Ali and Fleming, K.N. and Parry, G.W. and Paula, H.M. and Worledge, D.H. and Rasmuson, D.M.},
	month = dec,
	year = {1988},
}

@article{mosallam_data-driven_2016,
	title = {Data-driven prognostic method based on {Bayesian} approaches for direct remaining useful life prediction},
	volume = {27},
	number = {5},
	journal = {Journal of Intelligent Manufacturing},
	author = {Mosallam, Ahmed and Medjaher, Kamal and Zerhouni, Noureddine},
	year = {2016},
	pages = {1037--1048},
}

@article{mosleh_common_1991,
	title = {Common cause failures: {An} analysis methodology and examples},
	volume = {34},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/095183209190104F},
	doi = {http://dx.doi.org/10.1016/0951-8320(91)90104-F},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Mosleh, Ali},
	year = {1991},
	pages = {249 -- 292},
}

@article{moore_cached_1998,
	title = {Cached {Sufficient} {Statistics} for {Efficient} {Machine} {Learning} with {Large} {Datasets}},
	volume = {8},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.7911},
	abstract = {This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.},
	journal = {Journal of Artificial Intelligence Research},
	author = {Moore, Andrew and Lee, Mary S.},
	year = {1998},
	keywords = {graphical\_model, learning, data},
	pages = {67--91},
}

@book{mclachlan_em_1997,
	address = {New York},
	title = {The {EM} {Algorithm} and {Extensions}},
	publisher = {Wiley},
	author = {McLachlan, G. and Krishnan, T.},
	year = {1997},
	keywords = {em},
}

@inproceedings{montani_automatically_2006,
	title = {Automatically translating dynamic fault trees into dynamic {Bayesian} networks by means of a software tool},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1625390},
	abstract = {This paper presents a software tool allowing the automatic analysis of a dynamic fault tree (DFT) exploiting its conversion to a dynamic Bayesian network (DBN). First, the architecture of the tool is described, together with the rules implemented in the tool, to convert dynamic gates in DBNs. Then, the tool is tested on a case of system: its DFT model and the corresponding DBN are provided and analyzed by means of the tool. The obtained unreliability results are compared with those returned by other tools, in order to verify their correctness.},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Montani, S. and Portinale, L. and Bobbio, A. and Codetta-Raiteri, D.},
	month = apr,
	year = {2006},
	note = {event-place: Vienna, Austria},
	keywords = {graphical\_model, fault\_trees, dynamic},
	pages = {804--809},
}

@article{matloff_introduction_2008,
	title = {Introduction to discrete-event simulation and the simpy language},
	volume = {2},
	journal = {Davis, CA. Dept of Computer Science. University of California at Davis. Retrieved on August},
	author = {Matloff, Norm},
	year = {2008},
	pages = {2009},
}

@article{marshall_multivariate_1967,
	title = {A {Multivariate} {Exponential} {Distribution}},
	volume = {62},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1967.10482885},
	doi = {10.1080/01621459.1967.10482885},
	abstract = {Abstract A number of multivariate exponential distributions are known, but they have not been obtained by methods that shed light on their applicability. This paper presents some meaningful derivations of a multivariate exponential distribution that serves to indicate conditions under which the distribution is appropriate. Two of these derivations are based on “shock models,” and one is based on the requirement that residual life is independent of age. It is significant that the derivations all lead to the same distribution. For this distribution, the moment generating function is obtained, comparison is made with the case of independence, the distribution of the minimum is discussed, and various other properties are investigated. A multivariate Weibull distribution is obtained through a change of variables.},
	number = {317},
	journal = {Journal of the American Statistical Association},
	author = {Marshall, Albert W. and Olkin, Ingram},
	year = {1967},
	pages = {30--44},
}

@article{marshall_multivariate_1967-1,
	title = {A {Multivariate} {Exponential} {Distribution}},
	volume = {62},
	copyright = {Copyright © 1967 American Statistical Association},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2282907},
	abstract = {A number of multivariate exponential distributions are known, but they have not been obtained by methods that shed light on their applicability. This paper presents some meaningful derivations of a multivariate exponential distribution that serves to indicate conditions under which the distribution is appropriate. Two of these derivations are based on "shock models," and one is based on the requirement that residual life is independent of age. It is significant that the derivations all lead to the same distribution. For this distribution, the moment generating function is obtained, comparison is made with the case of independence, the distribution of the minimum is discussed, and various other properties are investigated. A multivariate Weibull distribution is obtained through a change of variables.},
	language = {English},
	number = {317},
	journal = {Journal of the American Statistical Association},
	author = {Marshall, Albert W. and Olkin, Ingram},
	year = {1967},
	pages = {pp. 30--44},
	file = {Attachment:/home/roland/Zotero/storage/H98G9L55/Marshall_Olkin1967MultivariateExpo.pdf:application/pdf},
}

@article{marseguerra_condition-based_2002,
	title = {Condition-based maintenance optimization by means of genetic algorithms and {Monte} {Carlo} simulation},
	volume = {77},
	url = {http://dx.doi.org/10.1016/S0951-8320(02)00043-1},
	doi = {10.1016/S0951-8320(02)00043-1},
	abstract = {Efficient maintenance policies are of fundamental importance in system engineering because of their fallbacks into the safety and economics of plants operation. When the condition of a system, such as its degradation level, can be continuously monitored, a Condition[hyphen]Based Maintenance (CBM) policy can be implemented, according to which the decision of maintaining the system is taken dynamically on the basis of the observed condition of the system.In this paper, we consider a continuously monitored multi[hyphen]component system and use a Genetic Algorithm (GA) for determining the optimal degradation level beyond which preventive maintenance has to be performed. The problem is framed as a multi[hyphen]objective search aiming at simultaneously optimizing two typical objectives of interest, profit and availability. For a closer adherence to reality, the predictive model describing the evolution of the degrading system is based on the use of Monte Carlo (MC) simulation. More precisely, the flexibility offered by the simulation scheme is exploited to model the dynamics of a stress[hyphen]dependent degradation process in load[hyphen]sharing components and to account for limitations in the number of maintenance technicians available. The coupled (GA[plus ]MC) approach is rendered particularly efficient by the use of the 'drop[hyphen]by[hyphen]drop' technique, previously introduced by some of the authors, which allows to effectively drive the combinatorial search towards the most promising solutions.},
	number = {2},
	journal = {Reliability Engineering \& System Safety},
	author = {Marseguerra, M. and Zio, E. and Podofillini, L.},
	month = aug,
	year = {2002},
	keywords = {maintenance, mcmc, optimization, markov, process, condition-based, algorithm, genetic},
	pages = {151--165},
}

@article{mankamo_dependent_1992,
	title = {Dependent failure modeling in highly redundant structures—{Application} to \{{BWR}\} safety valves},
	volume = {35},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/095183209290082V},
	doi = {http://dx.doi.org/10.1016/0951-8320(92)90082-V},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Mankamo, Tuomas and Kosonen, Mikko},
	year = {1992},
	pages = {235 -- 244},
}

@book{lischner_exploring_2013,
	address = {Berkely, CA, USA},
	edition = {2nd},
	title = {Exploring {C}++ 11},
	isbn = {1-4302-6193-5 978-1-4302-6193-3},
	publisher = {Apress},
	author = {Lischner, Ray},
	year = {2013},
}

@article{lin_relevance-based_1999,
	title = {Relevance-based {Incremental} {Belief} {Updating} in {Bayesian} {Networks}},
	volume = {13},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001499000161},
	doi = {10.1142/S0218001499000161},
	number = {02},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Lin, Yan and Druzdzel, Marek J.},
	year = {1999},
	pages = {285--295},
}

@book{limnios_semi-markov_2001,
	series = {Statistics for {Industry} \& {Technology}},
	title = {Semi-{Markov} {Processes} and {Reliability}},
	publisher = {Springer},
	author = {Limnios, Nikolaos and Oprisan, Gheorghe},
	year = {2001},
	keywords = {reliability, process, semi-markov},
}

@article{li_bayesian_2005,
	title = {Bayesian {Inference} for {Origin}-{Destination} {Matrices} of {Transport} {Networks} {Using} the {EM} {Algorithm}},
	volume = {47},
	issn = {00401706},
	url = {http://www.jstor.org/stable/25471065},
	abstract = {Information on the origin-destination (OD) matrix of a transport network is a fundamental requirement in much transportation planning. A relatively inexpensive method for updating an OD matrix is to draw inference about the OD matrix based on a single observation of traffic flows on a specific set of network links, where the Bayesian approach is a natural choice for combining the prior knowledge about the OD matrix and the current observation of traffic flows. The existing approaches of Bayesian modeling of OD matrices include using normal approximations to Poisson distributions, which leads to the posterior being intractable even under some simple special cases, and using Markov chain Monte Carlo simulation, which incurs extreme demand of computational efforts. In this article, through the EM algorithm, Bayesian inference is reinvestigated for a transport network for estimating the population means of traffic flows, reconstructing traffic flows, and predicting future traffic flows. It is shown that the resultant estimates have very simple forms with minimal computational costs.},
	number = {4},
	journal = {Technometrics},
	author = {Li, Baibing},
	year = {2005},
	pages = {399--408},
	file = {Li2005OD_EM.pdf:/home/roland/Zotero/storage/TNKQE2U7/Li2005OD_EM.pdf:application/pdf},
}

@phdthesis{lefebvre_nouveaux_2003,
	type = {{PhD} {Thesis}},
	title = {Nouveaux développements et justifications de méthodes de calcul de mesures de performance en sûreté de fonctionnement},
	school = {Université de Marne-la-Vallée},
	author = {Lefebvre, Yannick},
	year = {2003},
	keywords = {exploration, heuristique, quantification, séquence},
	file = {Lefebvre2003SdF.pdf:/home/roland/Zotero/storage/TNGXNKDB/Lefebvre2003SdF.pdf:application/pdf},
}

@techreport{le_bihan_prevention_2005,
	title = {Prévention des ruptures de rails : {Nouvelle} politique de renouvellement des rais},
	url = {http://www.lemonde.fr/international/article/2009/10/05/l-europe-veut-massivement-investir-dans-la-recherche_1249619_3210.html#xtor=RSS-3208},
	institution = {RATP, Département EST, Unité Voie SLI/PMM},
	author = {Le Bihan, J. F. and Jouve, M.},
	month = mar,
	year = {2005},
	keywords = {railway},
}

@article{lauritzen_local_1988,
	title = {Local {Computations} with {Probabilities} on {Graphical} {Structures} and {Their} {Application} to {Expert} {Systems}},
	volume = {50},
	issn = {00359246},
	url = {http://dx.doi.org/10.2307/2345762},
	doi = {10.2307/2345762},
	abstract = {A causal network is used in a number of areas as a depiction of patterns of `influence' among sets of variables. In expert systems it is common to perform `inference' by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed `marrying' and `filling-in'. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
	number = {2},
	journal = {Journal of the Royal Statistical Society},
	author = {Lauritzen, S. L. and Spiegelhalter, D. J.},
	year = {1988},
	keywords = {graphical\_model, inference},
	pages = {157--224},
}

@techreport{le_bihan_maintenance_2005,
	title = {Maintenance préventive et corrective - {IT09} : {Classement} des défauts internes, des fissures et ruptures de rails},
	institution = {RATP EST Voie},
	author = {Le Bihan, J. F.},
	year = {2005},
	keywords = {railway, maintenance},
}

@book{lang_redmine_2010,
	title = {Redmine},
	url = {http://www.redmine.org/},
	author = {Lang, Jean-Philippe and {others}},
	year = {2010},
}

@article{langseth_bayesian_2007,
	title = {Bayesian networks in reliability},
	volume = {92},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2005.11.037},
	doi = {10.1016/j.ress.2005.11.037},
	abstract = {Over the last decade, Bayesian networks (BNs) have become a popular tool for modelling many kinds of statistical problems. We have also seen a growing interest for using BNs in the reliability analysis community. In this paper we will discuss the properties of the modelling framework that make BNs particularly well suited for reliability applications, and point to ongoing research that is relevant for practitioners in reliability.},
	number = {1},
	journal = {Reliability Engineering \& System Safety},
	author = {Langseth, Helge and Portinale, Luigi},
	month = jan,
	year = {2007},
	keywords = {reliability, graphical\_model},
	pages = {92--108},
}

@inproceedings{kriaa_modeling_2012,
	address = {Cork, Ireland},
	title = {Modeling the {Stuxnet} {Attack} with {BDMP}: {Towards} {More} {Formal} {Risk} {Assessments}},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on {Risks} and {Security} of {Internet} and {Systems}},
	author = {Kriaa, Siwar and Bouissou, Marc and Piétre-Cambacédès, Ludovic},
	month = oct,
	year = {2012},
	file = {Kriaa_al2012Stuxnet_BDMP.pdf:/home/roland/Zotero/storage/U6MFIP74/Kriaa_al2012Stuxnet_BDMP.pdf:application/pdf},
}

@book{kooperberg_logspline_1992,
	title = {Logspline density estimation for censored data},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.5512},
	abstract = {Logspline density estimation is developed for data that may be right censored, left censored or interval censored. A fully automatic method, which involves the maximum likelihood method and may involve stepwise knot deletion and either AIC or BIC, is used to determine the estimate; in solving the maximum likelihood equations, the Newton-Raphson method is augmented by occasional searches in the direction of steepest ascent. Also, a user interface based on S is described for obtaining...},
	author = {Kooperberg, C. and Stone, C.},
	year = {1992},
	keywords = {estimation, splines, maximum\_likelihood, interval\_censoring},
}

@inproceedings{komarek_dynamic_2000,
	title = {A {Dynamic} {Adaptation} of {AD}-trees for {Efficient} {Machine} {Learning} on {Large} {Data} {Sets}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.1219},
	abstract = {This paper has no novel learning or statistics: it is concerned with making a wide class of preexisting statistics and learning algorithms computationally tractable when faced with data sets with massive numbers of records or attributes. It briefly reviews the static AD-tree structure of Moore and Lee (1998), and offers a new structure with more attractive properties: (1) the new structure scales better with the number of attributes in the data set; (2) it has zero initial build time; (3) it adaptively caches only statistics relevant to the current task; and (4) it can be used incrementally in cases where new data is frequently being appended to the data set. We provide a careful explanation of the data structure, and then empirically evaluate the performance under varying access patterns induced by different learning algorithms such as association rules, decision trees and Bayes net structures. We conclude by discussing the longer term benefits of the new structure...},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Machine} {Learning}},
	author = {Komarek, P. and Moore, A.},
	year = {2000},
	keywords = {data\_structure},
	pages = {495--502},
}

@article{kohda_risk-based_nodate,
	title = {Risk-based reconfiguration of safety monitoring system using dynamic {Bayesian} network},
	volume = {In Press, Corrected Proof},
	url = {http://dx.doi.org/10.1016/j.ress.2006.09.012},
	doi = {10.1016/j.ress.2006.09.012},
	abstract = {To prevent an abnormal event from leading to an accident, the role of its safety monitoring system is very important. The safety monitoring system detects symptoms of an abnormal event to mitigate its effect at its early stage. As the operation time passes by, the sensor reliability decreases, which implies that the decision criteria of the safety monitoring system should be modified depending on the sensor reliability as well as the system reliability. This paper presents a framework for the decision criteria (or diagnosis logic) of the safety monitoring system. The logic can be dynamically modified based on sensor output data monitored at regular intervals to minimize the expected loss caused by two types of safety monitoring system failure events: failed-dangerous (FD) and failed-safe (FS). The former corresponds to no response under an abnormal system condition, while the latter implies a spurious activation under a normal system condition. Dynamic Bayesian network theory can be applied to modeling the entire system behavior composed of the system and its safety monitoring system. Using the estimated state probabilities, the optimal decision criterion is given to obtain the optimal diagnosis logic. An illustrative example of a three-sensor system shows the merits and characteristics of the proposed method, where the reasonable interpretation of sensor data can be obtained.},
	journal = {Reliability Engineering \& System Safety},
	author = {Kohda, Takehisa and Cui, Weimin},
	keywords = {graphical\_model, modelling, dynamic, safety},
}

@techreport{kjaerulff_triangulation_1990,
	address = {Denmark},
	title = {Triangulation of graphs : algorithms giving small total state space},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.9834},
	abstract = {The problem of achieving small total state space for triangulated belief graphs (networks) is considered. It is an NP-complete problem to find a triangulation with minimum state space. Our interest in this topic originates from the field of knowledge engineering where the applied knowledge representation scheme is provided by the notion of causal probabilistic networks (belief networks); CPNs for short. The application of a generalised evidence propagation scheme in CPNs requires triangularity (chordality) of the actual network. The paper includes a survey and evaluation of existing triangulation algorithms most of which are found to be highly ineffective w.r.t. the applied efficiency measure. Simple heuristic methods are presented and found to produce high-quality triangulations. Moreover, we introduce a method by which any non-minimal triangulation may be turned into a minimal one. Furthermore, we present a stochastic algorithm based on a technique known},
	institution = {Aalborg University},
	author = {Kjaerulff, Uffe},
	month = mar,
	year = {1990},
	keywords = {graph},
}

@article{khan_estimating_1989,
	title = {On estimating parameters in a discrete {Weibull} distribution},
	volume = {38},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=44179},
	abstract = {Two discrete Weibull distributions are discussed, and a simple method is presented to estimate the parameters for one of them. Simulation results are given to compare this method with the method of moments. The estimates obtained by the two methods appear to have almost similar properties. The discrete Weibull data arise in reliability problems when the observed variable is discrete. The modeling of such a random phenomenon has already been accomplished. Estimation of parameters in these models is considered. Since the usual methods of estimation are not easy to apply, a simple method is suggested to estimate the unknown parameters. The estimates obtained by this method are comparable to those obtained by the method of moments. The method can be applied in most inferential problems. Though the authors have restricted themselves to type I distribution, their method of proportions for the estimation of parameters can be easily applied to the type II distribution as well},
	number = {3},
	journal = {Reliability, IEEE Transactions on},
	author = {Khan, M. S. A. and Khalique, A. and Abouammoh, A. M.},
	year = {1989},
	keywords = {estimation, discrete\_time, weibull, moment, proportion},
	pages = {348--350},
}

@book{kevin_bayes_2001,
	title = {The {Bayes} {Net} {Toolbox} for {MATLAB}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.25.1216},
	abstract = {The Bayes Net Toolbox (BNT) is an open-source Matlab package for directed graphical models. BNT supports many kinds of nodes (probability distributions), exact and approximate inference, parameter and structure learning, and static and dynamic models. BNT is widely used in teaching and research: the web page has received over 28,000 hits since May 2000. In this paper, we discuss a broad spectrum of issues related to graphical models (directed and undirected), and describe, at a high-level, how...},
	author = {Kevin, Murphy P.},
	year = {2001},
	keywords = {graphical\_model, software},
}

@article{keller_historical_2005,
	title = {A historical overview of probabilistic risk assessment development and its use in the nuclear power industry: a tribute to the late {Professor} {Norman} {Carl} {Rasmussen}},
	volume = {89},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832004002327},
	doi = {http://dx.doi.org/10.1016/j.ress.2004.08.022},
	abstract = {This paper reviews the historical development of the probabilistic risk assessment (PRA) methods and applications in the nuclear industry. A review of nuclear safety and regulatory developments in the early days of nuclear power in the United States has been presented. It is argued that due to technical difficulties for measuring and characterizing uncertainties and concerns over legal challenges, safety design and regulation of nuclear power plants has primarily relied upon conservative safety assessment methods derived based on a set of design and safety principles. Further, it is noted that the conservatism adopted in safety and design assessments has allowed the use of deterministic performance assessment methods. This approach worked successfully in the early years of nuclear power epoch as the reactor design proved to be safe enough. However, it has been observed that as the conservative approach to design and safety criteria proved arbitrary, and yielded inconsistencies in the degree to which different safety measures in nuclear power plants protect safety and public heath, the urge for a more consistent assessment of safety became apparent in the late 1960s. In the early 1970s, as a result of public and political pressures, then the \{US\} Atomic Energy Commission initiated a new look at the safety of the nuclear power plants through a comprehensive study called ‘Reactor Safety Study’ (WASH-1400, or ‘Rasmussen Study’—after its charismatic study leader Professor Norman Rasmussen of MIT) to demonstrate safety of the nuclear power plants. Completed in October 1975, this landmark study introduced a novel probabilistic, systematic and holistic approach to the assessment of safety, which ultimately resulted in a sweeping paradigm shift in safety design and regulation of nuclear power in the United States in the turn of the Century. Technical issues of historic significance and concerns raised by the subsequent reviews of the Rasmussen Study have been discussed. Effect of major events and developments such as the Three Mile Island accident and the Nuclear Regulatory Commission and the Nuclear Industry sponsored studies on the tools, techniques and applications of the \{PRA\} that culminated in the present day risk-informed initiatives has been discussed.},
	number = {3},
	journal = {Reliability Engineering \& System Safety},
	author = {Keller, William and Modarres, Mohammad},
	year = {2005},
	keywords = {PRA},
	pages = {271 -- 285},
}

@book{keeping_introduction_1962,
	address = {Princeton, New Jersey},
	title = {Introduction to {Statistical} {Inference}},
	publisher = {D. Van Nostrand},
	author = {Keeping, E. S.},
	year = {1962},
	keywords = {statistics},
}

@inproceedings{kebaili_novel_2007,
	title = {A novel {Bayesian} {Network} structure learning algorithm based on minimal correlated itemset mining techniques},
	booktitle = {Proceedings of the second {IEEE} {International} {Conference} on {Digital} {Information} {Management}},
	author = {Kebaili, Zahra and Aussem, Alexandre},
	year = {2007},
	note = {event-place: Lyon, France},
	keywords = {graphical\_model, structure\_learning},
	pages = {121--126},
}

@article{kebaili_novel_2009,
	title = {A novel hybrid {Bayesian} network structure learning algorithm based on correlated itemset mining techniques},
	volume = {5},
	number = {1},
	journal = {International Journal of Computational Intelligence Research},
	author = {Kebaili, Zahra and Aussem, Alexandre},
	year = {2009},
	keywords = {graphical\_model, structure\_learning},
	pages = {16--21},
}

@book{kay_fundamentals_1993,
	series = {Signal {Processing} {Series}},
	title = {Fundamentals of statistical signal processing : {Estimation} theory},
	publisher = {Prentice Hall},
	author = {Kay, S. M.},
	year = {1993},
	keywords = {estimation, statistics, signal},
}

@article{kancev_new_2012,
	title = {A new method for explicit modelling of single failure event within different common cause failure groups},
	volume = {103},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832012000439},
	doi = {http://dx.doi.org/10.1016/j.ress.2012.03.009},
	abstract = {Redundancy and diversity are the main principles of the safety systems in the nuclear industry. Implementation of safety components redundancy has been acknowledged as an effective approach for assuring high levels of system reliability. The existence of redundant components, identical in most of the cases, implicates a probability of their simultaneous failure due to a shared cause—a common cause failure. This paper presents a new method for explicit modelling of single component failure event within multiple common cause failure groups simultaneously. The method is based on a modification of the frequently utilised Beta Factor parametric model. The motivation for development of this method lays in the fact that one of the most widespread softwares for fault tree and event tree modelling as part of the probabilistic safety assessment does not comprise the option for simultaneous assignment of single failure event to multiple common cause failure groups. In that sense, the proposed method can be seen as an advantage of the explicit modelling of common cause failures. A standard standby safety system is selected as a case study for application and study of the proposed methodology. The results and insights implicate improved, more transparent and more comprehensive models within probabilistic safety assessment.},
	number = {0},
	journal = {Reliability Engineering \& System Safety},
	author = {Kančev, Duško and Čepin, Marko},
	year = {2012},
	keywords = {Common cause failures},
	pages = {84 -- 93},
}

@article{kay_proportional_1977,
	title = {Proportional {Hazard} {Regression} {Models} and the {Analysis} of {Censored} {Survival} {Data}},
	volume = {26},
	number = {3},
	journal = {Applied Statistics},
	author = {Kay, R.},
	year = {1977},
	keywords = {reliability, estimation, regression, censoring},
	pages = {227--237},
}

@article{kang_bayesian_1999,
	title = {A {Bayesian} belief network-based advisory system for operational availability focused diagnosis of complex nuclear power systems},
	volume = {17},
	url = {http://dx.doi.org/10.1016/S0957-4174(99)00018-4},
	doi = {10.1016/S0957-4174(99)00018-4},
	abstract = {The work reported here provides a framework of diagnostic advisory system for improved operational availability in complex nuclear power plant systems. The rule-based approach typically used for conventional expert systems is abandoned in this work. This is because of the inability of rule-based approaches to properly model the inherent uncertainties and complexities of the relationships involved in the diagnosis of actual complex engineering systems. Rather, our advisory system employs Bayesian belief network (BBN) as a high-level reasoning tool for incorporating inherent uncertainty for use in probabilistic inference. We demonstrate that a rule-based knowledge representation is simply a special case of a general BBN. First, we outline a sequential algorithm to be used in formulating the BBN-based diagnostic operational advice. Then, a prototype BBN-based representation is encoded explicitly through topological symbols and links between them, oriented in a causal direction. Once new system state related evidence from an associated sensor network is entered into this advisory system, it provides an operational advice concerning how to maintain both operational availability and safety.Based upon the framework presented here, further development of our diagnostic maintenance network, integrating a comprehensive sensor network, can be expected to lead to substantial economic gains.},
	number = {1},
	journal = {Expert Systems with Applications},
	author = {Kang, C. W. and Golay, M. W.},
	month = jul,
	year = {1999},
	keywords = {graphical\_model, maintenance},
	pages = {21--32},
}

@article{kaltenbrunner_urban_2010,
	title = {Urban cycles and mobility patterns: {Exploring} and predicting trends in a bicycle-based public transport system},
	volume = {6},
	issn = {1574-1192},
	url = {//www.sciencedirect.com/science/article/pii/S1574119210000568},
	doi = {http://dx.doi.org/10.1016/j.pmcj.2010.07.002},
	abstract = {This paper provides an analysis of human mobility data in an urban area using the amount of available bikes in the stations of the community bicycle program Bicing in Barcelona. Based on data sampled from the operator’s website, it is possible to detect temporal and geographic mobility patterns within the city. These patterns are applied to predict the number of available bikes for any station some minutes/hours ahead. The predictions could be used to improve the bicycle program and the information given to the users via the Bicing website.},
	number = {4},
	journal = {Pervasive and Mobile Computing},
	author = {Kaltenbrunner, Andreas and Meza, Rodrigo and Grivolla, Jens and Codina, Joan and Banchs, Rafael},
	year = {2010},
	keywords = {Mobility pattern BSS availability},
	pages = {455 -- 466},
}

@article{kalman_new_1960,
	title = {A new approach to linear filtering and prediction problems},
	volume = {82},
	number = {1},
	journal = {Journal of basic Engineering},
	author = {Kalman, R. E.},
	year = {1960},
	keywords = {signal, filtering},
	pages = {35--45},
}

@book{kalbfleisch_statistical_2002,
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {The {Statistical} {Analysis} of {Failure} {Time} {Data}. {Second} {Edition}},
	publisher = {Wiley},
	author = {Kalbfleisch, J. D. and Prentice, R. L.},
	year = {2002},
	keywords = {reliability, estimation, continuous\_time, modelling, non\_parametric, parametric},
}

@techreport{jordan_graphical_2002,
	address = {Berkeley, CA},
	title = {Graphical models : {Probabilistic} inference},
	institution = {EECS Computer Science Division},
	author = {Jordan, Micheal I. and Weiss, Yair},
	year = {2002},
	keywords = {graphical\_model, inference},
}

@book{jordan_learning_1999,
	address = {Cambridge, MA, USA},
	title = {Learning in graphical models},
	publisher = {MIT Press},
	author = {Jordan, M.},
	year = {1999},
	keywords = {graphical\_model, learning, bibtex-import},
}

@article{jordan_introduction_1999,
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	volume = {37},
	issn = {08856125},
	url = {http://dx.doi.org/10.1023/A:1007665907178},
	doi = {10.1023/A:1007665907178},
	abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
	number = {2},
	journal = {Machine Learning},
	author = {Jordan, M. I. and Ghahramani, Z. and Jaakkola, T. S. and Saul, L. K.},
	month = nov,
	year = {1999},
	keywords = {graphical\_model, inference},
	pages = {183--233},
}

@article{jiang_graphical_1992,
	title = {Graphical representation of two mixed-{Weibull} distributions},
	volume = {41},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=257789},
	abstract = {A variety of shapes of two-Weibull mixtures on Weibull probability paper are explored and classified into six types of Cdf curves (A-F). Types B-D represent Cdf's composed of two well-mixed subpopulations. Type F represents the Cdf composed of two very well-separated subpopulations. Types A and E are in between types B-D and type F. The Kao-Cran graphical parameter estimation method cannot be applied to type A-C, E, and F curves. It is recommended that it not be applied to the type D curve. The theoretical basis is developed for the Jensen-Petersen graphical method for mixtures with well-separated subpopulations, viz, the type F curve. For type A and E curves, the method can be applied. However, for type B-D curves, there is no theoretical basis for the method},
	number = {2},
	journal = {Reliability, IEEE Transactions on},
	author = {Jiang, S. and Kececioglu, D.},
	year = {1992},
	pages = {241--247},
}

@article{jensen_bayesian_1990,
	title = {Bayesian updating in causal probabilistic networks by local computations},
	volume = {4},
	journal = {Computational Statistics Quaterly},
	author = {Jensen, F. V. and Lauritzen, S. L. and Olesen, K. G.},
	year = {1990},
	keywords = {graphical\_model, inference},
	pages = {269--282},
}

@book{jones_scipy:_2001,
	title = {{SciPy}: {Open} source scientific tools for {Python}},
	url = {http://www.scipy.org/},
	author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and {others}},
	year = {2001},
}

@article{jiang_maximum_1992,
	title = {Maximum likelihood estimates, from censored data, for mixed-{Weibull} distributions},
	volume = {41},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=257791},
	abstract = {An algorithm for estimating the parameters of mixed-Weibull distributions from censored data is presented. The algorithm follows the principle of the MLE (maximum likelihood estimate) through the EM (expectation and maximization) algorithm, and it is derived for both postmortem and non-postmortem time-to-failure data. The MLEs of the nonpostmortem data are obtained for mixed-Weibull distributions with up to 14 parameters in a five-subpopulation mixed-Weibull distribution. Numerical examples indicate that some of the log-likelihood functions of the mixed-Weibull distributions have multiple local maxima; therefore the algorithm should start at several initial guesses of the parameters set. It is shown that the EM algorithm is very efficient. On the average for two-Weibull mixtures with a sample size of 200, the CPU time (on a VAX 8650) is 0.13 s/iteration. The number of iterations depends on the characteristics of the mixture. The number of iterations is small if the subpopulations in the mixture are well separated. Generally, the algorithm is not sensitive to the initial guesses of the parameters},
	number = {2},
	journal = {Reliability, IEEE Transactions on},
	author = {Jiang, S. and Kececioglu, D.},
	year = {1992},
	pages = {248--255},
}

@book{jensen_introduction_1996,
	title = {An introduction to {Bayesian} networks},
	publisher = {UCL Press},
	author = {Jensen, Finn V.},
	year = {1996},
	keywords = {graphical\_model},
}

@article{huang_inference_1996,
	title = {Inference in {Belief} {Networks}: {A} {Procedural} {Guide}},
	volume = {15},
	url = {#},
	number = {3},
	journal = {International Journal of Approximate Reasoning},
	author = {Huang, C. and Darwiche, A.},
	month = oct,
	year = {1996},
	keywords = {graphical\_model, inference},
	pages = {225--263},
}

@inproceedings{jensen_influence_1994,
	title = {From influence diagrams to junction trees},
	url = {http://eprints.kfupm.edu.sa/42043/},
	booktitle = {Proceedings of the tenth conference on {Uncertainty} in {Artificial} {Intelligence}},
	author = {Jensen, F. and Jensen, F. V. and Dittmer, S. L.},
	year = {1994},
	keywords = {influence\_diagrams},
}

@book{howard_dynamic_2007,
	edition = {1},
	title = {Dynamic {Probabilistic} {Systems}, {Volume} {II}: {Semi}-{Markov} and {Decision} {Processes}},
	isbn = {0-486-45872-5},
	url = {http://www.worldcat.org/isbn/0486458725},
	abstract = {This book is an integrated work published in two volumes. The first volumetreats the basic Markov process and its variants; the second, semi-Markov anddecision processes. It equips readers to formulate, analyze, and evaluatesimple and advanced Markov models of systems, ranging from genetics to spaceengineering to marketing. 1971 edition.},
	publisher = {Dover Publications},
	author = {Howard, Ronald A.},
	month = jun,
	year = {2007},
	note = {Published: Paperback},
}

@book{howard_dynamic_2007-1,
	title = {Dynamic {Probabilistic} {Systems}, {Volume} {I}: {Markov} {Models}},
	isbn = {0-486-45870-9},
	url = {http://www.worldcat.org/isbn/0486458709},
	abstract = {{\textless}div{\textgreater}{\textless}div{\textgreater}{\textless}div{\textgreater}This book is an integrated work published in two volumes. The first volume treats the basic Markov process and its variants; the second, semi-Markov and decision processes. It equips readers to formulate, analyze, and evaluate simple and advanced Markov models of systems, ranging from genetics to space engineering to marketing. 1971 edition.{\textless}/div{\textgreater}{\textless}/div{\textgreater}{\textless}/div{\textgreater}},
	publisher = {Dover Publications},
	author = {Howard, Ronald A.},
	month = jun,
	year = {2007},
	note = {Published: Paperback},
}

@incollection{hokstad_common_2008,
	title = {Common cause failure modeling: status and trends},
	booktitle = {Handbook of performability engineering},
	publisher = {Springer},
	author = {Hokstad, Per and Rausand, Marvin},
	year = {2008},
	pages = {621--640},
}

@article{horenbeek_dynamic_2013,
	title = {A dynamic predictive maintenance policy for complex multi-component systems},
	volume = {120},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832013000732},
	doi = {https://doi.org/10.1016/j.ress.2013.02.029},
	journal = {Reliability Engineering \& System Safety},
	author = {Horenbeek, Adriaan Van and Pintelon, Liliane},
	year = {2013},
	keywords = {Maintenance grouping, Maintenance optimization, Multi-component system, Predictive maintenance policy},
	pages = {39 -- 50},
	file = {VanHorenbeek_Pintelon2013Dynamic_PM.PDF:/home/roland/Zotero/storage/FRLYLCVE/VanHorenbeek_Pintelon2013Dynamic_PM.PDF:application/pdf},
}

@article{hernandez_weibull_2006,
	title = {Weibull mixture model to characterise end-to-end {Internet} delay at coarse time-scales},
	volume = {153},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1610505},
	abstract = {Traces collected at monitored points around the Internet contain representative performance information about the paths their probes traverse. If processed appropriately, basic measurement attributes, such as delay and loss, can be used to output conclusions about the performance status of the network, with subsequent applications in fault and performance management, network provisioning, traffic engineering and performance prediction. However, the task of analysis and extracting such valuable information from measurements only remains challenging. The Weibull mixture model, a method to characterise end-to-end network delay measurements within a few simple, accurate, representative and handleable parameters using a finite combination of Weibull distributions is presented. The model parameters are related to meaningful delay characteristics, such as average peak and tail behaviour in a daily profile, and can be optimally found using an iterative algorithm known as expectation maximisation. Studies on such parameter evolution can reflect current workload status and all possible network events impacting packet dynamics, with further applications in network management. The model is further tested and validated with real GPS synchronised measurements taken across the Internet, donated by RIPE NCC.},
	number = {2},
	journal = {Communications, IEE Proceedings-},
	author = {Hernandez, J. A. and Phillips, I. W.},
	year = {2006},
	pages = {295--304},
}

@article{henrion_practical_1988,
	title = {Practical issues in constructing a {Bayes} belief network},
	volume = {2},
	number = {3},
	journal = {Int. J. Approx. Reasoning},
	author = {Henrion, Max},
	year = {1988},
	pages = {337},
	file = {Henrion1988Practical.pdf:/home/roland/Zotero/storage/45WBAWW4/Henrion1988Practical.pdf:application/pdf},
}

@article{hartley_maximum_1958,
	title = {Maximum {Likelihood} {Estimation} from {Incomplete} {Data}},
	volume = {14},
	issn = {0006341X},
	url = {http://dx.doi.org/10.2307/2527783},
	doi = {10.2307/2527783},
	number = {2},
	journal = {Biometrics},
	author = {Hartley, H. O.},
	month = jun,
	year = {1958},
	keywords = {em, multinomial},
	pages = {174--194},
}

@article{guo_linear-spline_1996,
	title = {Linear-spline approximation for semi-parametric modeling of failure data with proportional hazards},
	volume = {45},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=510812},
	abstract = {Modeling of failure processes using proportional hazards involves estimating both a baseline failure intensity as well as the parameters of the proportional failure intensity. In the absence of any information regarding the baseline failure intensity, a nonparametric form is typically assumed. This paper proposes a linear-spline function to approximate this baseline failure intensity, and develops such a spline function appropriate to bad-as-old failure data generated from a repairable system. Field data from an industrial setting demonstrate an improved approximation using such a spline function as compared to other procedures in the literature},
	number = {2},
	journal = {Reliability, IEEE Transactions on},
	author = {Guo, R. and Love, C. E.},
	year = {1996},
	pages = {261--266},
}

@book{gross_handbook_2004,
	title = {Handbook of {Graph} {Theory}},
	publisher = {CRC Press},
	author = {Gross, J. L. and Yellen, J.},
	year = {2004},
	keywords = {graph},
}

@article{grigoriev_modeling_2006,
	title = {Modeling and solving the periodic maintenance problem},
	volume = {172},
	issn = {03772217},
	url = {http://dx.doi.org/10.1016/j.ejor.2004.11.013},
	doi = {10.1016/j.ejor.2004.11.013},
	abstract = {We study the problem of scheduling maintenance services. Given is a set of m machines and integral cost-coefficients a i and b i for each machine i (1 ... i ... m ). Time is discretized into unit-length periods; in each period at most one machine can be serviced at a given service cost b i . The operating cost of machine i in a period equals a i times the number of periods since the last servicing of that machine i . The problem is to find a cyclic maintenance schedule of a given length T that minimizes total service and operating costs. We call this problem the periodic maintenance problem or PMP. In this work we are interested in computing optimal solutions to instances of PMP. We investigate several formulations for PMP. Two formulations, referred to as a flow formulation and a set-partitioning formulation, appear to have good linear programming relaxations. We exploit the problem structure by showing how the column generation subproblem can be solved in polynomial time. Our work leads to the first exact solutions for larger sized problem instances, and we present extensive computational results.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Grigoriev, A. and Vandeklundert, J. and Spieksma, F.},
	month = aug,
	year = {2006},
	keywords = {maintenance, periodic, preventive},
	pages = {783--797},
}

@inproceedings{griffault_langage_1999,
	title = {Le langage {AltaRica}},
	booktitle = {11th {IMdR} annual {Conference} on {Dependability} ({Lambda}-{Mu})},
	author = {Griffault, A and Lajeunesse, S and Point, G and Rauzy, A and Signoret, JP and Thomas, P},
	year = {1999},
}

@article{grall_condition-based_2002,
	title = {A condition-based maintenance policy for stochastically deteriorating systems},
	volume = {76},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/S0951-8320(01)00148-X},
	doi = {10.1016/S0951-8320(01)00148-X},
	abstract = {We focus on the analytical modeling of a condition -based inspection/replacement policy for a stochastically and continuously deteriorating single-unit system. We consider both the replacement threshold and the inspection schedule as decision variables for this maintenance problem and we propose to implement the maintenance policy using a multi-level control-limit rule. In order to assess the performance of the proposed maintenance policy and to minimize the long run expected maintenance cost per unit time, a mathematical model for the maintained system cost is derived, supported by the existence of a stationary law for the maintained system state. Numerical experiments illustrate the performance of the proposed policy and confirm that the maintenance cost rate on an infinite horizon can be minimized by a joint optimization of the maintenance structure thresholds, or equivalently by a joint optimization of a system replacement threshold and the aperiodic inspection schedule.},
	number = {2},
	journal = {Reliability Engineering \& System Safety},
	author = {Grall, A. and Bérenguer, C. and Dieulle, L.},
	month = may,
	year = {2002},
	keywords = {maintenance, preventive, condition-based},
	pages = {167--180},
}

@book{gilks_markov_1995,
	edition = {1},
	title = {Markov {Chain} {Monte} {Carlo} in {Practice}: {Interdisciplinary} {Statistics} ({Chapman} \& {Hall}/{CRC} {Interdisciplinary} {Statistics})},
	isbn = {0-412-05551-1},
	url = {http://www.worldcat.org/isbn/0412055511},
	abstract = {General state-space Markov chain theory has evolved to make it both more accessible and more powerful. Markov Chain Monte Carlo in Practice introduces MCMC methods and their applications while also providing some theoretical background. Considering the broad audience, the editors emphasize practice rather than theory and keep the technical content to a minimum. They offer step-by-step instructions for using the methods presented and show the importance of MCMC in real applications with examples ranging from the simple to the more complex in fields such as archaeology, astronomy, biostatistics, genetics, epidemiology, and image analysis.},
	publisher = {Chapman \& Hall/CRC},
	author = {Gilks, W. R.},
	month = dec,
	year = {1995},
	note = {Published: Hardcover},
}

@book{gelman_bayesian_2003,
	edition = {Second},
	series = {Texts in statistical {Science}},
	title = {Bayesian {Data} {Analysis}},
	publisher = {Chapman \& Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
	year = {2003},
	keywords = {statistics, bayesian},
}

@inproceedings{gebler_towards_2016,
	title = {Towards the implementation of a predictive maintenance strategy: {Lessons} learned from a case study within a waste processing plant},
	booktitle = {European {Conference} of the {Prognostics} and {Health} {Management} {Society} 2016},
	author = {Gebler, Owen Freeman and Hicks, Ben and Harrison, Andrew and Barker, Matt and Stirling, Pete},
	year = {2016},
	pages = {1--17},
	file = {Gebler_al2016PM_CaseStudy.pdf:/home/roland/Zotero/storage/RN6H9TYW/Gebler_al2016PM_CaseStudy.pdf:application/pdf},
}

@book{garey_computers_1979,
	title = {Computers and {Intractability}: {A} {Guide} to the {Theory} of {NP}-{Completeness}},
	publisher = {W. H. Freeman},
	author = {Garey, M. R. and Johnson, D. S.},
	year = {1979},
	keywords = {complexity},
}

@inproceedings{fricks_modeling_1997,
	address = {Istanbul, Turkey},
	title = {Modeling failure dependencies in reliability analysis using stochastic petri nets},
	booktitle = {Proceedings of {European} {Simulation} {Multiconference} ({ESM} 97)},
	author = {Fricks, Ricardo M and Trivedi, Kishor S},
	year = {1997},
}

@article{fries_survey_1996,
	title = {A survey of discrete reliability-growth models},
	volume = {45},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=556581},
	abstract = {This paper focuses on discrete reliability-growth models (DRGM), for which the relevant data comprise sequences of dichotomous success-failure outcomes from successive system configurations or stages. It presents a comprehensive compilation of model descriptions and characterizations, as well as discussions of related statistical methodologies for parameter estimation and confidence interval (or Bayes interval limit) construction. The emphasis is on the interrelationships between various models and the assumptions that underlie their development. Specific methodological enhancements that are either lacking from or have not been fully integrated into typical DRGM applications are specified},
	number = {4},
	journal = {Reliability, IEEE Transactions on},
	author = {Fries, A. and Sen, A.},
	year = {1996},
	pages = {582--604},
}

@phdthesis{francois_identification_2006,
	type = {{PhD} {Thesis}},
	title = {De l'identification de structure de réseaux bayésiens à la reconnaissance de formes à partir d'informations complètes ou incomplètes},
	school = {Institut National des Sciences Appliquées de Rouen},
	author = {François, O.},
	year = {2006},
	keywords = {graphical\_model, learning},
}

@inproceedings{foulliaron_specific_2015,
	title = {A specific semi-markovian dynamic bayesian network estimating residual useful life},
	booktitle = {16th {ASMDA} {Conference}},
	author = {Foulliaron, Josquin and Bouillaut, Laurent and Aknin, Patrice and Barros, Anne},
	year = {2015},
	pages = {13--p},
}

@article{forrest_odds-setters_2005,
	title = {Odds-setters as forecasters: {The} case of {English} football},
	volume = {21},
	url = {http://dx.doi.org/10.1016/j.ijforecast.2005.03.003},
	doi = {10.1016/j.ijforecast.2005.03.003},
	abstract = {Sets of odds issued by bookmakers may be interpreted as incorporating implicit probabilistic forecasts of sporting events. Employing a sample of nearly 10 000 English football (soccer) games, we compare the effectiveness of forecasts based on published odds and forecasts made using a benchmark statistical model incorporating a large number of quantifiable variables relevant to match outcomes. The experts' views, represented by the published odds, are shown to be increasingly effective over a 5-year period. Bootstraps performed on the statistical model fail to outperform the expert judges. The trend towards odds-setters displaying greater expertise as forecasters coincided with a period during which intensifying competition is likely to have increased the financial penalties for bookmakers of imprecise odds-setting. In the context of a financially pressured environment, the main findings of this paper challenge the consensus that subjective forecasting by experts will normally be inferior to forecasts from statistical models.},
	number = {3},
	journal = {International Journal of Forecasting},
	author = {Forrest, David and Goddard, John and Simmons, Robert},
	year = {2005},
	keywords = {forecasting, gambling, soccer},
	pages = {551--564},
}

@article{fleming_systematic_1986,
	title = {A systematic procedure for the incorporation of common cause events into risk and reliability models},
	volume = {93},
	issn = {0029-5493},
	url = {http://www.sciencedirect.com/science/article/pii/0029549386902232},
	doi = {http://dx.doi.org/10.1016/0029-5493(86)90223-2},
	number = {2–3},
	journal = {Nuclear Engineering and Design},
	author = {Fleming, Karl N. and Mosleh, Ali and Deremer, R. Kenneth},
	year = {1986},
	pages = {245 -- 273},
}

@techreport{fleming_reliability_1974,
	title = {Reliability model for common mode failures in redundant safety systems},
	url = {http://www.osti.gov/scitech/servlets/purl/4206606},
	number = {GA-A13284},
	institution = {General Atomic Company},
	author = {Fleming, K.N.},
	month = dec,
	year = {1974},
}

@techreport{fleming_issues_2003,
	title = {Issues and {Recommendations} for {Advancement} of {PRA} {Technology} in {Risk}-{Informed} {Decision} {Making}},
	number = {NUREG/CR-6813},
	institution = {U.S. Nuclear Regulatory Commission},
	author = {Fleming, K. N.},
	year = {2003},
}

@article{fleming_common_1985,
	title = {Common cause data analysis and implications in system modeling},
	volume = {19},
	abstract = {The purpose of this paper is to discuss some recent advances that have been made in the treatment of dependent events in the utility-sponsored probabilistic risk assessments (PRA) on Seabrook and Midland nuclear power plants and in research projects sponsored by the Electric Power Research Institute (EPRI). The particular topics of interest in this paper are the applications of a classification system to the development of a dependent events data base and the analysis of these data to support system level modeling and quantification of dependent events of the type performed in applied risk and reliability evaluation. The EPRI classification system has made it possible to develop a dependent events data base to support qualitative and quantitative systems analyses. The portion of the data base completed thus far for selected components has identified 422 dependent events, of which 113 have been classified as generic common cause events. Many of the existing published PRA studies, especially those that did not include parametric modeling of common cause events, did not account for most of these 113 events. This is due to overreliance on explicit modeling, which has proved reliable only for those dependent events resulting from the functional dependence among several components. It is important that future applied risk and reliability studies be held accountable for all experienced and documented dependent events},
	number = {06},
	journal = {International topical meeting on probabilistic safety methods and applications},
	author = {Fleming, K.N. and Mosleh, A.},
	year = {1985},
}

@article{fisher_mathematical_1922,
	title = {On the {Mathematical} {Foundations} of {Theoretical} {Statistics}},
	volume = {222},
	issn = {02643952},
	url = {http://dx.doi.org/10.2307/91208},
	doi = {10.2307/91208},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Fisher, R. A.},
	year = {1922},
	keywords = {maximum\_likelihood, statistics},
	pages = {309--368},
}

@article{fernandes_methodes_1998,
	title = {Méthodes numériques pour la solution de systèmes {Markoviens} à grand espace d’états},
	journal = {INPG, Grenoble},
	author = {Fernandes, P},
	year = {1998},
}

@article{engell_analysis_2003,
	title = {Analysis and design of hybrid systems},
	journal = {Lecture Notes in Computer Science. Springer},
	author = {Engell, S and Guéguen, H and Zaytoon, J},
	year = {2003},
}

@article{duval_analyse_2012,
	title = {L'analyse intégrée des risques au profit des système socio-techniques en lien fort avec l'environnement},
	volume = {18},
	journal = {Actes du congrès Lambda Mu},
	author = {Duval, Carole and Fallet-Fidry, Geoffrey and Sibler, Alain and Iung, Benoˆıt},
	year = {2012},
}

@book{dreo_metaheuristiques_2003,
	series = {Algorithmes},
	title = {Métaheuristiques pour l'optimisation difficile},
	isbn = {2-212-11368-4},
	publisher = {Eyrolles},
	author = {Dréo, J. and Pétrowski, A. and Siarry, P. and Taillard, E.},
	month = sep,
	year = {2003},
}

@inproceedings{donat_reseaux_2008,
	title = {Réseaux bayésiens dynamiques pour la représentation de modèles de durée en temps discret},
	booktitle = {actes de la 4ème {Journée} {Francaise} sur les {Réseaux} {Bayésiens}},
	author = {Donat, Roland and Leray, Philippe and Bouillaut, Laurent and Aknin, Patrice},
	month = may,
	year = {2008},
	note = {event-place: Lyon},
	keywords = {graphical\_model, duration},
}

@article{donat_dynamic_2010-1,
	title = {A {Dynamic} {Bayesian} {Network} to represent {Discrete} {Duration} {Models}},
	volume = {73},
	copyright = {All rights reserved},
	abstract = {Originally devoted to specific applications such as biology, medicine and demography, duration models are now widely used in economy, finance or reliability. Recent works in various fields of application have shown the relevancy of using Bayesian networks to model complex systems, namely stochastic systems with an underlying distribution that does not fit to a well-known parametric form. In this paper, the description of a specific dynamic Bayesian network, referred to as Graphical Duration Model (GDM), is given. A GDM aims to represent a wide range of duration models. Its structure allows especially to fit multi-state systems featuring complex sojourn-time distributions and contextual dependencies. To that end, a duration variable is explicitly introduced to the state transition model which is classically represented by a Markov chain. A recursive algorithm efficiently perform inference in this model is derived along with its proof of correctness and space and time complexity studies. Finally, this approach is illustrated with an application in survival analysis in which the proposed model is compared with the commonly used Markov chain modelling.},
	number = {4-6},
	journal = {Neurocomputing},
	author = {Donat, Roland and Leray, Philippe and Bouillaut, Laurent and Aknin, Patrice},
	month = jan,
	year = {2010},
	keywords = {graphical\_model, inference},
	pages = {570--577},
}

@inproceedings{donat_common_2015,
	address = {Zürich},
	title = {Common {Cause} {Failures} in {Discrete} {Dynamic} {Models}: {Theory} and {Applications} in the {Figaro} {Modelling} {Language}},
	copyright = {All rights reserved},
	booktitle = {proceedings of 25th {European} {Safety} and {Reliability} {Conference} ({ESREL})},
	author = {Donat, Roland and Bouissou, Marc},
	month = sep,
	year = {2015},
	pages = {to appear},
}

@inproceedings{donat_comparison_2009,
	title = {Comparison of two {Graphical} models approaches for the modelling of multi-components system's reliability},
	copyright = {All rights reserved},
	booktitle = {proceedings of the 39th {International} {Conference} on {Computers} \& {Industrial} {Engineering}},
	author = {Donat, Roland and Bouillaut, Laurent and Neji, Abdelmoez and Aknin, Patrice},
	month = jul,
	year = {2009},
	note = {event-place: Troyes, France},
	keywords = {reliability, graphical\_model, markov, chain, duration, comparison},
}

@inproceedings{donat_generic_2007,
	address = {Glasgow, Scotland},
	title = {A {Generic} {Approach} to {Model} {Complex} {System} {Reliability} using {Graphical} {Duration} {Models}},
	copyright = {All rights reserved},
	booktitle = {proceedings of the {Fifth} {International} {Mathematical} {Methods} in {Reliability} {Conference}},
	author = {Donat, Roland and Bouillaut, Laurent and Aknin, Patrice and Leray, Philippe and Levy, Didier},
	month = jul,
	year = {2007},
	keywords = {reliability, graphical\_model, inference, duration, dynamic},
}

@inproceedings{donat_reliability_2008,
	address = {Barcelona, Spain},
	title = {Reliability {Analysis} using {Graphical} {Duration} {Models}},
	copyright = {All rights reserved},
	booktitle = {proceedings of the {Third} {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Donat, Roland and Bouillaut, Laurent and Aknin, Patrice and Leray, Philippe and Levy, Didier},
	month = mar,
	year = {2008},
	pages = {795--800},
}

@inproceedings{donat_specific_2008,
	title = {Specific {Graphical} {Models} for analysing {Reliability}},
	copyright = {All rights reserved},
	booktitle = {proceedings of 16th {IEEE} {Mediterranean} {Conference} on {Control} and {Automation}},
	author = {Donat, Roland and Bouillaut, Laurent and Aknin, Patrice and Leray, Philippe and Bondeux, Sandrine},
	month = jun,
	year = {2008},
	note = {event-place: Ajaccio, France},
	pages = {621--626},
}

@incollection{donat_dynamic_2008,
	address = {Amsterdam},
	title = {A {Dynamic} {Graphical} {Model} to {Represent} {Complex} {Survival} {Distributions}},
	copyright = {All rights reserved},
	booktitle = {Advances in {Mathematical} {Modeling} for {Reliability}},
	publisher = {IOS Press},
	author = {Donat, Roland and Bouillaut, Laurent and Aknin, Patrice and Leray, Philippe},
	editor = {Bedford, T. and Quigley, J. and Walls, L. and Alkali, B. and Daneshkhah, A. and Hardman, G.},
	year = {2008},
	pages = {17--24},
}

@techreport{donat_traitement_2004,
	title = {Traitement en ligne de vidéos et suivi de visages},
	institution = {Laboratoire Heudiasyc - UMR UTC/CNRS 6599},
	author = {Donat, Roland},
	month = feb,
	year = {2004},
}

@mastersthesis{donat_cinematographie_2005,
	title = {Cinématographie ultra rapide - Évaluation de méthodes de restauration d'images},
	school = {Université de Technologies de Compiègne},
	author = {Donat, Roland},
	month = sep,
	year = {2005},
}

@phdthesis{donat_reliability_2009,
	type = {Theses},
	title = {Reliability and maintenance modelling based on probabilistic graphical models : case study on rail prevention},
	url = {https://tel.archives-ouvertes.fr/tel-00474389},
	school = {INSA de Rouen},
	author = {Donat, Roland},
	month = nov,
	year = {2009},
	keywords = {Bayesian network, Fiabilité, Generic modelling, Inférence, Maintenance ferroviaire, Markovian assumption, Prévisionnelle, Réseaux bayésiens},
}

@phdthesis{donat_modelisation_2009,
	type = {Thèse {CIFRE} {RATP} - {IFFSTAR}},
	title = {Modélisation de la fiabilité et de la maintenance par modèles graphiques probabilistes - {Application} à la prévention des ruptures de rails},
	school = {IOS Press},
	author = {Donat, Roland and Bedford, T. and Quigley, J. and Walls, L. and Alkali, B. and Daneshkhah, A. and Hardman, G.},
	month = nov,
	year = {2009},
}

@techreport{dommes_traversee_2008,
	title = {La traversée de rue chez le piéton âgé - {Effets} d'une méthode de réentraînement sur simulateur},
	number = {final n°3},
	institution = {Convention INRETS/Fondation MAIF},
	author = {Dommes, Aurélie and Cavallo, Viola and Boustelitane, Fatma and Vienne, Fabrice and Caro, Stéphane and Donat, Roland and Perrot, Claude},
	month = oct,
	year = {2008},
}

@book{julien_jumeau_2020,
	title = {Le jumeau numérique: {De} l'intelligence artificielle à l'industrie agile},
	isbn = {978-2-10-081403-9},
	shorttitle = {Le jumeau numérique},
	abstract = {Un jumeau numérique est une représentation virtuelle dynamique d’un objet (produit, process ou service) qui permet des analyses, des simulations ou des prédictions. Il s’agit en somme de représenter n’importe quel objet physique ou processus industriel (d’une raquette de tennis à une ville, en passant par une chaîne de production) sous forme numérique afin de l’améliorer, l’optimiser ou encore en assurer la maintenance.En agrégeant toutes les données utiles à la conception, la production et le fonctionnement d'un objet, le jumeau numérique permet aussi de définir virtuellement de nouveaux produits, process et services en un temps réduit et à moindre coût, sans prototypage.Cet ouvrage, émaillé de nombreux témoignages d'industriels et de chercheurs, permettra aux responsables industriels de comprendre ces techniques et de développer des modèles adaptés à leurs usages et applications.},
	language = {fr},
	publisher = {Dunod},
	author = {Julien, Nathalie and Martin, Éric},
	month = jun,
	year = {2020},
	note = {Google-Books-ID: JozgDwAAQBAJ},
	keywords = {Science / Applied Sciences, Technology \& Engineering / General},
}

@techreport{voas_considerations_2021,
	title = {Considerations for {Digital} {Twin} {Technology} and {Emerging} {Standards}},
	url = {https://csrc.nist.gov/publications/detail/nistir/8356/archive/2021-04-16},
	abstract = {Digital twin technology enables the creation of electronic representations of real-world entities and the viewing of the state of those entities. Its full vision will require standards that have not yet been developed. It is relatively new although it uses many existing foundational technologies and, in many cases, appears similar to existing modeling and simulation capabilities. This report attempts to provide clarity in understanding the concept and purpose of digital twins. It offers a new definition for a digital twin, and describes characteristics, features, functions, and expected operational uses. The report then discusses novel cybersecurity challenges presented by digital twin architectures. Lastly, it discusses traditional cybersecurity challenges as well as trust considerations in the context of existing NIST guidance and documents.},
	language = {en},
	number = {NIST Internal or Interagency Report (NISTIR) 8356 (Draft)},
	urldate = {2022-05-03},
	institution = {National Institute of Standards and Technology},
	author = {Voas, Jeffrey and Mell, Peter and Piroumian, Vartan},
	month = apr,
	year = {2021},
	doi = {10.6028/NIST.IR.8356-draft},
	file = {Full Text PDF:/home/roland/Zotero/storage/24QKYRIN/Voas et al. - 2021 - Considerations for Digital Twin Technology and Eme.pdf:application/pdf;Snapshot:/home/roland/Zotero/storage/B2LIXA3X/2021-04-16.html:text/html},
}

@article{noauthor_survey_2015,
	title = {A survey of approaches combining safety and security for industrial control systems},
	volume = {139},
	issn = {0951-8320},
	url = {https://sciencedirect.ezproxy.univ-ubs.fr/science/article/pii/S0951832015000538},
	doi = {10.1016/j.ress.2015.02.008},
	abstract = {The migration towards digital control systems creates new security threats that can endanger the safety of industrial infrastructures. Addressing the …},
	language = {en},
	urldate = {2022-05-11},
	journal = {Reliability Engineering \& System Safety},
	month = jul,
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {156--178},
	file = {Snapshot:/home/roland/Zotero/storage/PZ3FLR44/S0951832015000538.html:text/html},
}

@article{kriaa_survey_2015,
	title = {A survey of approaches combining safety and security for industrial control systems},
	volume = {139},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832015000538},
	doi = {10.1016/j.ress.2015.02.008},
	abstract = {The migration towards digital control systems creates new security threats that can endanger the safety of industrial infrastructures. Addressing the convergence of safety and security concerns in this context, we provide a comprehensive survey of existing approaches to industrial facility design and risk assessment that consider both safety and security. We also provide a comparative analysis of the different approaches identiﬁed in the literature.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Reliability Engineering \& System Safety},
	author = {Kriaa, Siwar and Pietre-Cambacedes, Ludovic and Bouissou, Marc and Halgand, Yoran},
	month = jul,
	year = {2015},
	pages = {156--178},
	file = {Kriaa et al. - 2015 - A survey of approaches combining safety and securi.pdf:/home/roland/Zotero/storage/55DI72LY/Kriaa et al. - 2015 - A survey of approaches combining safety and securi.pdf:application/pdf},
}

@inproceedings{kriaa_modeling_2012-1,
	address = {Cork, Ireland},
	title = {Modeling the {Stuxnet} attack with {BDMP}: {Towards} more formal risk assessments},
	isbn = {978-1-4673-3089-3 978-1-4673-3087-9 978-1-4673-3088-6},
	shorttitle = {Modeling the {Stuxnet} attack with {BDMP}},
	url = {http://ieeexplore.ieee.org/document/6378942/},
	doi = {10.1109/CRISIS.2012.6378942},
	abstract = {Attack modeling has recently been adopted by security analysts as a useful tool in risk assessment of cyber-physical systems. We propose in this paper to model the Stuxnet attack with BDMP (Boolean logic Driven Markov Processes) formalism and to show the advantages of such modeling. After a description of the architecture targeted by Stuxnet, we explain the steps of the attack and model them formally with a BDMP. Based on estimated values of the success probabilities and rates of the elementary attack steps, we give a quantiﬁcation of the main possible sequences leading to the physical destruction of the targeted industrial facility. This example completes a series of papers on BDMP applied to security by modeling a real case study. It highlights the advantages of BDMP compared to attack trees often used in security assessment.},
	language = {en},
	urldate = {2022-05-11},
	booktitle = {2012 7th {International} {Conference} on {Risks} and {Security} of {Internet} and {Systems} ({CRiSIS})},
	publisher = {IEEE},
	author = {Kriaa, Siwar and Bouissou, Marc and Pietre-Cambacedes, Ludovic},
	month = oct,
	year = {2012},
	pages = {1--8},
	file = {Kriaa et al. - 2012 - Modeling the Stuxnet attack with BDMP Towards mor.pdf:/home/roland/Zotero/storage/JZRY7B9F/Kriaa et al. - 2012 - Modeling the Stuxnet attack with BDMP Towards mor.pdf:application/pdf},
}

@incollection{bondavalli_safety_2014,
	address = {Cham},
	title = {Safety and {Security} {Interactions} {Modeling} {Using} the {BDMP} {Formalism}: {Case} {Study} of a {Pipeline}},
	volume = {8666},
	isbn = {978-3-319-10505-5 978-3-319-10506-2},
	shorttitle = {Safety and {Security} {Interactions} {Modeling} {Using} the {BDMP} {Formalism}},
	url = {http://link.springer.com/10.1007/978-3-319-10506-2_22},
	abstract = {The digitalization of industrial control systems (ICS) raises several security threats that can endanger the safety of the critical infrastructures supervised by such systems. This paper presents an analysis method that enables the identiﬁcation and ranking of risks leading to a safety issue, regardless of the origin of those risks: accidental or due to malevolence. This method relies on a modeling formalism called BDMP (Boolean logic Driven Markov Processes) that was initially created for safety studies, and then adapted to security. The use of the method is ﬁrst illustrated on a simple case to show how it can be used to make decisions in a situation where security requirements are in conﬂict with safety requirements. Then it is applied to a realistic industrial system: a pipeline and its instrumentation and control system in order to highlight possible interactions between safety and security.},
	language = {en},
	urldate = {2022-05-11},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Kriaa, Siwar and Bouissou, Marc and Colin, Frederic and Halgand, Yoran and Pietre-Cambacedes, Ludovic},
	editor = {Bondavalli, Andrea and Di Giandomenico, Felicita},
	year = {2014},
	doi = {10.1007/978-3-319-10506-2_22},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {326--341},
	file = {Kriaa et al. - 2014 - Safety and Security Interactions Modeling Using th.pdf:/home/roland/Zotero/storage/3ZTVXMNG/Kriaa et al. - 2014 - Safety and Security Interactions Modeling Using th.pdf:application/pdf},
}

@article{kriaa_joint_nodate,
	title = {Joint safety and security modeling for risk assessment in cyber physical systems},
	language = {fr},
	author = {Kriaa, Siwar},
	pages = {173},
	file = {Kriaa - Joint safety and security modeling for risk assess.pdf:/home/roland/Zotero/storage/93BKRJDE/Kriaa - Joint safety and security modeling for risk assess.pdf:application/pdf},
}

@inproceedings{kriaa_better_2020,
	address = {Milano, Italy},
	title = {Better {Safe} than {Sorry}: {Modeling} {Reliability} and {Security} in {Replicated} {SDN} {Controllers}},
	isbn = {978-1-72816-300-0},
	shorttitle = {Better {Safe} than {Sorry}},
	url = {https://ieeexplore.ieee.org/document/9089388/},
	doi = {10.1109/DRCN48652.2020.1570604424},
	abstract = {Software-defined networks (SDN), through their programmability, significantly increase network resilience by enabling dynamic reconfiguration of network topologies in response to faults and potentially malicious attacks detected in real-time. Another key trend in network softwarization is cloudnative software, which, together with SDN, will be an integral part of the core of future 5G networks. In SDN, the control plane forms the ``brain’’ of the software-defined network and is typically implemented as a set of distributed controller replicas to avoid a single point of failure. Distributed consensus algorithms are used to ensure agreement among the replicas on key data even in the presence of faults. Security is also a critical concern in ensuring that attackers cannot compromise the SDN control plane; byzantine fault tolerance algorithms can provide protection against compromised controller replicas. However, while reliability/availability and security form key attributes of resilience, they are typically modeled separately in SDN, without consideration of the potential impacts of their interaction. In this paper we present an initial framework for a model that unifies reliability, availability, and security considerations in distributed consensus. We examine -- via simulation of our model -- some impacts of the interaction between accidental faults and malicious attacks on SDN and suggest potential mitigations unique to cloud-native software.},
	language = {en},
	urldate = {2022-05-11},
	booktitle = {2020 16th {International} {Conference} on the {Design} of {Reliable} {Communication} {Networks} {DRCN} 2020},
	publisher = {IEEE},
	author = {Kriaa, Siwar and Papillon, Serge and Jagadeesan, Lalita and Mendiratta, Veena},
	month = mar,
	year = {2020},
	pages = {1--6},
	file = {Kriaa et al. - 2020 - Better Safe than Sorry Modeling Reliability and S.pdf:/home/roland/Zotero/storage/NDRA6KSH/Kriaa et al. - 2020 - Better Safe than Sorry Modeling Reliability and S.pdf:application/pdf},
}

@article{kriaa_new_2018,
	title = {A new safety and security risk analysis framework for industrial control systems},
	volume = {233},
	doi = {10.1177/1748006X18765885},
	abstract = {The migration of modern industrial control systems toward information and communication technologies exposes them to cyber-attacks that can alter the way they function, thereby causing adverse consequences on the system and its environment. It has consequently become crucial to consider security risks in traditional safety risk analyses for industrial systems controlled by modern industrial control system. We propose in this article a new framework for safety and security joint risk analysis for industrial control systems. S-cube (for supervisory control and data acquisition safety and security joint modeling) is a new model-based approach that enables, thanks to a knowledge base, formal modeling of the physical and functional architecture of cyber-physical systems and automatic generation of a qualitative and quantitative analysis encompassing safety risks (accidental) and security risks (malicious). We first give the principle and rationale of S-cube and then we illustrate its inputs and outputs on a case study.},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
	author = {Kriaa, Siwar and Bouissou, Marc and Laarouchi, Youssef},
	month = apr,
	year = {2018},
	pages = {1748006X1876588},
	file = {Version soumise:/home/roland/Zotero/storage/DM7VI64D/Kriaa et al. - 2018 - A new safety and security risk analysis framework .pdf:application/pdf},
}

@inproceedings{peres_maitrise_2018,
	address = {Reims, France},
	title = {{MAITRISE} {DES} {RISQUES} {LIES} {AUX} {ASPECTS} {DE} {CYBERSECURITE} {ET} {SECURITE} {FERROVIAIRE}},
	url = {https://hal.archives-ouvertes.fr/hal-02074253},
	urldate = {2022-05-11},
	booktitle = {Congrès {Lambda} {Mu} 21 “ {Maîtrise} des risques et transformation numérique : opportunités et menaces ”},
	author = {Peres, Joanna and Caire, Jean and Delebarre, Véronique},
	month = oct,
	year = {2018},
	file = {HAL PDF Full Text:/home/roland/Zotero/storage/H7HUI5UI/Peres et al. - 2018 - MAITRISE DES RISQUES LIES AUX ASPECTS DE CYBERSECU.pdf:application/pdf},
}

@article{turner_game_nodate,
	title = {A {Game} {Theoretic} {Approach} to {Minimizing} {Cybersecurity} {Risk}},
	language = {en},
	author = {Turner, Andrew},
	pages = {30},
	file = {Turner - A Game Theoretic Approach to Minimizing Cybersecur.pdf:/home/roland/Zotero/storage/4TW67APA/Turner - A Game Theoretic Approach to Minimizing Cybersecur.pdf:application/pdf},
}

@inproceedings{caire_vers_2015,
	title = {Vers un cycle de vie de sécurité globale pour les systèmes informatiques industriels},
	isbn = {978-2-35147-037-4},
	url = {http://hdl.handle.net/2042/56195},
	doi = {10.4267/2042/56195},
	abstract = {Cybersecurity has become a paramount stake for industrial control systems, particularly those supporting critical infrastructures. This concern is leading States to establish new regulations demanding for a security certification of critical systems. But these ones must also satisfy several safety requirements which are met through stringent development and assurance processes, by using a set of rigorous and formalized methods. The question is how combining into a single process the essential safety and cybersecurity requirements. Our communication aims at defining a comprehensive set of cybersecurity principles and models allowing us to build a global security (i.e. safety plus security) lifecycle.},
	language = {fr},
	urldate = {2022-05-11},
	booktitle = {Congrès {Lambda} {Mu} 19 de {Maîtrise} des {Risques} et {Sûreté} de {Fonctionnement}},
	publisher = {IMdR},
	author = {Caire, J.},
	month = jan,
	year = {2015},
	file = {Caire - 2015 - Vers un cycle de vie de sécurité globale pour les .pdf:/home/roland/Zotero/storage/R88KKBW4/Caire - 2015 - Vers un cycle de vie de sécurité globale pour les .pdf:application/pdf},
}

@misc{noauthor_special_nodate,
	title = {Special {Session}: {Reliability} {Analysis} for {AI}/{ML} {Hardware}},
	shorttitle = {Special {Session}},
	url = {https://ieee.ezproxy.univ-ubs.fr/document/9441050/},
	abstract = {Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive in today's applications, such as autonomous vehicles, healthcare, aerospace, cybersecurity, and many critical applications. Ensuring the reliability and robustness of the underlying AI/ML hardware becomes our paramount importance. In this paper, we explore and evaluate the reliability of different AI/ML hardware. The first section outlines the reliability issues in a commercial systolic array-based ML accelerator in the presence of faults engendering from device-level non-idealities in the DRAM. Next, we quantified the impact of circuit-level faults in the MSB and LSB logic cones of the Multiply and Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we present two key reliability issues- circuit aging and endurance in emerging neuromorphic hardware platforms and present our system-level approach to mitigate them.},
	language = {en-US},
	urldate = {2022-07-05},
	file = {Snapshot:/home/roland/Zotero/storage/YALZBLZR/9441050.html:text/html},
}

@article{baklouti_improved_2019,
	title = {Improved {Safety} {Analysis} {Integration} in a {Systems} {Engineering} {Approach}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/6/1246},
	doi = {10.3390/app9061246},
	abstract = {The goal of the paper is the integration of safety analysis in a model-based systems engineering approach to ensure consistency between system design and safety artifacts. This integration permits the continuous improvement of the structure and behavior of the system. It also reduces system development time and prevents late detection of errors. To reach this purpose, the SafeSysE methodology is extended. In SafeSysE, a preliminary Failure Mode and Effects Analysis (FMEA) is automatically generated from a SysML model, and this FMEA is then completed by the safety expert but no further development was proposed. The contribution of this paper is to suggest recommendations based on the FMEA analysis in order to enhance the system design and make it comply with safety requirements. First, an updated system structure that may contain redundancy is proposed. Then, a redundancy profile is used to enrich the system model with redundancy information, which will allow the generation of a dynamic fault tree considering the system behavior. Finally, the generated dynamic fault tree should be analyzed in order to create a state machine diagram that describes the behavior of the system. The created state machine with an internal block diagram will help the system designers to better understand the system dysfunctions by simulating the system. The proposed methodology is applied to an Electro-Mechanical Actuator system which is used in the aeronautics domain.},
	language = {en},
	number = {6},
	urldate = {2022-07-05},
	journal = {Applied Sciences},
	author = {Baklouti, Anis and Nguyen, Nga and Mhenni, Faïda and Choley, Jean-Yves and Mlika, Abdelfattah},
	month = jan,
	year = {2019},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {redundancy, FMEA, MBSE, dynamic fault trees, redundancy profile, SA, system description},
	pages = {1246},
	file = {Full Text PDF:/home/roland/Zotero/storage/PWZUYYL5/Baklouti et al. - 2019 - Improved Safety Analysis Integration in a Systems .pdf:application/pdf;Snapshot:/home/roland/Zotero/storage/3MSUKDYE/1246.html:text/html},
}

@article{li_generalized_2021,
	title = {A generalized petri net-based modeling framework for service reliability evaluation and management of cloud data centers},
	volume = {207},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095183202030870X},
	doi = {10.1016/j.ress.2020.107381},
	abstract = {A cloud data center is a critical infrastructure whose service reliability is relevant for service delivery. Consti­ tuting the carrier of service for cloud computing, the IT architecture of a cloud data center plays an important role, since it directly relates to service reliability. However, most existing research focuses only on the con­ nectivity of the IT architecture and on service considering only the processing procedure. In order to bridge the gap between the existing works and reality, a hierarchical colored generalized stochastic petri net is proposed to evaluate the service reliability by Monte Carlo simulation, which comprehensively considers the connectivity and performance of the IT architecture and the dynamic of service delivery. The modeling and simulation framework is applied to the cloud data center of an insurance company and cost-effective strategies are found to support the configuration, operation and maintenance of the cloud data center.},
	language = {en},
	urldate = {2022-07-05},
	journal = {Reliability Engineering \& System Safety},
	author = {Li, Xiao-Yang and Liu, Yue and Lin, Yan-Hui and Xiao, Liang-Hua and Zio, Enrico and Kang, Rui},
	month = mar,
	year = {2021},
	pages = {107381},
	file = {Li et al. - 2021 - A generalized petri net-based modeling framework f.pdf:/home/roland/Zotero/storage/CLZHKK9N/Li et al. - 2021 - A generalized petri net-based modeling framework f.pdf:application/pdf},
}

@article{zhang_piecewise_2008,
	title = {Piecewise deterministic {Markov} processes and dynamic reliability},
	volume = {222},
	issn = {1748-006X},
	url = {https://doi.org/10.1243/1748006XJRR181},
	doi = {10.1243/1748006XJRR181},
	abstract = {If the reliability community remains interested in dynamic reliability theory, it is not really convinced by the ability of already available approaches to treating current problems from within the operational domain, even if the methodological quality of these approaches is undeniable. This paper is in keeping with two papers presented in earlier conferences. Its aim is to show the potentialities of a method that combines the high modelling capacity of the piecewise-deterministic processes with the great computing power inherent in the Monte Carlo simulation. This method has been applied to a well-known test-case example to test its ability to solve common dynamic reliability problems. Two sets of results have been obtained. The first one has been compared to those coming from a Petri-net model to obtain a preliminary validation of the proposed method. The second one, related to a more complex case, has been compared to already published results found in the literature. Contrary to already existing methods, the approach here is an exact Monte Carlo sampling method; it does not need time-space discretization.},
	language = {en},
	number = {4},
	urldate = {2022-08-22},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
	author = {Zhang, H and Dufour, F and Dutuit, Y and Gonzalez, K},
	month = dec,
	year = {2008},
	note = {Publisher: SAGE Publications},
	keywords = {Petri nets, dynamic reliability, piecewise deterministic Markov processes},
	pages = {545--551},
}

@article{desgeorges_formalism_2021,
	title = {Formalism and semantics of {PyCATSHOO}: {A} simulator of distributed stochastic hybrid automata},
	volume = {208},
	issn = {0951-8320},
	shorttitle = {Formalism and semantics of {PyCATSHOO}},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832020308711},
	doi = {10.1016/j.ress.2020.107384},
	abstract = {This article lays the mathematical foundations of PyCATSHOO, a Model-Based Safety Analysis (MBSA) framework relying on distributed stochastic hybrid automata. This tool was initially developed for use cases where continuous evolution of physical variables or component failure rates matter to assess the dependability attributes. The modelling language has been designed in order to provide to the analyst the best expressiveness and ease of use. Nevertheless, although the structure and behaviour of a PyCATSHOO model have been informally described previously, they have never been formally established, which precludes its scientific acceptance and slows down its adoption by new users. To fill this lack, this article introduces formal definitions of the structure of PyCATSHOO models using set theory and of their operational semantics using inference rules (exactly 1 axiom and eight inference rules). These formal definitions are illustrated on a simple case study: the heated room. As a result, our proposing disambiguates the semantics of PyCATSHOO models, provides a formal specification of its input language and the core logic of its simulator engine and paves the way to the integration of model checking techniques in the PyCATSHOO framework.},
	language = {en},
	urldate = {2022-08-25},
	journal = {Reliability Engineering \& System Safety},
	author = {Desgeorges, Loïc and Piriou, Pierre-Yves and Lemattre, Thibault and Chraibi, Hassane},
	month = apr,
	year = {2021},
	keywords = {Simulation, Semantics, Model based safety analysis, Formalization, Inference rules, Stochastic hybrid automata},
	pages = {107384},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/W72ZN45X/S0951832020308711.html:text/html;Version acceptée:/home/roland/Zotero/storage/EM9GB8A8/Desgeorges et al. - 2021 - Formalism and semantics of PyCATSHOO A simulator .pdf:application/pdf},
}

@article{rychkova_measuring_nodate,
	title = {Measuring {Maintainability} of {DPRA} {Models}: {A} {Pragmatic} {Approach}},
	abstract = {Dynamic Probabilistic Risk Assessment (DPRA) is a powerful concept that is used to evaluate design and safety of complex industrial systems. A DPRA model uses a conceptual system representation as a formal basis for simulation and analysis. In this paper we consider an adaptive maintenance of DPRA models that consist in modifying and extending a simpliﬁed model to a real-size DPRA model. We propose an approach for quantitative maintainability assessment of DPRA models created with an industrial modeling tool called PyCATSHOO. We review and adopt some metrics from conceptual modeling, software engineering and OO design for assessing maintainability of PyCATSHOO models. On the example of well-known ”Heated Room” test case, we illustrate how the selected metrics can serve as early indicators of model modiﬁability and complexity. These indicators would allow experts to make better decisions early in the DPRA model development life cycle.},
	language = {en},
	author = {Rychkova, Irina and Boissier, Fabrice and Chraibi, Hassane and Rychkov, Valentin},
	pages = {14},
	file = {Rychkova et al. - Measuring Maintainability of DPRA Models A Pragma.pdf:/home/roland/Zotero/storage/IZGC9752/Rychkova et al. - Measuring Maintainability of DPRA Models A Pragma.pdf:application/pdf},
}

@book{sanny_dynamic_2019,
	title = {{DYNAMIC} {PROBABILISTIC} {RISK} {ASSESSMENT} {WITH} {PyCATSHOO}: {THE} {CASE} {OF} {THE} {EMERGENCY} {POWER} {SUPPLY} {OF} {A} {NUCLEAR} {POWER} {PLANT}},
	shorttitle = {{DYNAMIC} {PROBABILISTIC} {RISK} {ASSESSMENT} {WITH} {PyCATSHOO}},
	author = {Sanny, Keoni and Picoco, Claudia and Aldemir, Tunc},
	month = apr,
	year = {2019},
}

@article{chraibi_integrated_2022,
	title = {Integrated dynamic probabilistic safety assessments with {PyCATSHOO}: a new coupling approach},
	abstract = {Integrated dynamic probabilistic safety assessment (IDPSA) approaches provide a valuable complement to the PSA classic methods that no longer needs to be justified. These hybrid approaches gather in the same model stochastic discrete events behavior and deterministic and time-dependent one which account for physical phenomena. However, these approaches are still facing several challenges such as modeling complexity, computational costs, data availability, post-processing difficulties etc. EDF contributes to meeting these challenges by developing the PyCATSHOO tool, which, among others, addresses the modeling complexity and calculation costs. The improvement effort of this tool is still ongoing and focuses on another challenge, namely the coupling methods between models that deal with discrete stochastic aspects and physical codes. In most experiments conducted to date, this coupling has been carried out thanks to ad hoc solutions and required a significant effort. However, a solution exists which could benefit IDPSA models. This solution is the FMI (Functional Mockup Interface) standard widely used in 0D/1D physical simulations. We have recently integrated this standard into PyCATSHOO. This article reports on this integration and gives an illustration based on the well-known Heated Tank system.},
	language = {en},
	author = {Chraibi, Hassane and Houdebine, Jean-Christophe and Picoco, Claudia and Rychkov, Valentin},
	year = {2022},
	pages = {12},
	file = {Chraibi et al. - 2022 - Integrated dynamic probabilistic safety assessment.pdf:/home/roland/Zotero/storage/XHI4G2QE/Chraibi et al. - 2022 - Integrated dynamic probabilistic safety assessment.pdf:application/pdf},
}

@article{dhaultfoeuille_regression_2014,
	title = {La régression quantile en pratique},
	volume = {471},
	issn = {0336-1454},
	url = {http://www.persee.fr/doc/estat_0336-1454_2014_num_471_1_10484},
	doi = {10.3406/estat.2014.10484},
	abstract = {Quantile regressions are statistical tools that describe the impact of explanatory variables on a variable of interest. They provide a more detailed picture than classic linear regression, as they focus on the entire conditional distribution of the dependent variable, not only on its mean. They are also more suited to some kind of data such as truncated and censored dependent variable, outcomes with fat-tailed distributions, nonlinear models... This document proposes a practical introduction to these tools, with a special interest on their implementation in standard statistical software (Sas, R, Stata). We also present in details two empirical applications, to help people interpreting studies that rely on these methods. Finally, we propose for more advanced readers recent extensions in particular on endogeneity issues (instrumental variables, panel data...).},
	language = {fr},
	number = {1},
	urldate = {2023-01-02},
	journal = {Economie et statistique},
	author = {D’Haultfoeuille, Xavier and Givord, Pauline},
	year = {2014},
	keywords = {\_tablet},
	pages = {85--111},
	file = {D’Haultfoeuille_Givord_2014_La régression quantile en pratique.pdf:/home/roland/Zotero/storage/X8W6B2KS/D’Haultfoeuille_Givord_2014_La régression quantile en pratique.pdf:application/pdf},
}

@article{hegazy_comparitive_nodate,
	title = {Comparitive {Automated} {Bitcoin} {Trading} {Strategies}},
	language = {en},
	author = {Hegazy, Kareem and Mumford, Samuel},
	keywords = {\_tablet\_modified},
	file = {Hegazy_Mumford_Comparitive Automated Bitcoin Trading Strategies.pdf:/home/roland/Zotero/storage/5WS8YR4D/Hegazy_Mumford_Comparitive Automated Bitcoin Trading Strategies.pdf:application/pdf},
}

@article{madan_automated_nodate,
	title = {Automated {Bitcoin} {Trading} via {Machine} {Learning} {Algorithms}},
	abstract = {In this project, we attempt to apply machine-learning algorithms to predict Bitcoin price. For the ﬁrst phase of our investigation, we aimed to understand and better identify daily trends in the Bitcoin market while gaining insight into optimal features surrounding Bitcoin price. Our data set consists of over 25 features relating to the Bitcoin price and payment network over the course of ﬁve years, recorded daily. Using this information we were able to predict the sign of the daily price change with an accuracy of 98.7\%. For the second phase of our investigation, we focused on the Bitcoin price data alone and leveraged data at 10-minute and 10-second interval timepoints, as we saw an opportunity to evaluate price predictions at varying levels of granularity and noisiness. By predicting the sign of the future change in price, we are modeling the price prediction problem as a binomial classiﬁcation task, experimenting with a custom algorithm that leverages both random forests and generalized linear models. These results had 50-55\% accuracy in predicting the sign of future price change using 10 minute time intervals.},
	language = {en},
	author = {Madan, Isaac and Saluja, Shaurya and Zhao, Aojia},
	keywords = {\_tablet\_modified},
	file = {Madan et al_Automated Bitcoin Trading via Machine Learning Algorithms.pdf:/home/roland/Zotero/storage/G4KWBQHH/Madan et al_Automated Bitcoin Trading via Machine Learning Algorithms.pdf:application/pdf},
}

@inproceedings{rychkov_model_2022,
	title = {Model based software engineering techniques for dynamic reliability assessment},
	doi = {10.3850/978-981-18-5183-4_S12-06-630-cd},
	abstract = {Despite obvious conceptual advantages, dynamic reliability methods are still far from the broad industrial applications. Very small market of industrial applications of dynamic reliability methods makes it very difficult to develop and maintain specific analysis tools. In this paper we present an application of an industrial tool coming from model driven software engineering domain that implements statechart concept in the context of dynamic reliability assessment. The main motivation behind this work is to show that a dynamic reliability model can be developed as a piece of software taking the advantage of existing tools leveraging decades of experience of the software development.},
	author = {Rychkov, Valentin and Picoco, Claudia},
	month = aug,
	year = {2022},
	keywords = {\_tablet},
	file = {Rychkov_Picoco_2022_Model based software engineering techniques for dynamic reliability assessment.pdf:/home/roland/Zotero/storage/JDA27D8F/Rychkov_Picoco_2022_Model based software engineering techniques for dynamic reliability assessment.pdf:application/pdf},
}

@article{musman_game_2018,
	title = {A game theoretic approach to cyber security risk management},
	volume = {15},
	issn = {1548-5129, 1557-380X},
	url = {http://journals.sagepub.com/doi/10.1177/1548512917699724},
	doi = {10.1177/1548512917699724},
	abstract = {This paper describes the Cyber Security Game (CSG). Cyber Security Game is a method that has been implemented in software that quantitatively identifies cyber security risks and uses this metric to determine the optimal employment of security methods for any given investment level. Cyber Security Game maximizes a system’s ability to operate in today’s contested cyber environment by minimizing its mission risk. The risk score is calculated by using a mission impact model to compute the consequences of cyber incidents and combining that with the likelihood that attacks will succeed. The likelihood of attacks succeeding is computed by applying a threat model to a system topology model and defender model. Cyber Security Game takes into account the widespread interconnectedness of cyber systems, where defenders must defend all multi-step attack paths and an attacker only needs one to succeed. It employs a game theoretic solution using a game formulation that identifies defense strategies to minimize the maximum cyber risk (MiniMax). This paper discusses the methods and models that compose Cyber Security Game . A limited example of a Point of Sale system is used to provide specific demonstrations of Cyber Security Game models and analyses.},
	language = {en},
	number = {2},
	urldate = {2023-01-24},
	journal = {The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology},
	author = {Musman, Scott and Turner, Andrew},
	month = apr,
	year = {2018},
	keywords = {\_tablet},
	pages = {127--146},
	file = {Musman_Turner_2018_A game theoretic approach to cyber security risk management.pdf:/home/roland/Zotero/storage/KERQ9TTK/Musman_Turner_2018_A game theoretic approach to cyber security risk management.pdf:application/pdf},
}

@article{musman_computing_2011,
	title = {Computing the {Impact} of {Cyber} {Attacks} on {Complex} {Missions}},
	volume = {21},
	issn = {23345837},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2334-5837.2011.tb01255.x},
	doi = {10.1002/j.2334-5837.2011.tb01255.x},
	abstract = {This paper describes how to evaluate the impact of a cyber attack on a mission. We accomplish this by computing impact as the changes to mission measures of effectiveness, based on the reported effects of a known or suspected attack on one or more parts of the information technology (IT) supporting the mission. Our previous papers have described our goals for computing mission impact and the choices of the techniques we use for modeling missions, IT, and cyber attacks. This paper focuses on how we compute the impact of cyber attacks on IT processes and information. These computations will improve decision-making when under cyber attack by providing accurate and detailed assessments of the impact of those attacks. Although the focus of our work has been on the calculation of cyber mission impacts during mission execution, we have also demonstrated how our representations and computations can be used for performing cyber risk analysis and crown jewels analysis.},
	language = {en},
	number = {1},
	urldate = {2023-01-24},
	journal = {INCOSE International Symposium},
	author = {Musman, Scott and Tanner, Mike and Temin, Aaron and Elsaesser, Evan and Loren, Lewis},
	month = jun,
	year = {2011},
	keywords = {\_tablet},
	pages = {978--983},
	file = {Musman et al_2011_7.pdf:/home/roland/Zotero/storage/PPHY3KUY/Musman et al_2011_7.pdf:application/pdf},
}

@inproceedings{musman_cyber_2015,
	address = {Waltham, MA, USA},
	title = {A {Cyber} {Mission} {Impact} assessment tool},
	isbn = {978-1-4799-1737-2},
	url = {http://ieeexplore.ieee.org/document/7225283/},
	doi = {10.1109/THS.2015.7225283},
	abstract = {The promise of practicing mission assurance is to be able to leverage an understanding of how mission objectives and outcomes are dependent on supporting cyber resources. This makes it possible to analyze, monitor, and manage your cyber resources in a mission context. In previous work, we demonstrated how process modeling tools can simulate mission systems to allow us to dynamically compute the mission impacts of cyber events. We demonstrated the value of using this approach, but unfortunately practical deployment of our work was hampered by limitations of existing commercial off-the-shelf (COTS) tools for process modeling. To address this deficiency, we have developed our own Cyber Mission Impact Business Process Modeling tool. Although it implements only a functional subset of the business process modeling notation (BPMN), it has, unlike the more generic COTS tools, been specifically designed for the representation of cyber processes, resources, and cyber incident effects. The method and tool are described in this paper.},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {2015 {IEEE} {International} {Symposium} on {Technologies} for {Homeland} {Security} ({HST})},
	publisher = {IEEE},
	author = {Musman, Scott and Temin, Aaron},
	month = apr,
	year = {2015},
	keywords = {\_tablet},
	pages = {1--7},
	file = {Musman_Temin_2015_A Cyber Mission Impact assessment tool.pdf:/home/roland/Zotero/storage/9QQTJ3K3/Musman_Temin_2015_A Cyber Mission Impact assessment tool.pdf:application/pdf},
}

@misc{noauthor_documents_nodate,
	title = {Documents intéressants ({DGA}) - roland.donat@edgemind.net - {Messagerie} {EdgeMind}},
	url = {https://mail.google.com/mail/u/0/#inbox/FMfcgzGrcPQPQDPvdsTdRmDzBSCBrbSJ?projector=1&messagePartId=0.1},
	urldate = {2023-02-03},
	file = {Documents intéressants (DGA) - roland.donat@edgemind.net - Messagerie EdgeMind:/home/roland/Zotero/storage/GBSV5YYB/0.html:text/html},
}

@techreport{brun_guide_2022,
	title = {Guide d’application relatif à la cybersécurité pour les {STRA}},
	institution = {Service Technique des Remontées Mécaniques et des Transports Guidés},
	author = {Brun, François},
	month = dec,
	year = {2022},
	keywords = {\_tablet},
	file = {Guide d’application relatif à la cybersécurité pour les STRA.pdf:/home/roland/Zotero/storage/Y2FHKWMM/Guide d’application relatif à la cybersécurité pour les STRA.pdf:application/pdf},
}

@article{bornette_methodologie_nodate,
	title = {Méthodologie d’analyse de risques et sécurité globale},
	abstract = {As stated in a NATO report ([3]), current risk analysis methods are not ideally suited to cover complex systems. From our experience in Information Technology systems risk management for the French Ministry of Defence, we have developed a new approach which, from our point of view, seems applicable to global security.},
	language = {fr},
	author = {Bornette, Eric and Lebee, Jean-Pierre},
	keywords = {\_tablet},
	file = {Bornette_Lebee_Méthodologie d’analyse de risques et sécurité globale.pdf:/home/roland/Zotero/storage/LTISW2MP/Bornette_Lebee_Méthodologie d’analyse de risques et sécurité globale.pdf:application/pdf},
}

@techreport{bornette_etude_2001,
	title = {Etude, développement et évaluation d’emploi d’un modèle dynamique de système d’information à partir de technologie multi-agents dans le cadre des études menées par le {Centre} de l’{Armement} pour la {Sécurité} des {Systèmes} d’{Information} ({CASSI})},
	language = {fr},
	institution = {CNAM},
	author = {Bornette, Eric},
	year = {2001},
	keywords = {\_tablet},
	file = {Bornette_2001_Etude, développement et évaluation d’emploi d’un modèle dynamique de système.pdf:/home/roland/Zotero/storage/SEYR99K3/Bornette_2001_Etude, développement et évaluation d’emploi d’un modèle dynamique de système.pdf:application/pdf},
}

@article{bornette_methodologie_nodate-1,
	title = {Méthodologie d’analyse de risque basée sur le point de vue de l’attaquant.},
	abstract = {Résumé – Les méthodes classiques d’analyse de risque sont mal adaptées pour traiter des systèmes complexes. Il existe de nombreux travaux portant sur des méthodes alternatives qui utilisent les arbres d’attaque pour tenter d’évaluer le risque qui pèse sur ces systèmes. A partir d’un nouveau processus d’analyse de la menace informatique, nous avons développé une approche qui repose sur une analyse du point de vue de l’attaquant. Cette approche ne sépare pas les vulnérabilités et les menaces, elle propose de construire des scénarios basés sur des objectifs opérationnels choisis par l’attaquant. Les scénarios sont construits par un enchaînement d’actions qui se composent d’un couple (vulnérabilité, menace) indissociable et s’appuient sur une représentation du système analysé. Le coût des scénarios est comparé aux gains envisagés afin d’effectuer un classement des risques.},
	language = {fr},
	author = {Bornette, Eric and Lebee, Jean-Pierre and Eymery, Didier},
	keywords = {\_tablet},
	file = {Bornette et al_Méthodologie d’analyse de risque basée sur le point de vue de l’attaquant.pdf:/home/roland/Zotero/storage/XEVGQM6A/Bornette et al_Méthodologie d’analyse de risque basée sur le point de vue de l’attaquant.pdf:application/pdf},
}

@techreport{seyadou_consigne_2016,
	type = {{CSF}},
	title = {{CONSIGNE} {DE} {SECURITE} {FERROVIAIRE} - {LIGNE} 1 - {ALARMES} {MAJEURES} - {Mode} {Opératoire} {Incidents}},
	number = {CSF AM – 01 – 3},
	institution = {RATP, MTS},
	author = {SEYADOU},
	year = {2016},
	file = {SEYADOU - 2016 - CONSIGNE DE SECURITE FERROVIAIRE - LIGNE 1 - ALARM.pdf:/home/roland/Zotero/storage/RGP5UANE/SEYADOU - 2016 - CONSIGNE DE SECURITE FERROVIAIRE - LIGNE 1 - ALARM.pdf:application/pdf},
}

@techreport{halle_specification_2021,
	title = {Spécification {Technique} {Système} - {SAET} {L4}},
	language = {fr},
	number = {RC-FR\_MO\_MM\_GOA34- COC\_AS/PARISL4/129.0122.16/JLH/JLH},
	institution = {RATP/SIEMENS},
	author = {Hallé, JL and Guide, D and Pons, N},
	month = mar,
	year = {2021},
	file = {Hallé et al. - 2021 - Spécification Technique Système - SAET L4.pdf:/home/roland/Zotero/storage/Q7W45TYZ/Hallé et al. - 2021 - Spécification Technique Système - SAET L4.pdf:application/pdf},
}

@techreport{dolle_sequences_2018,
	title = {Séquences exploitation/maintenance - modes dégradés majeurs du {SAET} {L4}},
	number = {RC-FR MO MM PE RATP/PARISL4/164.0075.18/DDO/DDO},
	author = {Dollé, Daniel},
	month = jun,
	year = {2018},
	file = {Dollé - 2018 - Séquences exploitationmaintenance - modes dégradé.pdf:/home/roland/Zotero/storage/KDUMM9BN/Dollé - 2018 - Séquences exploitationmaintenance - modes dégradé.pdf:application/pdf},
}

@techreport{halle_saet_2016,
	title = {{SAET} - {DICTIONNAIRE} {DES} {TERMES}},
	number = {RC-FR MO MM PE RATP/PARISL4/164.0021.16/VFO/VFO},
	institution = {RATP/SIEMENS},
	author = {HALLE, J.-L.},
	month = mar,
	year = {2016},
	file = {HALLE - 2016 - SAET - DICTIONNAIRE DES TERMES.pdf:/home/roland/Zotero/storage/D383FCQD/HALLE - 2016 - SAET - DICTIONNAIRE DES TERMES.pdf:application/pdf},
}

@inproceedings{donat_demarche_2022,
	address = {Paris Saclay},
	title = {Démarche d'analyse {MBSA} dynamique pour la réalisation d'études {FMD} sur une ligne de transport ferroviaire complète},
	abstract = {This article describes the application of a specific MBSA approach dedicated to rail transportation systems. Our methodology consists in modelling the probabilistic dynamic behavior of technical and environmental systems involved in the Global Operating System of a metro line. The objective is to build high-level models close to both architecture and behavior of the target systems, so as to ease their interpretability and maintainability. Our approach meets the new RAMS requirements generated by the increasing complexity of railway systems. From a technical point of view, our solution relies on the Altarica framework to write and simulate systems models. The hierarchical and modular properties of the Altarica 3.0 language ensure model scalability and synchronization with the physical system life cycle.},
	language = {fr},
	author = {Donat, Roland and Legendre, Anthony and Perez, Loïc},
	month = oct,
	year = {2022},
	file = {Donat et al. - Démarche d'analyse MBSA dynamique pour la réalisat.pdf:/home/roland/Zotero/storage/39CSLAGE/Donat et al. - Démarche d'analyse MBSA dynamique pour la réalisat.pdf:application/pdf},
}

@article{dag_tree_2023,
	title = {A {Tree} {Augmented} {Naïve} {Bayes}-based methodology for classifying cryptocurrency trends},
	volume = {156},
	issn = {01482963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0148296322009870},
	doi = {10.1016/j.jbusres.2022.113522},
	abstract = {As the popularity of blockchain technology and investor confidence in Bitcoin (BTC) increased in recent years, many individuals started making BTC and other cryptocurrency investments, in expectation of high returns. However, as recent market movements have shown, the lack of regulation and oversight makes it difficult to guard against high volatility and potentially significant losses in this sector. In this study, we propose a datadriven Tree Augmented Naïve (TAN) Bayes methodology that can be used for identifying the most important factors (as well as their conditional, interdependent relationships) influencing BTC price movements. As the model is parsimonious without sacrificing accuracy, sensitivity, and specificity—as evident from the average accuracy value—the proposed methodology can be used in practice for making short-term investment decisions.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Journal of Business Research},
	author = {Dag, Ali and Dag, Asli Z. and Asilkalkan, Abdullah and Simsek, Serhat and Delen, Dursun},
	month = feb,
	year = {2023},
	keywords = {\_tablet},
	pages = {113522},
	file = {Dag et al_2023_A Tree Augmented Naïve Bayes-based methodology for classifying cryptocurrency.pdf:/home/roland/Work/Biblio/zotfiles/Dag et al_2023_A Tree Augmented Naïve Bayes-based methodology for classifying cryptocurrency.pdf:application/pdf},
}

@article{huang_predicting_2019-1,
	title = {Predicting bitcoin returns using high-dimensional technical indicators},
	volume = {5},
	issn = {24059188},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405918818300928},
	doi = {10.1016/j.jfds.2018.10.001},
	abstract = {There has been much debate about whether returns on ﬁnancial assets, such as stock returns or commodity returns, are predictable; however, few studies have investigated cryptocurrency return predictability. In this article we examine whether bitcoin returns are predictable by a large set of bitcoin price-based technical indicators. Speciﬁcally, we construct a classiﬁcation tree-based model for return prediction using 124 technical indicators. We provide evidence that the proposed model has strong out-of-sample predictive power for narrow ranges of daily returns on bitcoin. This ﬁnding indicates that using big data and technical analysis can help predict bitcoin returns that are hardly driven by fundamentals.},
	language = {en},
	number = {3},
	urldate = {2023-03-13},
	journal = {The Journal of Finance and Data Science},
	author = {Huang, Jing-Zhi and Huang, William and Ni, Jun},
	month = sep,
	year = {2019},
	keywords = {\_tablet},
	pages = {140--155},
	file = {Huang et al_2019_Predicting bitcoin returns using high-dimensional technical indicators.pdf:/home/roland/Zotero/storage/MVC4LH5K/Huang et al_2019_Predicting bitcoin returns using high-dimensional technical indicators.pdf:application/pdf},
}

@article{liu_are_2019,
	title = {Are {Bitcon} returns predictable?: {Evidence} from technical indicators},
	volume = {533},
	issn = {03784371},
	shorttitle = {Are {Bitcon} returns predictable?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437119311380},
	doi = {10.1016/j.physa.2019.121950},
	abstract = {We examine the predictive ability of technical indicators for excess returns to Bitcoin prices. Our out-of-sample evidence suggests the existence of significant return predictability. Combining all technical information results in out-of-sample R2 as high as 0.523\%. The dynamic strategy based on the return forecasts from combining technical information achieves the CER gains greater than 130\%.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Liu, Li},
	month = nov,
	year = {2019},
	keywords = {\_tablet},
	pages = {121950},
	file = {Liu_2019_Are Bitcon returns predictable.pdf:/home/roland/Zotero/storage/T5WYMZ6H/Liu_2019_Are Bitcon returns predictable.pdf:application/pdf},
}

@article{basher_forecasting_2022,
	title = {Forecasting {Bitcoin} price direction with random forests: {How} important are interest rates, inflation, and market volatility?},
	volume = {9},
	issn = {26668270},
	shorttitle = {Forecasting {Bitcoin} price direction with random forests},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S266682702200055X},
	doi = {10.1016/j.mlwa.2022.100355},
	abstract = {Bitcoin has grown in popularity and has now attracted the attention of individual and institutional investors. Accurate Bitcoin price direction forecasts are important for determining the trend in Bitcoin prices and asset allocation. This paper addresses several unanswered questions. How important are business cycle variables like interest rates, inflation, and market volatility for forecasting Bitcoin prices? Does the importance of these variables change across time? Are the most important macroeconomic variables for forecasting Bitcoin prices the same as those for gold prices? To answer these questions, we utilize tree-based machine learning classifiers, along with traditional logit econometric models. The analysis reveals several important findings. First, random forests predict Bitcoin and gold price directions with a higher degree of accuracy than logit models. Prediction accuracy for bagging and random forests is between 75\% and 80\% for a five-day prediction. For 10-day to 20-day forecasts bagging and random forests record accuracies greater than 85\%. Second, technical indicators are the most important features for predicting Bitcoin and gold price direction, suggesting some degree of market inefficiency. Third, oil price volatility is important for predicting Bitcoin and gold prices indicating that Bitcoin is a substitute for gold in diversifying this type of volatility. By comparison, gold prices are more influenced by inflation than Bitcoin prices, indicating that gold can be used as a hedge or diversification asset against inflation.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Machine Learning with Applications},
	author = {Basher, Syed Abul and Sadorsky, Perry},
	month = sep,
	year = {2022},
	keywords = {\_tablet},
	pages = {100355},
	file = {Basher_Sadorsky_2022_Forecasting Bitcoin price direction with random forests.pdf:/home/roland/Zotero/storage/SZLU3SHV/Basher_Sadorsky_2022_Forecasting Bitcoin price direction with random forests.pdf:application/pdf},
}

@article{nagula_new_2022,
	title = {A new hybrid machine learning model for predicting the bitcoin ({BTC}-{USD}) price},
	volume = {36},
	issn = {22146350},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214635022000673},
	doi = {10.1016/j.jbef.2022.100741},
	abstract = {Several machine learning techniques and hybrid architectures for predicting bitcoin price movement have been presented in the past. Our paper proposes a hybrid model encompassing classification and regression models for predicting bitcoin prices. Our analysis found that the automated feature interactions learner (deep cross networks) error performance using a plethora of technical indicators, including crypto-specific technical indicator difficulty ribbon compression and control variables such as Metcalfe’s value of bitcoin, number of unique active addresses, bitcoin network hash rate, and S\&P 500 log returns, in a hybrid architecture is better than the single-stage architecture. The hybrid model predicted a 100\% directional hit rate and maintained steady volatility in returns for the out-of-sample period. Our paper concludes that in terms of risk (Sharpe ratio 1.03) and profitability (260\% and 82\%), the hybrid model’s bitcoin futures strategy performed better than the deep cross network regression and buy-and-hold benchmark strategies.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Journal of Behavioral and Experimental Finance},
	author = {Nagula, Pavan Kumar and Alexakis, Christos},
	month = dec,
	year = {2022},
	keywords = {\_tablet},
	pages = {100741},
	file = {Nagula_Alexakis_2022_A new hybrid machine learning model for predicting the bitcoin (BTC-USD) price.pdf:/home/roland/Zotero/storage/SQHDMEE7/Nagula_Alexakis_2022_A new hybrid machine learning model for predicting the bitcoin (BTC-USD) price.pdf:application/pdf},
}

@article{cavalli_cnn-based_2021,
	title = {{CNN}-based multivariate data analysis for bitcoin trend prediction},
	volume = {101},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494620310036},
	doi = {10.1016/j.asoc.2020.107065},
	abstract = {Bitcoin is the most widely known blockchain, a distributed ledger that records an increasing number of transactions based on the bitcoin cryptocurrency. New bitcoins are created at a predictable and decreasing rate, which means that the demand must follow this level of inflation to keep the price stable. Actually, the price is highly volatile, because it is affected by many factors including the supply of bitcoin, its market demand, the cost of the mining process, as well as economic and political world-class news.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Applied Soft Computing},
	author = {Cavalli, Stefano and Amoretti, Michele},
	month = mar,
	year = {2021},
	keywords = {\_tablet},
	pages = {107065},
	file = {Cavalli_Amoretti_2021_CNN-based multivariate data analysis for bitcoin trend prediction.pdf:/home/roland/Zotero/storage/9BAZCM5M/Cavalli_Amoretti_2021_CNN-based multivariate data analysis for bitcoin trend prediction.pdf:application/pdf},
}

@article{ibrahim_predicting_2021,
	title = {Predicting market movement direction for bitcoin: {A} comparison of time series modeling methods},
	volume = {89},
	issn = {00457906},
	shorttitle = {Predicting market movement direction for bitcoin},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790620307576},
	doi = {10.1016/j.compeleceng.2020.106905},
	abstract = {Many traders participate in activities known as "day-trading", trading Bitcoin against the dollar bill as the United States Dollar (USD) on very short timeframes to squeeze out profits from small market fluctuations. This paper aims to help traders decide how to best act by creating a model that can predict price movement’s direction for the next 5-min time frame. Several machinelearning models have been tested for this Up/Down binary-classification problem. In this paper, we provide a comparison of the state-of-art strategies in predicting the movement direction for bitcoin, including Random Guessing and a Momentum-Based Strategy. The tested models include Autoregressive Integrated Moving Average (ARIMA), Prophet (by Facebook), Random Forest, Random Forest Lagged-Auto-Regression, and Multi-Layer Perceptron (MLP) Neural Net­ works. The MLP deep neural network has achieved the highest accuracy of 54\% compared to other time-series prediction models. Also, in this paper, various data transformation and feature engineering have been applied in the comparison.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Computers \& Electrical Engineering},
	author = {Ibrahim, Ahmed and Kashef, Rasha and Corrigan, Liam},
	month = jan,
	year = {2021},
	keywords = {\_tablet},
	pages = {106905},
	file = {Ibrahim et al_2021_Predicting market movement direction for bitcoin.pdf:/home/roland/Zotero/storage/Q6R99V2I/Ibrahim et al_2021_Predicting market movement direction for bitcoin.pdf:application/pdf},
}

@article{mallqui_predicting_2019,
	title = {Predicting the direction, maximum, minimum and closing prices of daily {Bitcoin} exchange rate using machine learning techniques},
	volume = {75},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494618306707},
	doi = {10.1016/j.asoc.2018.11.038},
	abstract = {Bitcoin is the most accepted cryptocurrency in the world, which makes it attractive for investors and traders. However, the challenge in predicting the Bitcoin exchange rate is its high volatility. Therefore, the prediction of its behavior is of great importance for financial markets. In this way, recent studies have been carried out on what internal and/or external Bitcoin information is relevant to its prediction. The increased use of machine learning techniques to predict time series and the acceptance of cryptocurrencies as financial instruments motivated the present study to seek more accurate predictions for the Bitcoin exchange rate. In this way, in a first stage of the proposed methodology, different feature selection techniques were evaluated in order to obtain the most relevant attributes for the predictions. In the sequence, it was analyzed the behavior of Artificial Neural Networks (ANN), Support Vector Machines (SVM) and Ensemble algorithms (based on Recurrent Neural Networks and the k-Means clustering method) for price direction predictions. Likewise, the ANN and SVM were employed for regression of the maximum, minimum and closing prices of the Bitcoin. Moreover, the regression results were also used as inputs to try to improve the price direction predictions. The results showed that the selected attributes and the best machine learning model achieved an improvement of more than 10\%, in accuracy, for the price direction predictions, with respect to the state-of-the-art papers, using the same period of information. In relation to the maximum, minimum and closing Bitcoin prices regressions, it was possible to obtain Mean Absolute Percentage Errors between 1\% and 2\%. Based on these results, it was possible to demonstrate the efficacy of the proposed methodology when compared to other studies.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Applied Soft Computing},
	author = {Mallqui, Dennys C.A. and Fernandes, Ricardo A.S.},
	month = feb,
	year = {2019},
	keywords = {\_tablet},
	pages = {596--606},
	file = {Mallqui_Fernandes_2019_Predicting the direction, maximum, minimum and closing prices of daily Bitcoin.pdf:/home/roland/Zotero/storage/F8U9TB2B/Mallqui_Fernandes_2019_Predicting the direction, maximum, minimum and closing prices of daily Bitcoin.pdf:application/pdf},
}

@article{gerritsen_profitability_2020,
	title = {The profitability of technical trading rules in the {Bitcoin} market},
	volume = {34},
	issn = {15446123},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1544612319303770},
	doi = {10.1016/j.frl.2019.08.011},
	abstract = {We apply seven trend-following indicators to assess the profitability of technical trading rules in the Bitcoin market. Using daily price data from July 2010 to January 2019, our main results show that specific technical analysis trading rules, mainly trading range breakout, contain significant forecasting power for Bitcoin prices, allowing the outperformance of the buy-and-hold strategy through the Sharpe ratio computed via the bootstrapping method. Results from various subperiods, representing normal and boom markets, generally confirm our main finding and show that the added value of the trading range breakout rule delivers outperformance in strongly trending markets.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Finance Research Letters},
	author = {Gerritsen, Dirk F. and Bouri, Elie and Ramezanifar, Ehsan and Roubaud, David},
	month = may,
	year = {2020},
	keywords = {\_tablet},
	pages = {101263},
	file = {Gerritsen et al_2020_The profitability of technical trading rules in the Bitcoin market.pdf:/home/roland/Zotero/storage/USTW2J5N/Gerritsen et al_2020_The profitability of technical trading rules in the Bitcoin market.pdf:application/pdf},
}

@inproceedings{cinar_digital_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Management} and {Industrial} {Engineering}},
	title = {Digital {Twins} for {Industry} 4.0: {A} {Review}},
	isbn = {978-3-030-42416-9},
	shorttitle = {Digital {Twins} for {Industry} 4.0},
	doi = {10.1007/978-3-030-42416-9_18},
	abstract = {Digital twins (DT) are the key enablers for transformation to Industry 4.0 (I4.0), they are required and indispensable to the virtual design and optimization of smart manufacturing systems for I4.0. Recently many researchers have contributed to the development of Digital Twins for smart products processes and manufacturing systems. This paper presents a systematic literature review of recent developments in Digital Twins in I4.0 for Smart manufacturing, by examining the most researches related to DT, and classifying the existing publications according to the applications in various aspects of manufacturing i.e. product design, process design, manufacturing process (such as machining, cutting), additive manufacturing, 3D printing, plant layout design, production planning, ergonomics, maintenance and product lifecycle. This paper classifies, identifies, and analyzes the research on Digital Twin application to smart manufacturing systems for I4.0.},
	language = {en},
	booktitle = {Industrial {Engineering} in the {Digital} {Disruption} {Era}},
	publisher = {Springer International Publishing},
	author = {Cinar, Zeki Murat and Nuhu, Abubakar Abdussalam and Zeeshan, Qasim and Korhan, Orhan},
	editor = {Calisir, Fethi and Korhan, Orhan},
	year = {2020},
	keywords = {Digital Twin, Additive manufacturing, Big Data, Cyber-Physical System, Industrial Internet of Things, Industry 4.0, Smart Manufacturing},
	pages = {193--203},
}

@inproceedings{zhang_degradation-modeling_2016,
	title = {A degradation-modeling based prognostic approach for systems with switching operating process},
	doi = {10.1109/PHM.2016.7819815},
	abstract = {Degradation-modeling based prognostic approach has been proved as an effective alternative to the conventional lifetime-data dependent residual life prediction method, and thus draw much attention of both scholars and engineers in the field of reliability. The degradation process of a system is the result of interaction between its inner states and working environments. To provide a reasonable reference for the sequential decision making based on prognostic result, the influence of operation process has to been incorporated into degradation modeling and prognosis. Therefore, this paper concerns the residual life prediction issue for system experiencing switching operation process whose influence on the system's performance degradation includes both deterioration and shocks. Besides the fact that the concerned system exhibits different deteriorating rates in each operation state, the change of operation states introduces external stresses and causes mutation in performance of the system. Therefore, the operation process depicted through a continuous time Markov chain (CTMC) is incorporated into the system's degradation modeling, based on which the system's residual life distribution is derived approximately yet explicitly after it is defined under the concept of first hitting time (FHT). Such a residual lifetime distribution is quite desired in prognostics and health management, especially for cases where online updating is required. The proposed approach is illustrated and validated by a numerical study.},
	booktitle = {2016 {Prognostics} and {System} {Health} {Management} {Conference} ({PHM}-{Chengdu})},
	author = {Zhang, Zheng-Xin and Hu, Chang-Hua and Si, Xiao-Sheng and Zhou, Shao-Hua},
	month = oct,
	year = {2016},
	note = {ISSN: 2166-5656},
	keywords = {Electric shock, Reliability, Markov processes, Continuous time Markov Chain, Degradation, Degradation modeling, Prognostics and health management, Prognostics and Health Management, Residual Life Prediction, Switches},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/roland/Zotero/storage/DXGPUT59/7819815.html:text/html},
}

@article{daigle_model-based_2009,
	title = {Model-based {Prognostics} with {Fixed}-lag {Particle} {Filters}},
	volume = {1},
	copyright = {Copyright (c) 2009 Matthew Daigle , Kai Goebel},
	issn = {2325-0178},
	url = {http://papers.phmsociety.org/index.php/phmconf/article/view/1598},
	abstract = {Model-based prognostics exploits domain knowledge of the system, its components, and how they fail by casting the underlying physical phenomena in a physics-based model that is derived from first principles. In most applications, uncertainties from a number of sources cause the predictions to be inaccurate and imprecise even with accurate models. Therefore, algorithms are employed that help in managing these uncertainties. Particle filters have become a popular choice to solve this problem due to their wide applicability and ease of implementation. We present a general model-based prognostics methodology using particle filters. In order to provide more accurate and precise estimates, and, therefore, more accurate and precise predictions, we investigate the use of fixed-lag filters. We develop a detailed physics-based model of a pneumatic valve, and perform comprehensive simulation experiments to illustrate our prognostics approach. The experiments demonstrate the advantages that fixed-lag filters may provide in the context of prognostics, as measured by prognostics performance metrics.},
	language = {en},
	number = {1},
	urldate = {2023-04-08},
	journal = {Annual Conference of the PHM Society},
	author = {Daigle, Matthew and Goebel, Kai},
	year = {2009},
	note = {Number: 1},
	keywords = {applications: space},
	file = {Daigle_Goebel_2009_Model-based Prognostics with Fixed-lag Particle Filters.pdf:/home/roland/Work/Biblio/zotfiles/Daigle_Goebel_2009_Model-based Prognostics with Fixed-lag Particle Filters.pdf:application/pdf},
}

@incollection{celaya_discussion_nodate,
	title = {A {Discussion} on {Uncertainty} {Representation} and {Interpretation} in {Model}-based {Prognostics} {Algorithms} based on {Kalman} {Filter} {Estimation} {Applied} to {Prognostics} of {Electronics} {Components}},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2012-2422},
	urldate = {2023-04-08},
	booktitle = {Infotech@{Aerospace} 2012},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Celaya, Jose and Saxena, Abhinav and Goebel, Kai},
	doi = {10.2514/6.2012-2422},
	note = {\_eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.2012-2422},
	file = {AIAA Snapshot:/home/roland/Zotero/storage/8WCQH93C/6.html:text/html;Celaya et al_A Discussion on Uncertainty Representation and Interpretation in Model-based.pdf:/home/roland/Work/Biblio/zotfiles/Celaya et al_A Discussion on Uncertainty Representation and Interpretation in Model-based.pdf:application/pdf},
}

@article{breslow_analysis_1975,
	title = {Analysis of {Survival} {Data} under the {Proportional} {Hazards} {Model}},
	volume = {43},
	issn = {0306-7734},
	url = {https://www.jstor.org/stable/1402659},
	doi = {10.2307/1402659},
	abstract = {Methodology is reviewed for the statistical analysis of censored survival data which arise from a model in which the factors under investigation act multiplicatively on the hazard function of an underlying non-parametric survival distribution. This flexible approach provides computationally feasible solutions to the following problems: (i) one-sample problem (relative death rate); (ii) multi-sample problem; (iii) regression with continuous covariates; (iv) regression in matched-pair designs; (v) evaluation of changes in treatment or prognostic status (time dependent covariates). For the multi-sample problem with stratification, numerical results are presented contrasting maximum likelihood with simple chi-square analyses. While several of the methods described have been used on an ad hoc basis for many years, study of their common theoretical underpinnings has commenced only recently. /// Cet article présente une revue des méthodes utilisées pour l'analyse statistique d'observations censurées de durée de vie, qui résultent d'un modèle où les facteurs étudiés agissent multiplicativement sur la fonction de hasard d'une distribution non paramétrique sous-jacente du nombre de survivants. Cette approche flexible fournit des solutions aisément calculables aux problèmes suivants: (1) problème portant sur un seul échantillon (relatif au taux de décès); (2) problème à plusieurs échantillons; (3) régression à variables continues; (4) régression dans les plans avec observation par paire; (5) évaluation de la modification du traitement ou du pronostic (covariables dépendant du temps). Dans le problème à plusieurs échantillons avec stratification, des exemples numériques sont présentés, qui confrontent le maximum de vraisemblance aux méthodes plus simples d'analyse de khi-carré. Bien que plusieurs des méthodes décrites aient été utilisées sur une base convenable pendant de nombreuses années, une étude de leur rattachement théorique commun n'a commencé que récemment.},
	number = {1},
	urldate = {2023-04-08},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Breslow, N. E.},
	year = {1975},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {45--57},
}

@book{cox_analysis_1984,
	address = {New York.},
	title = {Analysis of survival data},
	publisher = {Chapman and Hall},
	author = {Cox, D. R. and Oakes, D.},
	year = {1984},
}

@article{saikia_review_nodate,
	title = {A {Review} on {Accelerated} {Failure} {Time} {Models}},
	abstract = {Survival analysis is the analysis of statistical data in which the outcome variable of interest is time until an event occurs. In Statistical literature, it is observed that a good number of models have been developed for analyzing survival data or life time data .The most popular among them is the Cox Proportional Hazard (PH) model. Accelerated Failure Time (AFT) model, which is mainly used to study the reliability of industrial products can also be considered as a good alternative of Cox PH model in analyzing survival data. In this paper, the attempt has been made to present a review on Accelerated Failure Time models. Here the historical developments, technical developments and past research on AFT models are discussed.},
	language = {en},
	author = {Saikia, Rinku and Barman, Manash Pratim},
	file = {Saikia et Barman - A Review on Accelerated Failure Time Models.pdf:/home/roland/Zotero/storage/TNDFRVQ3/Saikia et Barman - A Review on Accelerated Failure Time Models.pdf:application/pdf},
}

@article{friedman_piecewise_1982,
	title = {Piecewise {Exponential} {Models} for {Survival} {Data} with {Covariates}},
	volume = {10},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-10/issue-1/Piecewise-Exponential-Models-for-Survival-Data-with-Covariates/10.1214/aos/1176345693.full},
	doi = {10.1214/aos/1176345693},
	abstract = {A general class of models for analysis of censored survival data with covariates is considered. If \$n\$ individuals are observed over a time period divided into \$I(n)\$ intervals, it is assumed that \${\textbackslash}lambda\_j(t)\$, the hazard rate function of the time to failure of the individual \$j\$, is constant and equal to \${\textbackslash}lambda\_\{ij\} {\textgreater} 0\$ on the \$i\$th interval, and that the vector \${\textbackslash}ell = {\textbackslash}\{{\textbackslash}log {\textbackslash}lambda\_\{ij\}: j = 1, {\textbackslash}ldots, n; i = 1, {\textbackslash}ldots, I(n){\textbackslash}\}\$ lies in a linear subspace. The maximum likelihood estimate \${\textbackslash}hat\{{\textbackslash}ell\}\$ of \${\textbackslash}ell\$ provides a simultaneous estimate of the underlying hazard rate function, and of the effects of the covariates. Maximum likelihood equations and conditions for existence of \${\textbackslash}hat\{{\textbackslash}ell\}\$ are given. The asymptotic properties of linear functionals of \${\textbackslash}hat\{{\textbackslash}ell\}\$ are studied in the general case where the true hazard rate function \${\textbackslash}lambda\_0(t)\$ is not a step function, and \$I(n)\$ increases without bound as the maximum interval length decreases. In comparison with recent work on regression analysis of survival data, the asymptotic results are obtained under more relaxed conditions on the regression variables.},
	number = {1},
	urldate = {2023-04-08},
	journal = {The Annals of Statistics},
	author = {Friedman, Michael},
	month = mar,
	year = {1982},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E20, 62F10, Asymptotic theory, Censored data, Log-linear model, maximum likelihood estimation, piecewise exponential model, survival data},
	pages = {101--113},
	file = {Friedman_1982_Piecewise Exponential Models for Survival Data with Covariates.pdf:/home/roland/Work/Biblio/zotfiles/Friedman_1982_Piecewise Exponential Models for Survival Data with Covariates.pdf:application/pdf},
}

@inproceedings{qian_multiple_2014,
	address = {Cham},
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Multiple {Change}-{Point} {Detection} in {Piecewise} {Exponential} {Hazard} {Regression} {Models} with {Long}-{Term} {Survivors} and {Right} {Censoring}},
	isbn = {978-3-319-02651-0},
	doi = {10.1007/978-3-319-02651-0_18},
	abstract = {Change-point detection in hazard rates is an important research topic in survival analysis. In this chapter, we first review the existing methods for a single change-point detection in piecewise exponential hazard models. Then, we propose a new change-point detection algorithm in multiple change-point hazard regression models for fitting failure times that allows the existence of both susceptibles and long-term survivors. For right censored failure time data, the proposed algorithm combines the Kaplan–Meier estimator for the susceptible proportion and weighted least square estimators for the multiple change-points and other model parameters. A simulation study is conducted for various model parameter settings. The results show that the proposed algorithm works superiorly on detecting the number of change-points with almost ignorable misclassification rate and on estimating other model parameters even for small to moderate sample sizes. Last, the proposed method is used to analyze clinical data on breast cancer.},
	language = {en},
	booktitle = {Contemporary {Developments} in {Statistical} {Theory}},
	publisher = {Springer International Publishing},
	author = {Qian, Lianfen and Zhang, Wei},
	editor = {Lahiri, Soumendra and Schick, Anton and SenGupta, Ashis and Sriram, T.N.},
	year = {2014},
	keywords = {Cumulative Hazard Function, Failure Time, Hazard Function, Hazard Rate, Meier Estimator},
	pages = {289--304},
}

@article{mercier_probabilistic_nodate,
	title = {Probabilistic construction and properties of {Gamma} processes and extensions},
	language = {en},
	author = {Mercier, Sophie},
	file = {Mercier_Probabilistic construction and properties of Gamma processes and extensions.pdf:/home/roland/Work/Biblio/zotfiles/Mercier_Probabilistic construction and properties of Gamma processes and extensions.pdf:application/pdf},
}

@article{aalen_understanding_2001,
	title = {Understanding the shape of the hazard rate: a process point of view ({With} comments and a rejoinder by the authors)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Understanding the shape of the hazard rate},
	url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-1/Understanding-the-shape-of-the-hazard-rate--a-process/10.1214/ss/998929473.full},
	doi = {10.1214/ss/998929473},
	abstract = {Survival analysis as used in the medical context is focused on the concepts of survival function and hazard rate, the latter of these being the basis both for the Cox regression model and of the counting process approach. In spite of apparent simplicity, hazard rate is really an elusive concept, especially when one tries to interpret its shape considered as a function of time. It is then helpful to consider the hazard rate from a different point of view than what is common, and we will here consider survival times modeled as first passage times in stochastic processes. The concept of quasistationary distribution,which is a well-defined entity for various Markov processes, will turn out to be useful. We study these matters for a number of Markov processes, including the following: finite Markov chains; birth-death processes; Wiener processes with and without randomization of parameters; and general diffusion processes. An example of regression of survival data with a mixed inverse Gaussian distribution is presented. The idea of viewing survival times as first passage times has been much studied by Whitmore and others in the context of Wiener processes and inverse Gaussian distributions. These ideas have been in the background compared to more popular appoaches to survival data, at least within the field of biostatistics,but deserve more attention.},
	number = {1},
	urldate = {2023-04-08},
	journal = {Statistical Science},
	author = {Aalen, Odd O. and Gjessing, Håkon K.},
	month = feb,
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Survival analysis, First passage time, hazard rate, Markov chain, quasistationary distribution, Wiener process},
	pages = {1--22},
	file = {Aalen_Gjessing_2001_Understanding the shape of the hazard rate.pdf:/home/roland/Work/Biblio/zotfiles/Aalen_Gjessing_2001_Understanding the shape of the hazard rate.pdf:application/pdf},
}

@phdthesis{nguyen_contribution_2015,
	type = {These de doctorat},
	title = {Contribution aux approches probabilistes pour le pronostic et la maintenance des systèmes contrôlés},
	copyright = {Licence Etalab},
	url = {https://www.theses.fr/2015TROY0010},
	abstract = {Les systèmes de contrôle-commande jouent un rôle important dans le développement de la civilisation et de la technologie moderne. La perte d’efficacité de l’actionneur agissant sur le système est nocive dans le sens où elle modifie le comportement du système par rapport à celui qui est désiré. Cette thèse est une contribution au pronostic de la durée de vie résiduelle (RUL) et à la maintenance des systèmes de contrôle-commande en boucle fermée avec des actionneurs soumis à dégradation. Dans une première contribution, un cadre de modélisation à l'aide d’un processus markovien déterministe par morceaux est considéré pour modéliser le comportement du système. Dans ce cadre, le comportement du système est représenté par des trajectoires déterministes qui sont intersectées par des sauts d'amplitude aléatoire se produisant à des instants aléatoires et modélisant le phénomène de dégradation discret de l'actionneur. La deuxième contribution est une méthode de pronostic de la RUL du système composée de deux étapes : estimation de la loi de probabilité de l'état du système à l'instant de pronostic par le filtre particulaire et calcul de la RUL qui nécessite l'estimation de la fiabilité du système à partir de cet instant. La troisième contribution correspond à la proposition  d’une politique de maintenance à structure paramétrique permettant de prendre en compte dynamiquement les informations disponibles conjointement sur l'état et sur l'environnement courant du système et sous la contrainte de dates d'opportunité},
	urldate = {2023-04-08},
	school = {Troyes},
	author = {Nguyen, Danh Ngoc},
	collaborator = {Dieulle, Laurence and Grall, Antoine},
	month = mar,
	year = {2015},
	keywords = {Reliability, Fiabilité, 629.8, Automatic control, Commande automatique, Durée de vie (ingénierie), Entretien -- Modèles mathématiques, Maintenance -- Mathematical models, Markov, Processus de, Marlov processes, Monte-Carlo method, Monte-Carlo, Méthode de, Prediction theory, Prévision, Théorie de la, Service life (Engineering)},
}

@article{davis_piecewise-deterministic_1984-1,
	title = {Piecewise-{Deterministic} {Markov} {Processes}: {A} {General} {Class} of {Non}-{Diffusion} {Stochastic} {Models}},
	volume = {46},
	issn = {2517-6161},
	shorttitle = {Piecewise-{Deterministic} {Markov} {Processes}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1984.tb01308.x},
	doi = {10.1111/j.2517-6161.1984.tb01308.x},
	abstract = {A general class of non-diffusion stochastic models is introduced with a view to providing a framework for studying optimization problems arising in queueing systems, inventory theory, resource allocation and other areas. The corresponding stochastic processes are Markov processes consisting of a mixture of deterministic motion and random jumps. Stochastic calculus for these processes is developed and a complete characterization of the extended generator is given; this is the main technical result of the paper. The relevance of the extended generator concept in applied problems is discussed and some recent results on optimal control of piecewise-deterministic processes are described.},
	language = {en},
	number = {3},
	urldate = {2023-04-08},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Davis, M. H. A.},
	year = {1984},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1984.tb01308.x},
	keywords = {capacity expansion, dynamic programming, extended generator, markov process, martingale, queueing theory, stochastic control theory, stochastic models},
	pages = {353--376},
	file = {Davis_1984_Piecewise-Deterministic Markov Processes.pdf:/home/roland/Work/Biblio/zotfiles/Davis_1984_Piecewise-Deterministic Markov Processes.pdf:application/pdf;Snapshot:/home/roland/Zotero/storage/NRW2GLDB/j.2517-6161.1984.tb01308.html:text/html},
}

@phdthesis{lair_modelisation_2011,
	type = {phdthesis},
	title = {Modélisation dynamique de systèmes complexes pour le calcul de grandeurs fiabilistes et l'optimisation de la maintenance},
	url = {https://theses.hal.science/tel-00643981},
	abstract = {L'objectif de cette thèse est de proposer une méthode permettant d'optimiser la stratégie de maintenance d'un système multi-composants. Cette nouvelle stratégie doit être adaptée aux conditions d'utilisation et aux contraintes budgétaires et sécuritaires. Le vieillissement des composants et la complexité des stratégies de maintenance étudiées nous obligent à avoir recours à de nouveaux modèles probabilistes afin de répondre à la problématique. Nous utilisons un processus stochastique issu de la Fiabilité Dynamique nommé processus markovien déterministe par morceaux (Piecewise Deterministic Markov Process ou PDMP). L'évaluation des quantités d'intérêt (fiabilité, nombre moyen de pannes...) est ici réalisée à l'aide d'un algorithme déterministe de type volumes finis. L'utilisation de ce type d'algorithme, dans ce cadre d'application, présente des difficultés informatiques dues à la place mémoire. Nous proposons plusieurs méthodes pour repousser ces difficultés. L'optimisation d'un plan de maintenance est ensuite effectuée à l'aide d'un algorithme de recuit simulé. Cette méthodologie a été adaptée à deux systèmes ferroviaires utilisés par la SNCF, l'un issu de l'infrastructure, l'autre du matériel roulant.},
	language = {fr},
	urldate = {2023-04-08},
	school = {Université de Pau et des Pays de l'Adour},
	author = {Lair, William},
	month = nov,
	year = {2011},
	file = {Lair_2011_Modélisation dynamique de systèmes complexes pour le calcul de grandeurs.pdf:/home/roland/Work/Biblio/zotfiles/Lair_2011_Modélisation dynamique de systèmes complexes pour le calcul de grandeurs.pdf:application/pdf},
}

@phdthesis{lorton_contribution_2012,
	type = {These de doctorat},
	title = {Contribution aux approches hybrides pour le pronostic à l'aide de processus de {Markov} déterministes par morceaux},
	copyright = {Licence Etalab},
	url = {https://www.theses.fr/2012TROY0022},
	abstract = {Nous proposons une approche fiabiliste du pronostic pour la maintenance conditionnelle. Le pronostic consiste à calculer la durée de vie résiduelle (RUL), durée restante avant un événement redouté (défaillance, perte de performance, etc). Le contexte industriel de cette thèse (système complexe, incertitudes) justifie une approche probabiliste basée sur les modèles. Ce travail répond à trois problématiques : faire le lien entre la fiabilité et le pronostic, choisir un cadre de modélisation intégrant des modèles physiques, et proposer une méthode de calcul de la RUL intégrant les données issues de la vie du bien. Nous répondons à la première question en formalisant le problème de calcul de la RUL. Le pronostic apparaît comme une fiabilité conditionnelle et décalée dans le temps. Nous considérons une modélisation par processus de Markov déterministes par morceaux, permettant un large spectre de représentation, conformément à la deuxième question. Nous proposons deux nouvelles méthodes de calcul de fiabilité par Monte-Carlo sur ces modèles. Enfin, nous présentons une méthode de calcul de RUL sur un processus de Markov, contenant deux étapes : un calcul de loi conditionnelle et un calcul de fiabilité. Nous proposons une méthode particulaire pour approximer la première étape. Nous étudions la convergence de l'approximation successive des deux étapes : on montre une convergence presque sure, et un théorème central-limite associé. L'ensemble est illustré sur plusieurs exemples, avec notamment une application de la RUL dans la prise de décision de maintenance},
	urldate = {2023-04-08},
	school = {Troyes},
	author = {Lorton, Ariane},
	collaborator = {Cocozza-Thivent, Christiane and Grall, Antoine},
	month = jan,
	year = {2012},
	keywords = {Fiabilité, Markov, Processus de, Monte-Carlo, Méthode de, Prévision, Théorie de la, Filtres (mathématiques)},
}

@phdthesis{ghamlouch_modelisation_2016,
	type = {These de doctorat},
	title = {Modélisation de la dégradation, maintenance conditionnelle et pronostic : usage des processus de diffusion},
	copyright = {Licence Etalab},
	shorttitle = {Modélisation de la dégradation, maintenance conditionnelle et pronostic},
	url = {https://www.theses.fr/2016TROY0019},
	abstract = {Aujourd’hui la prédiction des défaillances de certains systèmes industriels est devenue indispensable pour l’amélioration de la fiabilité et de la rentabilité de ces derniers. Cette prédiction s’appuie principalement sur l’analyse d’évolution du niveau de dégradation du système. Pour les systèmes dont l’état de détérioration n’est pas directement observable, la définition d’indicateurs de santé mesurables est nécessaire. Une modélisation du processus de dégradation à partir de ces données peut être ensuite effectuée. Dans cette thèse, nous considérons un ensemble d’indicateurs non-monotones pour un système opérant dans un environnement dynamique. Compte tenu des principales caractéristiques des données ainsi que de l’impact des conditions environnementales et de leur instabilité, une modélisation stochastique de l’évolution de ces indicateurs est proposée. Les modèles proposés se basent principalement sur une combinaison d’un processus de Wiener et de processus de sauts. Les motivations, les méthodes de calibration, l’utilité et les limites de chaque modèle sont discutées. Nous proposons ensuite une approche pour l’aide à la décision concernant les actions de maintenance préventive. Cette approche consiste à évaluer la valeur d’une option réelle qui présente la possibilité d’«Attendre avant d’Agir» suite à un signal d’avertissement sur une défaillance probable. Une application de cette approche pour le cas d'une éolienne équipée d’un système de surveillance et de gestion est traitée},
	urldate = {2023-04-08},
	school = {Troyes},
	author = {Ghamlouch, Houda},
	collaborator = {Fouladirad, Mitra and Grall, Antoine},
	month = jun,
	year = {2016},
	keywords = {Decision making, 620.004 5, Brownian mouvements, Condition-based Maintenance, Diffusion processes, Levy processes, Lévy, Processus de, Maintenance conditionnelle, Mouvement brownien, Poisson processes, Poisson, Processus de, Prise de décision, Processus de diffusion, Processus stochastiques, Stochastic processes},
}

@article{faraggi_neural_1995,
	title = {A neural network model for survival data},
	volume = {14},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780140108},
	doi = {10.1002/sim.4780140108},
	abstract = {Neural networks have received considerable attention recently, mostly by non-statisticians. They are considered by many to be very promising tools for classification and prediction. In this paper we present an approach to modelling censored survival data using the input—output relationship associated with a simple feed-forward neural network as the basis for a non-linear proportional hazards model. This approach can be extended to other models used with censored survival data. The proportional hazards neural network parameters are estimated using the method of maximum likelihood. These maximum likelihood based models can be compared, using readily available techniques such as the likelihood ratio test and the Akaike criterion. The neural network models are illustrated using data on the survival of men with prostatic carcinoma. A method of interpreting the neural network predictions based on the factorial contrasts is presented.},
	language = {en},
	number = {1},
	urldate = {2023-04-08},
	journal = {Statistics in Medicine},
	author = {Faraggi, David and Simon, Richard},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780140108},
	pages = {73--82},
	file = {Snapshot:/home/roland/Zotero/storage/WH2LPUAY/sim.html:text/html},
}

@article{katzman_deepsurv_2018,
	title = {{DeepSurv}: personalized treatment recommender system using a {Cox} proportional hazards deep neural network},
	volume = {18},
	issn = {1471-2288},
	shorttitle = {{DeepSurv}},
	url = {https://doi.org/10.1186/s12874-018-0482-1},
	doi = {10.1186/s12874-018-0482-1},
	abstract = {Medical practitioners use survival models to explore and understand the relationships between patients’ covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems.},
	number = {1},
	urldate = {2023-04-08},
	journal = {BMC Medical Research Methodology},
	author = {Katzman, Jared L. and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
	month = feb,
	year = {2018},
	keywords = {Survival analysis, Deep learning, Treatment recommendations},
	pages = {24},
	file = {Katzman et al_2018_DeepSurv.pdf:/home/roland/Work/Biblio/zotfiles/Katzman et al_2018_DeepSurv.pdf:application/pdf;Snapshot:/home/roland/Zotero/storage/PEIBG9DP/s12874-018-0482-1.html:text/html},
}

@misc{luck_deep_2017,
	title = {Deep {Learning} for {Patient}-{Specific} {Kidney} {Graft} {Survival} {Analysis}},
	url = {http://arxiv.org/abs/1705.10245},
	doi = {10.48550/arXiv.1705.10245},
	abstract = {An accurate model of patient-specific kidney graft survival distributions can help to improve shared-decision making in the treatment and care of patients. In this paper, we propose a deep learning method that directly models the survival function instead of estimating the hazard function to predict survival times for graft patients based on the principle of multi-task learning. By learning to jointly predict the time of the event, and its rank in the cox partial log likelihood framework, our deep learning approach outperforms, in terms of survival time prediction quality and concordance index, other common methods for survival analysis, including the Cox Proportional Hazards model and a network trained on the cox partial log-likelihood.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Luck, Margaux and Sylvain, Tristan and Cardinal, Héloïse and Lodi, Andrea and Bengio, Yoshua},
	month = may,
	year = {2017},
	note = {arXiv:1705.10245 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/roland/Zotero/storage/LMZGEKM7/1705.html:text/html;Luck et al_2017_Deep Learning for Patient-Specific Kidney Graft Survival Analysis.pdf:/home/roland/Work/Biblio/zotfiles/Luck et al_2017_Deep Learning for Patient-Specific Kidney Graft Survival Analysis.pdf:application/pdf},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2023-04-08},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
	file = {Ishwaran et al_2008_Random survival forests.pdf:/home/roland/Work/Biblio/zotfiles/Ishwaran et al_2008_Random survival forests.pdf:application/pdf},
}

@inproceedings{ranganath_deep_2016,
	title = {Deep {Survival} {Analysis}},
	url = {https://proceedings.mlr.press/v56/Ranganath16.html},
	abstract = {The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we introduce deep survival analysis, a hierarchical generative  approach to survival analysis in the context of the EHR. It departs from previous approaches in two main ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it handles heterogeneous data types that occur in the EHR. We validate deep survival analysis by stratifying patients according to risk of developing coronary heart disease (CHD) on 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is superior in stratifying patients according to their risk.},
	language = {en},
	urldate = {2023-04-08},
	booktitle = {Proceedings of the 1st {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Ranganath, Rajesh and Perotte, Adler and Elhadad, Noémie and Blei, David},
	month = dec,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {101--114},
	file = {Ranganath et al_2016_Deep Survival Analysis.pdf:/home/roland/Work/Biblio/zotfiles/Ranganath et al_2016_Deep Survival Analysis.pdf:application/pdf},
}

@inproceedings{fernandez_gaussian_2016,
	title = {Gaussian {Processes} for {Survival} {Analysis}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/ef1e491a766ce3127556063d49bc2f98-Abstract.html},
	abstract = {We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as  dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.},
	urldate = {2023-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fernandez, Tamara and Rivera, Nicolas and Teh, Yee Whye},
	year = {2016},
	file = {Fernandez et al_2016_Gaussian Processes for Survival Analysis.pdf:/home/roland/Work/Biblio/zotfiles/Fernandez et al_2016_Gaussian Processes for Survival Analysis.pdf:application/pdf},
}

@inproceedings{noauthor_deep_2017,
	title = {Deep {Multi}-task {Gaussian} {Processes} for {Survival} {Analysis} with {Competing} {Risks}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Abstract.html},
	abstract = {Designing optimal treatment plans for patients with comorbidities requires accurate cause-specific mortality prognosis. Motivated by the recent availability of linked electronic health records, we develop a nonparametric Bayesian model for survival analysis with competing risks, which can be used for jointly assessing a patient's risk of multiple (competing) adverse outcomes. The model views a patient's survival times with respect to the competing risks as the outputs of a deep multi-task Gaussian process (DMGP), the inputs to which are the patients' covariates. Unlike parametric survival analysis methods based on Cox and Weibull models, our model uses DMGPs to capture complex non-linear interactions between the patients' covariates and cause-specific survival times, thereby learning flexible patient-specific and cause-specific survival curves, all in a data-driven fashion without explicit parametric assumptions on the hazard rates. We propose a variational inference algorithm that is capable of learning the model parameters from time-to-event data while handling right censoring. Experiments on synthetic and real data show that our model outperforms the state-of-the-art survival models.},
	urldate = {2023-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	year = {2017},
	file = {2017_Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks.pdf:/home/roland/Work/Biblio/zotfiles/2017_Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks.pdf:application/pdf},
}

@article{lee_deephit_2018,
	title = {{DeepHit}: {A} {Deep} {Learning} {Approach} to {Survival} {Analysis} {With} {Competing} {Risks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{DeepHit}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11842},
	doi = {10.1609/aaai.v32i1.11842},
	abstract = {Survival analysis (time-to-event analysis) is widely used in economics and finance, engineering, medicine and many other areas. A fundamental problem is to understand the relationship between the covariates and the (distribution of) survival times(times-to-event). Much of the previous work has approached the problem by viewing the survival time as the first hitting time of a stochastic process, assuming a specific form for the underlying stochastic process, using available data to learn the relationship between the covariates and the parameters of the model, and then deducing the relationship between covariates and the distribution of first hitting times (the risk). However, previous models rely on strong parametric assumptions that are often violated. This paper proposes a very different approach to survival analysis, DeepHit, that uses a deep neural network to learn the distribution of survival times directly.DeepHit makes no assumptions about the underlying stochastic process and allows for the possibility that the relationship between covariates and risk(s) changes over time. Most importantly, DeepHit smoothly handles competing risks; i.e. settings in which there is more than one possible event of interest.Comparisons with previous models on the basis of real and synthetic datasets demonstrate that DeepHit achieves large and statistically significant performance improvements over previous state-of-the-art methods.},
	language = {en},
	number = {1},
	urldate = {2023-04-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lee, Changhee and Zame, William and Yoon, Jinsung and Schaar, Mihaela van der},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {first-hitting-time analysis},
	file = {Lee et al_2018_DeepHit.pdf:/home/roland/Work/Biblio/zotfiles/Lee et al_2018_DeepHit.pdf:application/pdf},
}

@misc{adhikari_introductory_2013,
	title = {An {Introductory} {Study} on {Time} {Series} {Modeling} and {Forecasting}},
	url = {http://arxiv.org/abs/1302.6613},
	doi = {10.48550/arXiv.1302.6613},
	abstract = {Time series modeling and forecasting has fundamental importance to various practical domains. Thus a lot of active research works is going on in this subject during several years. Many important models have been proposed in literature for improving the accuracy and effectiveness of time series forecasting. The aim of this dissertation work is to present a concise description of some popular time series forecasting models used in practice, with their salient features. In this thesis, we have described three important classes of time series models, viz. the stochastic, neural networks and SVM based models, together with their inherent forecasting strengths and weaknesses. We have also discussed about the basic issues related to time series modeling, such as stationarity, parsimony, overfitting, etc. Our discussion about different time series models is supported by giving the experimental forecast results, performed on six real time series datasets. While fitting a model to a dataset, special care is taken to select the most parsimonious one. To evaluate forecast accuracy as well as to compare among different models fitted to a time series, we have used the five performance measures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of the six datasets, we have shown the obtained forecast diagram which graphically depicts the closeness between the original and forecasted observations. To have authenticity as well as clarity in our discussion about time series modeling and forecasting, we have taken the help of various published research works from reputed journals and some standard books.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Adhikari, Ratnadip and Agrawal, R. K.},
	month = feb,
	year = {2013},
	note = {arXiv:1302.6613 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 68T01},
	file = {Adhikari_Agrawal_2013_An Introductory Study on Time Series Modeling and Forecasting.pdf:/home/roland/Work/Biblio/zotfiles/Adhikari_Agrawal_2013_An Introductory Study on Time Series Modeling and Forecasting.pdf:application/pdf;arXiv.org Snapshot:/home/roland/Zotero/storage/ADCMG4TQ/1302.html:text/html},
}

@article{tsui_prognostics_2015,
	title = {Prognostics and {Health} {Management}: {A} {Review} on {Data} {Driven} {Approaches}},
	volume = {2015},
	issn = {1024-123X},
	shorttitle = {Prognostics and {Health} {Management}},
	url = {https://www.hindawi.com/journals/mpe/2015/793161/},
	doi = {10.1155/2015/793161},
	abstract = {Prognostics and health management (PHM) is a framework that offers comprehensive yet individualized solutions for managing system health. In recent years, PHM has emerged as an essential approach for achieving competitive advantages in the global market by improving reliability, maintainability, safety, and affordability. Concepts and components in PHM have been developed separately in many areas such as mechanical engineering, electrical engineering, and statistical science, under varied names. In this paper, we provide a concise review of mainstream methods in major aspects of the PHM framework, including the updated research from both statistical science and engineering, with a focus on data-driven approaches. Real world examples have been provided to illustrate the implementation of PHM in practice.},
	language = {en},
	urldate = {2023-04-08},
	journal = {Mathematical Problems in Engineering},
	author = {Tsui, Kwok L. and Chen, Nan and Zhou, Qiang and Hai, Yizhen and Wang, Wenbin},
	month = may,
	year = {2015},
	note = {Publisher: Hindawi},
	pages = {e793161},
	file = {Tsui et al_2015_Prognostics and Health Management.pdf:/home/roland/Work/Biblio/zotfiles/Tsui et al_2015_Prognostics and Health Management.pdf:application/pdf},
}

@inproceedings{donat_reliability_2008-1,
	title = {Reliability {Analysis} using {Graphical} {Duration} {Models}},
	doi = {10.1109/ARES.2008.25},
	abstract = {Reliability analysis has become an integral part of system design and operating. This is especially true for systems performing critical tasks such as mass transportation systems. This explains the numerous advances in the field of reliability modelling. More recently, some studies involving the use of Probabilistic Graphical Models (PGMs), a.k.a. Bayesian Networks (BNs), have been proved relevant to represent complex systems and perform reliability studies. This paper aims to describe a Dynamic PGM (DPGM) designed to model stochastic degradation processes, allowing any kind of state sojourn distributions along with an accurate context description. We meet these objectives using a specific DPGM, namely a Graphical Duration Model (GDM). In this article, we give qualitative and quantitative descriptions of the proposed model and describe how to compute the reliability of the underlying system and some of its classic related metrics. Finally, we illustrate our approach by applying a GDM in order to perform the survival analysis of railway track supposed to be subjected to one context variable.},
	booktitle = {2008 {Third} {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Donat, Roland and Bouillaut, Laurent and Aknin, Patrice and Leray, Philippe},
	month = mar,
	year = {2008},
	keywords = {Availability, Bayesian methods, Graphical models, Reliability analysis, Degradation, Stochastic processes, Computer security, Context modeling, Graphical Duration Models, Laboratories, Probabilistic Graphical Models, Rail transportation, System analysis and design},
	pages = {795--800},
	file = {IEEE Xplore Abstract Record:/home/roland/Zotero/storage/9ZIK527J/4529425.html:text/html},
}

@article{bartram_probabilistic_2015,
	title = {Probabilistic {Prognosis} with {Dynamic} {Bayesian} {Networks}},
	volume = {6},
	copyright = {Copyright (c) 2015 Gregory Bartram, Sankaran Mahadevan},
	issn = {2153-2648},
	url = {http://papers.phmsociety.org/index.php/ijphm/article/view/2290},
	doi = {10.36001/ijphm.2015.v6i4.2290},
	abstract = {This paper proposes a methodology for probabilistic prognosis of a system using a dynamic Bayesian network (DBN). Dynamic Bayesian networks are suitable for probabilistic prognosis because of their ability to integrate information in a variety of formats from various sources and give a probabilistic representation of the system state. Further, DBNs provide a platform naturally suited for seamless integration of diagnosis, uncertainty quantification, and prediction. In the proposed methodology, a DBN is used for online diagnosis via particle filtering, providing a current estimate of the joint distribution over the system variables. The information available in the state estimate also helps to quantify the uncertainty in diagnosis. Next, based on this probabilistic state estimate, future states of the system are predicted using the DBN and sequential or recursive Monte Carlo sampling. Prediction in this manner provides the necessary information to estimate the distribution of remaining use life (RUL). The prognosis procedure, which is system specific, is validated using a suite of offline hierarchical metrics. The prognosis methodology is demonstrated on a hydraulic actuator subject to a progressive seal wear that results in internal leakage between the chambers of the actuator.},
	language = {en},
	number = {4},
	urldate = {2023-04-08},
	journal = {International Journal of Prognostics and Health Management},
	author = {Bartram, Gregory and Mahadevan, Sankaran},
	year = {2015},
	note = {Number: 4},
	keywords = {Dynamic Bayesian Network},
	file = {Bartram_Mahadevan_2015_Probabilistic Prognosis with Dynamic Bayesian Networks.pdf:/home/roland/Work/Biblio/zotfiles/Bartram_Mahadevan_2015_Probabilistic Prognosis with Dynamic Bayesian Networks.pdf:application/pdf},
}

@article{hu_dbn_2017,
	title = {{DBN} based failure prognosis method considering the response of protective layers for the complex industrial systems},
	volume = {79},
	issn = {1350-6307},
	url = {https://www.sciencedirect.com/science/article/pii/S1350630716308299},
	doi = {10.1016/j.engfailanal.2017.04.015},
	abstract = {In complex industrial systems, operating, regulating, maintenance activities and external incidents take place dynamically and multiple entities in same or different subsystems interact in a complex manner. Most of single faults have multiple propagation paths. Any local slight deviation is able to propagate, spread, accumulate and increase through system fault causal chains. It will finally result in system failure and unplanned outages or even catastrophic accidents. The key issues focus on both of how to reduce the probability of fault occurrence and decrease the loss of fault consequence. The implementation of such requirements can be studied in terms of the determination of the fault root causes, prediction of the possible consequence, and also estimation of the risk and timing of various maintenance activities, which are considered in a failure prognosis scheme. This study proposes a DBN based failure prognosis method for complex system. Not only the interaction between components, but also the influence of the layers of protection in the system is considered when the dynamic failure scenarios are analyzed. Therefore the proposed method considers multiple factors including degradation mechanism, parameter deviation, the response of the layers of protection and also the external environment. With this model, the dynamic influence diagram of the components' degradation trends can be calculated and be used to evaluate the different effects of the layers of protection quantitatively. Some key problems are also explained such as how to determine the new nodes in DBN representing the behavior of protective layers and how to update the CPT in the extended model. In the case study, the proposed method is tested on the flue gas energy recovery system (FGERS) which is widely used in the petrochemical industry to demonstrate its effectiveness. It makes a great help for early warning and optimization of the layers of protection in the complex industrial system.},
	language = {en},
	urldate = {2023-04-08},
	journal = {Engineering Failure Analysis},
	author = {Hu, Jinqiu and Zhang, Laibin and Tian, Wenhui and Zhou, Shuai},
	month = sep,
	year = {2017},
	keywords = {Dynamic Bayesian network, Complex industrial system, Failure prognosis, Protective layers},
	pages = {504--519},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/4G9QN7U9/S1350630716308299.html:text/html},
}

@inproceedings{mcnaught_using_2009,
	title = {Using dynamic {Bayesian} networks for prognostic modelling to inform maintenance decision making},
	doi = {10.1109/IEEM.2009.5372973},
	abstract = {In this paper, we consider the application of dynamic Bayesian networks to the prognostic modelling of equipment in order to better inform maintenance decision-making. We provide a brief overview of Bayesian networks and their application to reliability modelling. An example is then provided in which an equipment is considered to be in one of six states and there are two imperfect condition monitoring indicators available to provide evidence about the equipment's true state which tends to deteriorate over time. With this example, we show how the equipment's reliability decays over time in the situation where repair is not possible and then how a simple change to the model allows us to represent different maintenance policies for repairable equipment.},
	booktitle = {2009 {IEEE} {International} {Conference} on {Industrial} {Engineering} and {Engineering} {Management}},
	author = {McNaught, K. R. and Zagorecki, A.},
	month = dec,
	year = {2009},
	note = {ISSN: 2157-362X},
	keywords = {reliability, Maintenance, Artificial intelligence, Bayesian methods, Fault trees, Decision making, Fault diagnosis, Graphical models, probabilistic graphical model, Probability distribution, Condition monitoring, Condition-based maintenance, Systems engineering and theory},
	pages = {1155--1159},
	file = {IEEE Xplore Abstract Record:/home/roland/Zotero/storage/L2V435X7/5372973.html:text/html},
}

@article{xu_machine_2021,
	title = {Machine learning for reliability engineering and safety applications: {Review} of current status and future opportunities},
	volume = {211},
	issn = {0951-8320},
	shorttitle = {Machine learning for reliability engineering and safety applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832021000892},
	doi = {10.1016/j.ress.2021.107530},
	abstract = {Machine learning (ML) pervades an increasing number of academic disciplines and industries. Its impact is profound, and several fields have been fundamentally altered by it, autonomy and computer vision for example; reliability engineering and safety will undoubtedly follow suit. There is already a large but fragmented literature on ML for reliability and safety applications, and it can be overwhelming to navigate and integrate into a coherent whole. In this work, we facilitate this task by providing a synthesis of, and a roadmap to this ever-expanding analytical landscape and highlighting its major landmarks and pathways. We first provide an overview of the different ML categories and sub-categories or tasks, and we note several of the corresponding models and algorithms. We then look back and review the use of ML in reliability and safety applications. We examine several publications in each category/sub-category, and we include a short discussion on the use of Deep Learning to highlight its growing popularity and distinctive advantages. Finally, we look ahead and outline several promising future opportunities for leveraging ML in service of advancing reliability and safety considerations. Overall, we argue that ML is capable of providing novel insights and opportunities to solve important challenges in reliability and safety applications. It is also capable of teasing out more accurate insights from accident datasets than with traditional analysis tools, and this in turn can lead to better informed decision-making and more effective accident prevention.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Reliability Engineering \& System Safety},
	author = {Xu, Zhaoyi and Saleh, Joseph Homer},
	month = jul,
	year = {2021},
	keywords = {Safety, Machine learning, Reliability, Deep learning, Prognostic and health management},
	pages = {107530},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/QCYIE9YR/S0951832021000892.html:text/html;Xu_Saleh_2021_Machine learning for reliability engineering and safety applications.pdf:/home/roland/Work/Biblio/zotfiles/Xu_Saleh_2021_Machine learning for reliability engineering and safety applications.pdf:application/pdf},
}

@article{carvalho_systematic_2019-1,
	title = {A systematic literature review of machine learning methods applied to predictive maintenance},
	volume = {137},
	issn = {0360-8352},
	url = {https://www.sciencedirect.com/science/article/pii/S0360835219304838},
	doi = {10.1016/j.cie.2019.106024},
	abstract = {The amount of data extracted from production processes has increased exponentially due to the proliferation of sensing technologies. When processed and analyzed, data can bring out valuable information and knowledge from manufacturing process, production system and equipment. In industries, equipment maintenance is an important key, and affects the operation time of equipment and its efficiency. Thus, equipment faults need to be identified and solved, avoiding shutdown in the production processes. Machine Learning (ML) methods have been emerged as a promising tool in Predictive Maintenance (PdM) applications to prevent failures in equipment that make up the production lines in the factory floor. However, the performance of PdM applications depends on the appropriate choice of the ML method. The aim of this paper is to present a systematic literature review of ML methods applied to PdM, showing which are being explored in this field and the performance of the current state-of-the-art ML techniques. This review focuses on two scientific databases and provides a useful foundation on the ML techniques, their main results, challenges and opportunities, as well as it supports new research works in the PdM field.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Computers \& Industrial Engineering},
	author = {Carvalho, Thyago P. and Soares, Fabrízzio A. A. M. N. and Vita, Roberto and Francisco, Roberto da P. and Basto, João P. and Alcalá, Symone G. S.},
	month = nov,
	year = {2019},
	keywords = {Artificial intelligence, Machine learning, PdM, Predictive maintenance, Systematic literature review},
	pages = {106024},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/F89RLQ8T/S0360835219304838.html:text/html},
}

@article{chang_hybrid_2019,
	title = {A hybrid prognostic method for system degradation based on particle filter and relevance vector machine},
	volume = {186},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832018309086},
	doi = {10.1016/j.ress.2019.02.011},
	abstract = {Prognostics of the remaining useful life has become a critical technique to ensure the reliability and safety of system, however, due to the uncertainty of system degradation, the prognostic result is usually not so satisfactory. To solve this problem, a hybrid prognostic scheme with the capability of uncertainty assessment is proposed in this paper, which combines particle filter (PF) and relevance vector machine (RVM). The prognostic result comprises a set of deterministic prediction values to represent the degradation process and a prediction interval to evaluate the prediction uncertainty. In order to examine the performance of the proposed hybrid method, four types of comparative experiments based on two types of lithium-ion battery datasets and two degradation models are performed. The experimental results show that the proposed hybrid scheme is a reliable prognostic method which can ensure the accuracy of the deterministic prediction result and provide precise assessment for the prediction uncertainty.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Reliability Engineering \& System Safety},
	author = {Chang, Yang and Fang, Huajing},
	month = jun,
	year = {2019},
	keywords = {Prognostics, Deterministic prediction, Lithium-ion battery, Particle filter, Prediction interval, Relevance vector machine},
	pages = {51--63},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/9972425W/S0951832018309086.html:text/html},
}

@article{chen_gated_2019,
	title = {Gated recurrent unit based recurrent neural network for remaining useful life prediction of nonlinear deterioration process},
	volume = {185},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S095183201830838X},
	doi = {10.1016/j.ress.2019.01.006},
	abstract = {Remaining useful life (RUL) prediction is a key process for prognostics and health management (PHM). However, conventional model-based methods and data-driven methods for RUL prediction are bad at a very complex system with multiple components, multiple states and therefore extremely large amount of parameters. In order to solve the problem, a general two-step solution is proposed in this paper. In the first step, kernel principle component analysis (KPCA) is applied for nonlinear feature extraction. Then, a novel recurrent neural network called gated recurrent unit (GRU) is presented as the second step to predict RUL. GRU network is capable of describing a very complex system because of its specially designed structure. The effectiveness of the proposed solution for RUL prediction of a nonlinear degradation process is proved by a case study of commercial modular aero-propulsion system simulation data (C-MAPSS-Data) from NASA. Results also show that the proposed method requires less training time and has better prediction accuracy than other data-driven methods.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Reliability Engineering \& System Safety},
	author = {Chen, Jinglong and Jing, Hongjie and Chang, Yuanhong and Liu, Qian},
	month = may,
	year = {2019},
	keywords = {RUL prediction, Nonlinear deterioration, PHM, Recurrent neural network},
	pages = {372--382},
	file = {ScienceDirect Snapshot:/home/roland/Zotero/storage/LTADHVUF/S095183201830838X.html:text/html},
}

@article{listou_ellefsen_remaining_2019,
	title = {Remaining useful life predictions for turbofan engine degradation using semi-supervised deep architecture},
	volume = {183},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832018307506},
	doi = {10.1016/j.ress.2018.11.027},
	abstract = {In recent years, research has proposed several deep learning (DL) approaches to providing reliable remaining useful life (RUL) predictions in Prognostics and Health Management (PHM) applications. Although supervised DL techniques, such as Convolutional Neural Network and Long-Short Term Memory, have outperformed traditional prognosis algorithms, they are still dependent on large labeled training datasets. With respect to real-life PHM applications, high-quality labeled training data might be both challenging and time-consuming to acquire. Alternatively, unsupervised DL techniques introduce an initial pre-training stage to extract degradation related features from raw unlabeled training data automatically. Thus, the combination of unsupervised and supervised (semi-supervised) learning has the potential to provide high RUL prediction accuracy even with reduced amounts of labeled training data. This paper investigates the effect of unsupervised pre-training in RUL predictions utilizing a semi-supervised setup. Additionally, a Genetic Algorithm (GA) approach is applied in order to tune the diverse amount of hyper-parameters in the training procedure. The advantages of the proposed semi-supervised setup have been verified on the popular C-MAPSS dataset. The experimental study, compares this approach to purely supervised training, both when the training data is completely labeled and when the labeled training data is reduced, and to the most robust results in the literature. The results suggest that unsupervised pre-training is a promising feature in RUL predictions subjected to multiple operating conditions and fault modes.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Reliability Engineering \& System Safety},
	author = {Listou Ellefsen, André and Bjørlykhaug, Emil and Æsøy, Vilmar and Ushakov, Sergey and Zhang, Houxiang},
	month = mar,
	year = {2019},
	keywords = {Prognostics and health management, Deep learning, C-MAPSS, Genetic algorithm, Remaining useful life, Semi-supervised learning},
	pages = {240--251},
	file = {Listou Ellefsen et al_2019_Remaining useful life predictions for turbofan engine degradation using.pdf:/home/roland/Work/Biblio/zotfiles/Listou Ellefsen et al_2019_Remaining useful life predictions for turbofan engine degradation using.pdf:application/pdf;ScienceDirect Snapshot:/home/roland/Zotero/storage/2M7RAZVT/S0951832018307506.html:text/html},
}

@article{fink_predicting_2014,
	title = {Predicting component reliability and level of degradation with complex-valued neural networks},
	volume = {121},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832013002391},
	doi = {10.1016/j.ress.2013.08.004},
	abstract = {In this paper, multilayer feedforward neural networks based on multi-valued neurons (MLMVN), a specific type of complex valued neural networks, are proposed to be applied to reliability and degradation prediction problems, formulated as time series. MLMVN have demonstrated their ability to extract complex dynamic patterns from time series data for mid- and long-term predictions in several applications and benchmark studies. To the authors' knowledge, it is the first time that MLMVN are applied for reliability and degradation prediction. MLMVN are applied to a case study of predicting the level of degradation of railway track turnouts using real data. The performance of the algorithms is first evaluated using benchmark study data. The results obtained in the reliability prediction study of the benchmark data show that MLMVN outperform other machine learning algorithms in terms of prediction precision and are also able to perform multi-step ahead predictions, as opposed to the previously best performing benchmark studies which only performed up to two-step ahead predictions. For the railway turnout application, MLMVN confirm the good performance in the long-term prediction of degradation and do not show accumulating errors for multi-step ahead predictions.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Reliability Engineering \& System Safety},
	author = {Fink, Olga and Zio, Enrico and Weidmann, Ulrich},
	month = jan,
	year = {2014},
	keywords = {Complex valued neural networks, Level of degradation, Neural networks, Railway turnout system, Reliability prediction},
	pages = {198--206},
	file = {Fink et al_2014_Predicting component reliability and level of degradation with complex-valued.pdf:/home/roland/Work/Biblio/zotfiles/Fink et al_2014_Predicting component reliability and level of degradation with complex-valued.pdf:application/pdf;ScienceDirect Snapshot:/home/roland/Zotero/storage/A5MKZS2J/S0951832013002391.html:text/html},
}

@inproceedings{el_mernissi_modelisation_2023,
	address = {Québec, Canada},
	title = {Modélisation comportementale d'un data center pour le développement d'un jumeau numérique},
	url = {https://hal.science/hal-04116913},
	abstract = {This article presents a new approach to building the digital twin of a data center based on behavioral modeling techniques inspired by the MBSA (Model-Based Safety Assessment) methodology. This approach is grounded in the development of a generic modeling library that describes the functional and dysfunctional behaviors of the elementary components involved in the data center's operations. Subsequently, this library is utilized to construct the digital twin of the data center. The twin is then employed to simulate the system, enabling the prediction of its future performance and identification of potential vulnerabilities. From a technical standpoint, the twin is developed using the PyCATSHOO modeling and simulation tool, which facilitates the calculation of various reliability and availability indicators, as well as the exploration of critical scenarios.},
	urldate = {2023-12-14},
	booktitle = {{CIGI} {Qualita} {MOSIM} 2023},
	author = {El Mernissi, Marouane and Donat, Roland and Hamzaoui, Mohammed and Julien, Nathalie},
	month = jun,
	year = {2023},
	keywords = {Predictive maintenance, data center, Data center, Digital twin, Jumeau numérique, Maintenance prédictive, Modeling, Modélisation},
	file = {El Mernissi et al_2023_Modélisation comportementale d'un data center pour le développement d'un jumeau.pdf:/home/roland/Work/Biblio/zotfiles/El Mernissi et al_2023_Modélisation comportementale d'un data center pour le développement d'un jumeau.pdf:application/pdf},
}

@inproceedings{legendre_interet_2020,
	address = {Le Havre (e-congrès), France},
	title = {Intérêt de l'outil {K6} 2.0 pour les études de {Sûreté} de {Fonctionnement} des réseaux électriques critiques},
	url = {https://hal.science/hal-03462767},
	abstract = {Cet article présente la nouvelle base de connaissances K6 2.0 développée pour la plate-forme KB3 qui est utilisée pour réaliser des études de sûreté de fonctionnement des réseaux électriques. Ce programme d’EDF R\&D développé en collaboration avec la société EdgeMind, est une version consolidée de la version initiale K6. Une étude d’un système critique de centrale nucléaire utilisant la nouvelle base de connaissance ainsi que les résultats pouvant être obtenus sur les indicateurs de fiabilité, de disponibilité et des séquences de défaillance sont présentés à la fin de cet article.},
	urldate = {2023-12-14},
	booktitle = {Congrès {Lambda} {Mu} 22 “ {Les} risques au cøeur des transitions ” (e-congrès) - 22e {Congrès} de {Maîtrise} des {Risques} et de {Sûreté} de {Fonctionnement}, {Institut} pour la {Maîtrise} des {Risques}},
	author = {Legendre, Anthony and Druet, Jules and Serru, Théo and Laguilliez, Romain and Donat, Roland},
	month = oct,
	year = {2020},
	keywords = {MBSA, Base de connaissances, Graphe de Markov, Réseaux électriques, Séquences de défaillance},
	file = {Legendre et al_2020_Intérêt de l'outil K6 2.pdf:/home/roland/Work/Biblio/zotfiles/Legendre et al_2020_Intérêt de l'outil K6 2.pdf:application/pdf},
}

@inproceedings{legendre_practical_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Practical {Application} of {Model}-{Based} {Safety} {Analysis} to the {Design} of {Global} {Operating} {System} of {New} {Rolling} {Stock} on {Automatic} {Metro} {Lines}},
	isbn = {978-3-031-15842-1},
	doi = {10.1007/978-3-031-15842-1_6},
	abstract = {System safety assessments are an integral part of system development, as indicated by EN 5012x railway standards. These activities are usually performed manually and rely on reviews and engineering judgments, with limited use of models to support the system assessment phase. In this paper, we present an application of Model-Based Safety Assessment to the Global Operating System (GOS) validation for automatic and semi-automatic metro lines. Safety assessment is a fundamental part of the development of railway systems and the use of model-based techniques provides an effective method for the formalization and analysis of such complex systems. A MBSA deployment methodology using AltaRica Wizard platform and its stochastic simulator is presented and results of the application of the automatic metro lines use-case are shown.},
	language = {en},
	booktitle = {Model-{Based} {Safety} and {Assessment}},
	publisher = {Springer International Publishing},
	author = {Legendre, Anthony and Donat, Roland},
	editor = {Seguin, Christel and Zeller, Marc and Prosvirnova, Tatiana},
	year = {2022},
	keywords = {Integration in interdisciplinary processes, Model based safety analysis, Global operating system, Railway applications, RAMS},
	pages = {68--82},
}

@inproceedings{niol_mise_2022,
	address = {Paris Saclay, France},
	title = {Mise à jour de l'état de l'art sur les méthodes et outils innovants pour le traitement des systèmes complexes et benchmarking},
	url = {https://hal.science/hal-03877902},
	abstract = {des pour traiter les systèmes complexes et faire un benchmark sur un cas d’usage. Le projet s’est intéressé aux jumeaux numériques selon une démarche MBSE/MBSA, à la théorie des réseaux complexes et aux modèles de systèmes vivants.
Le cas d’usage retenu porte sur une unité de production d’hydrogène. L’installation a fait l’objet d’un modèle MBSA en AltaRica avec l’outil SimfiaNeo. Ce modèle se veut unificateur en permettant l’étude des objectifs de production et de sûreté de fonctionnement, en intégrant les vulnérabilités de cybersécurité. Ce modèle a ensuite été transposé en réseaux pour en étudier des indicateurs de centralité.
Ceci a permis de calculer des indicateurs classiques mais surtout de s’interroger sur les apports d’autres disciplines, avec la nécessité d’approfondir celles-ci, en l’état actuel des connaissances.},
	urldate = {2023-12-14},
	booktitle = {Congrès {Lambda} {Mu} 23 “ {Innovations} et maîtrise des risques pour un avenir durable ” - 23e {Congrès} de {Maîtrise} des {Risques} et de {Sûreté} de {Fonctionnement}, {Institut} pour la {Maîtrise} des {Risques}},
	author = {Niol, Julien and Rifi, Mouna and Caire, Jean and Duval, Carole and Tarrisse, Albin and Hibti, Mohamed and Brissaud, Florent},
	month = oct,
	year = {2022},
	keywords = {jumeaux numériques, modélisation du vivant, réseaux de neurones, systèmes complexes, théorie des réseaux},
	file = {Niol et al_2022_Mise à jour de l'état de l'art sur les méthodes et outils innovants pour le.pdf:/home/roland/Work/Biblio/zotfiles/Niol et al_2022_Mise à jour de l'état de l'art sur les méthodes et outils innovants pour le.pdf:application/pdf},
}

@article{sun_comparison_2021,
	title = {Comparison of the {HAZOP}, {FMEA}, {FRAM}, and {STPA} {Methods} for the {Hazard} {Analysis} of {Automatic} {Emergency} {Brake} {Systems}},
	volume = {8},
	issn = {2332-9017},
	url = {https://doi.org/10.1115/1.4051940},
	doi = {10.1115/1.4051940},
	abstract = {As autonomous vehicle (AV) intelligence for controllability continues to develop, involving increasingly complex and interconnected systems, the maturity level of AV technology increasingly depends on the systems reliability level, also considering the interactions among them. Hazard analysis is typically used to identify potential system risks and avoid loss of AV system functionality. Conventional hazard analysis methods are commonly used for traditional standalone systems. New hazard analysis methods have been developed that may be more suitable for AV system-of-systems complexity. However, a comprehensive comparison of hazard analysis methods for AV systems is lacking. In this study, the traditional hazard analysis methods, hazard and operability (HAZOP) and failure mode and effects analysis (FMEA), as well as the most recent methods, like functional resonance analysis method (FRAM) and system-theoretic process analysis (STPA), are considered for implementation in the automatic emergency braking system. This system is designed to avoid collisions by utilizing the surrounding sensors to detect objects on the road, warning drivers with alerts about any collision risk, and actuating automatic partial/full braking through calculated adaptive braking deceleration. The objective of this work is to evaluate the methods with the unified theory of acceptance and use of technology (UTAUT) approach, in terms of their applicability to AV technologies. The advantages of HAZOP, FMEA, FRAM, and STPA, as well as the possibility of combining them to achieve systematic risk identification in practice, are discussed.},
	number = {031104},
	urldate = {2023-12-18},
	journal = {ASCE-ASME J Risk and Uncert in Engrg Sys Part B Mech Engrg},
	author = {Sun, Liangliang and Li, Yan-Fu and Zio, Enrico},
	month = oct,
	year = {2021},
	file = {Snapshot:/home/roland/Zotero/storage/VS89XYK7/Comparison-of-the-HAZOP-FMEA-FRAM-and-STPA-Methods.html:text/html},
}

@book{leveson_stpa_2018,
	title = {{STPA} {Handbook}},
	url = {https://psas.scripts.mit.edu/home/get_file.php?name=STPA_handbook.pdf},
	abstract = {This handbook is intended for those interested in using STPA on real systems. It is not meant to introduce the theoretical foundation, which is described elsewhere. Here our goal is to provide direction for those starting out with STPA on a real project or to supplement other materials in a class teaching STPA.},
	language = {en},
	author = {Leveson, Nancy and Thomas, John P},
	month = mar,
	year = {2018},
	file = {Leveson - This handbook is intended for those interested in .pdf:/home/roland/Zotero/storage/QNCL77JV/Leveson - This handbook is intended for those interested in .pdf:application/pdf},
}

@phdthesis{rifi_modelisation_2019,
	type = {phdthesis},
	title = {Modélisation et {Analyse} des {Réseaux} {Complexes} : application à la sûreté nucléaire},
	shorttitle = {Modélisation et {Analyse} des {Réseaux} {Complexes}},
	url = {https://theses.hal.science/tel-03169804},
	abstract = {Ce travail propose une modélisation adéquate en graphes pour les systèmes et séquences accidentelles de sûreté nucléaire. Ces systèmes et séquences proviennent des "Etudes Probabilistes de Sûreté" (EPS) qui consistent à analyser de façon exhaustive tous les scénarios accidentels envisageables, d’estimer leurs probabilités d’occurrence (en les regroupant par famille) et les conséquences associées.Ensuite une analyse des réseaux complexes résultants est effectuée par des mesures de centralités.Une première application consiste à la prédiction du Facteur d’Accroissement du Risque nucléaire en utilisant les algorithmes d’apprentissages supervisé : méthodes à base d’arbre de classification, régression logistique et méthodes ensemblistes, sur des données déséquilibrées.Par ailleurs, un nouveau coefficient synthétique de centralité et une mesure de similarité sont conçus pour comparer les structures de réseaux, indépendamment de leurs caractéristiques topologiques, en se basant sur les interdépendances entre leurs vecteurs de centralités.Cette nouvelle approche utilise des techniques statistiques (échantillonnage, corrélation ethomogénéité).La pertinence et l’efficacité de cette nouvelle mesure de similarité sont validées sur le clustering de graphes théoriques classiques et la prédiction du type de graphes. Enfin, une application de cette approche est réalisée pour le clustering des réseaux complexes des systèmes de sûreté nucléaire.},
	language = {fr},
	urldate = {2023-12-20},
	school = {Université Sorbonne Paris Cité},
	author = {Rifi, Mouna},
	month = may,
	year = {2019},
	file = {Rifi_2019_Modélisation et Analyse des Réseaux Complexes.pdf:/home/roland/Work/Biblio/zotfiles/Rifi_2019_Modélisation et Analyse des Réseaux Complexes.pdf:application/pdf},
}

@article{_project_2013,
	title = {{PROJECT} {MANAGEMENT} {MATHEMATICAL} {MODELS} {FOR} {THE} {CUSTOMER}},
	journal = {PM World Journal},
	author = {Гельруд, Я.Д},
	month = mar,
	year = {2013},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-02-11},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
	file = {McCulloch_Pitts_1943_A logical calculus of the ideas immanent in nervous activity.pdf:/home/roland/Work/Biblio/zotfiles/McCulloch_Pitts_1943_A logical calculus of the ideas immanent in nervous activity.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {1939-1471},
	shorttitle = {The perceptron},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Brain, Cognition, Memory, Nervous System},
	pages = {386--408},
	file = {Snapshot:/home/roland/Zotero/storage/CQ6C5HRS/1959-09865-001.html:text/html},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-02-11},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {533--536},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/roland/Zotero/storage/F47U6FEA/1810.html:text/html;Devlin et al_2019_BERT.pdf:/home/roland/Work/Biblio/zotfiles/Devlin et al_2019_BERT.pdf:application/pdf},
}

@misc{budzianowski_hello_2019,
	title = {Hello, {It}'s {GPT}-2 -- {How} {Can} {I} {Help} {You}? {Towards} the {Use} of {Pretrained} {Language} {Models} for {Task}-{Oriented} {Dialogue} {Systems}},
	shorttitle = {Hello, {It}'s {GPT}-2 -- {How} {Can} {I} {Help} {You}?},
	url = {http://arxiv.org/abs/1907.05774},
	doi = {10.48550/arXiv.1907.05774},
	abstract = {Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework (Wolf et al., 2019) and generative model pre-training (Radford et al., 2019), we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Budzianowski, Paweł and Vulić, Ivan},
	month = aug,
	year = {2019},
	note = {arXiv:1907.05774 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/roland/Zotero/storage/6685FDWD/1907.html:text/html;Budzianowski_Vulić_2019_Hello, It's GPT-2 -- How Can I Help You.pdf:/home/roland/Work/Biblio/zotfiles/Budzianowski_Vulić_2019_Hello, It's GPT-2 -- How Can I Help You.pdf:application/pdf},
}
